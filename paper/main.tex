\documentclass{scrartcl}[2013/05/29]% Need at least an early state of KOMA-Script 3.12
% \documentclass[journal=jacsat,manuscript=article]{achemso}
\usepackage[utf8]{inputenc}
\usepackage[mincitenames=2, maxcitenames=2, maxbibnames=99, style=chem-acs, sorting = none]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk}
\usepackage{soul}
\usepackage{bm}
\usepackage{lineno}

% \usepackage[table,xcdraw]{xcolor}
% \usepackage{subfloat}

\linenumbers
\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in


\newcommand{\cheng}[1]{\textcolor{purple}{{\bf Cheng:~}#1}}
\newcommand{\mengyan}[1]{\textcolor{magenta}{#1}}
\newcommand{\maciej}[1]{\textcolor{blue}{#1}}

% \captionsetup[subfigure]{position=top, labelfont=bf,textfont=normalfont,singlelinecheck=off,justification=raggedright}
\captionsetup[subfigure]{font={bf,small}, skip=1pt, singlelinecheck=false}
\renewcommand{\thesubfigure}{\Alph{subfigure}}
\renewcommand\Authfont{\fontsize{14}{14.4}\selectfont}
\renewcommand\Affilfont{\fontsize{11}{10.8}\itshape}


\addbibresource{ref.bib}

%\title{Machine Learning guided Design\\
%of Ribosome Binding Sites}

\title{Machine learning guided batched design\\
of a bacterial Ribosome Binding Site}

\author[1,2,3]{Mengyan Zhang}
\author[4,5]{Maciej Bartosz Holowko}
\author[4,5]{Huw Hayman Zumpe}
\author[1,2,3,*]{Cheng Soon Ong}
\affil[1]{Machine Learning and Artificial Intelligence Future Science Platform, CSIRO, Canberra ACT 2601}
\affil[2]{Department of Computer Science, Australian National University, Canberra ACT 2601}
\affil[3]{Data61, CSIRO, Canberra ACT 2601}
\affil[4]{Synthetic Biology Future Science Platform, CSIRO, Canberra ACT 2601}
\affil[5]{Land and Water, CSIRO, Canberra ACT 2601}
\affil[*]{email: cheng-soon.ong@data61.csiro.au}

\date{\today{}}


\bibliography{ref.bib}
% \DeclareUnicodeCharacter{2212}{-}
\begin{document}

\maketitle

\section*{Abstract}

Optimisation of gene expression levels is an essential part of the organism design process.
Fine control of this process can be achieved by engineering transcription and translation control elements, including the ribosome binding site (RBS).
Unfortunately, the design of specific genetic parts remains to be challenging due to the lack of reliable design methods.
To address this problem, we have created a machine learning guided Design-Build-Test-Learn (DBTL) cycle for the experimental design of bacterial RBSs to demonstrate how small genetic parts can be reliably designed using relatively small, high-quality data sets.
We used Gaussian Process Regression for the Learn phase of the cycle and the Upper Confidence Bound multi-armed bandit algorithm for the Design of genetic variants to be tested in vivo.
We have integrated these machine learning algorithms with laboratory automation and high-throughput processes for reliable data generation.
Notably, by Testing a total of 450 RBS variants in four DBTL cycles, we have experimentally validated RBSs with high translation initiation rates equalling or exceeding our benchmark RBS by up to 34\%.
Overall, our results show that machine learning is a powerful tool for designing RBSs, and they pave the way towards more complicated genetic devices.\\


Keywords: machine learning, optimisation, genetic part design, ribosome binding site

\newpage

\section{Introduction}

One of the main tenets of synthetic biology is the design, evaluation and standardisation of genetic parts \cite{Brophy2014,Canton2008,Stanton2014}.
A central challenge is part design, which is understood as modifying the sequence of a genetic part for it to meet specific requirements.
Genetic parts are the units which are ultimately combined into more complex genetic circuits that produce desired functions in the target organisms.
This is usually done in terms of the Design-Build-Test-Learn (DBTL) cycle, where a given genetic part or organism is continually improved through an iterative process.
This cycle involves Designing (D) new DNA sequences to achieve a desired property in computer-aided design software, then physically Building (B) new constructed variants and Testing (T) using an analytical instrument in a laboratory.
Computer modelling can be used to Learn (L) and predict the characteristics of a genetic part \cite{Yeoh2019,Nielsen2016}.
Most of these computer models are based on either the thermodynamic properties of the involved molecules (DNA, RNA, proteins, etc.) or empirically-obtained values describing a relevant design property, like translation initiation rate (TIR) in the case of ribosome binding sites (RBSs) \cite{Xia1998,Chen2013,Reeve2014}.
However, de novo design of small genetic elements is challenging due to unknown relationships between their sequence and performance.\\

In this paper, we propose a machine learning guided Design-Build-Test-Learn cycle
for the experimental design of bacterial RBSs. This consists of two distinct
types of machine learning methods, one in the Learn phase and a second in the
Design phase.
We show how small genetic parts can be reliably designed using even relatively small, but high-quality data sets.
% In particular, we develop a method and use the learned predictor to generate both mean and uncertainty predictions for the Learn phase.
% Furthermore we develop a method the Design phase
% that recommends batches of genetic designs that optimize the characteristic of interest.
This work focuses on RBS part design and TIR prediction, rather than looking at its wider genetic context and impact on the general performance of the cell.
As the RBS is one of the key genetic elements controlling protein expression in bacteria,
it is a suitable target for establishing a workflow that could be potentially translated to more complicated systems.\\

In the Design phase of the DBTL cycle, designers often fine tune the characteristics of parts to give the resulting strains their desired properties. The ability to predict a characteristic of a genetic part (for example using a machine
learning method) only provides inputs to the problem. The designer still needs to choose from the large number of
possible variants to Build.
For instance, to increase the yield of a protein, increasing the TIR of the RBS responsible for translation of that protein could be targeted.
% Hence the Design phase of TIR has two steps:
% first to predict the TIR value for each potential RBS sequence,
Hence, the goal of the Design phase of TIR is to select a small set of RBS sequences from the design space (i.e. all possible DNA sequences) based on the predictions from Learn phase.
However, since the computational predictions are not perfectly accurate,
there is uncertainty about which potential RBS sequence has the desired
property (for example highest protein yield).
The designer could use the mean predicted TIR values and choose the best
ones to exploit the knowledge modelled by the computational predictions.
But the designer may also want to explore regions of DNA design space where the predictions are highly uncertain (e.g. as measured by
the predicted TIR standard deviation) and could yield high payoff.
Therefore, a major challenge of the Design phase is to balance the exploitation and exploration, which is addressed by one class of machine learning approaches called multi-armed bandits \cite{lattimore2020bandit}.
As we will see in this paper, multi-armed bandits are well suited to solving the challenge of recommending a small set of RBS sequences in the Design phase of the DBTL cycle.
\\

One way to generate predicted TIR values is to use existing RBS calculators.
\textcite{Reeve2014} surveyed three main RBS calculators, all of which predict the TIR based on the thermodynamic properties of the RBS and the ribosome \cite{Seo2013,Na2010,Salis2009}.
Predictions reported from all of these models range from relatively good ($R^2 >0.8$) to low ($R^2 <0.2$) depending on the data set \cite{Reis2020}.
This may be caused by:
i) they rely on calculations of free energies which can be difficult to estimate with high precision,
ii) in general, one of the best ways to improve the models' accuracy is by increasing the number of phenomena taken into account, which can lead paradoxically to decreased model accuracy due to accumulation of errors \cite{EspahBorujeni2016}, and
iii) by using deterministic coefficients to calculate energies, one disregards the often stochastic nature of processes in cells which can potentially increase prediction error \cite{Goss1998}.
There is also evidence that binding energy calculations may be poor predictors of RBS strength \cite{Saito2020,Sherer1980}.
This is reinforced by studies suggesting that RNA secondary structure is potentially a more important feature in TIR determination \cite{DESMIT1994,EspahBorujeni2016}.
This suggests that multiple interactions determine the mRNA-ribosome binding, and predicting TIR from the genomic sequence is still challenging.\\

Recent work has explored the use of machine learning predictors to learn
from historical data and generate predictions for use in synthetic biology,
vastly improving the DBTL cycle's performance~\cite{Camacho2018,Radivojevic2020,LAWSON2021}.
This work leverages the exponential increase in experimental data produced in synthetic biology~\cite{Freemont2019}
to improve predictors in the Learn phase of the DBTL cycle.
For example, \textcite{Jervis2019} used support vector machines and neural networks to optimise production of monoterpenoids in \emph{Escherichia coli}.
Similarly, \textcite{Costello2018} have used a number of machine learning approaches to analyse time-series multi-omics data to predict metabolic pathway behaviour.
Deep learning techniques have also been successfully used to analyse large synthetic biology data sets \cite{Alipanahi2015,Angermueller2016,Hollerer2020}.
Recall that the multi-armed bandit approach balances the exploration-exploitation
trade off by using predictive uncertainty. The machine learning predictors
mentioned, as well as existing RBS calculators,do not yet provide uncertainty levels to guide exploration with the notable exception of ART \cite{Radivojevic2020} .
Furthermore large data sets may not be available for a genetic part of interest, for instance the RBS.
Hence there is a need to develop methods for training predictors on smaller data sets and generate predictions for both their mean values and uncertainties of the predictions in the Learn phase of DBTL. The predictions of characteristics of interest for
each potential variant
can then be used in the Design phase.
% We apply a Bayesian model called Gaussian Process Regression \cite{Rasmussen2004} to learn from data in previous rounds and predict in design space.
\\

Our overall experimental goal is to maximise the TIR by building and testing batches of RBS sequences with only a small number of DBTL cycle iterations.
%\maciej{I've decided not to put the "other than maximisation" sentence here, I think it's appropriate to have it in discussion.}
We demonstrate how machine learning (ML) algorithms can be incorporated
into the DBTL cycle to predict (Learn) and recommend (Design) variants
of a bacterial RBS with the goal of optimising associated protein expression
level in the specific genetic context
(i.e. with specified bases upstream and downstream of the investigated RBS sequences).
Our proposed machine learning guided DBTL cycle is summarized in Figure~\ref{fig: Flowchart}.
Two types of ML algorithms are applied in the DBTL cycle.
In the Learn phase, the goal is to train a predictor that will predict the protein expression level by learning from the logged data.
We used a Bayesian, non-parametric regression algorithm, called Gaussian Process Regression (GPR) \cite{Rasmussen2004} in our pipeline.
In addition to providing a predicted mean TIR, GPR also provides uncertainty estimates via its predicted standard deviation.
GPR has been shown to  perform well with small amounts of trainig data in biological prediction tasks~\cite{Rasmussen2004, desautels2014parallelizing}.
The goal of the Design phase is to find a policy to recommend RBS sequences to query in batches so that we can identify the optimized RBS sequences within a given budget.
We use a version of the Upper Confidence Bound multi-armed Bandit algorithms \cite{desautels2014parallelizing} to learn the policy.
The policy uses the outputs from the GPR predictions to recommend useful batches for the Build and Test phases.
We demonstrate that laboratory automation methods in the Build and Test phases result in high quality TIR data, that is well suited for training machine learning methods in the subsequent Learn phase.
The two types of algorithms cooperate with each other and provide powerful tools for DBTL cycle in genetic parts design \cite{srinivas2012information, desautels2014parallelizing}.
Using our proposed machine learning guided DBTL cycle
(Figure~\ref{fig: Flowchart}), we were able to increase expression of our
target protein by up to 35\%, as compared to the very strong benchmark RBS. \\


\section{Results}

We present our RBS-optimising DBTL workflow that uses machine learning in Section~\ref{sec:dbtl-workflow}.
Machine learning is used in two different ways: i) we show the efficacy of the ML recommendations
in the Design stage (Section~\ref{sec:ucb-results}),
and ii) we demonstrate that the ML predictions are accurate in the Learn stage (Section~\ref{sec:gp-results}).
We present our new RBS sequence library in Section~\ref{sec:characteristics-of-library} and describe some interesting
characteristics of the discovered sequences,
as well as show the effectiveness of the automated laboratory workflow.

\subsection{The experimental workflow}
\label{sec:dbtl-workflow}

Our DBTL workflow, which uses machine learning to optimise protein expression, is shown in Figure \ref{fig: Flowchart}.
The Build and Test phases are driven chiefly by choices made by human researchers and the use of automated methods.
Machine learning algorithms are applied in Learn and Design.
In the Learn phase, we use the Gaussian Process regression algorithm to predict the TIR of RBS sequences comprising the experimental space.
In the Design phase, we use the Bandit algorithm to recommend new RBS sequences based on the predictions from Learn.\\

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{plots/Main_Paper/flowchart.pdf}
    \caption{\textbf{Flowchart of the machine learning-based experimental design.} The RBS design is recommended by the Upper Confidence Bound multi-armed Bandit algorithm. After generating the recommendations, the RBSs are built and tested using automated laboratory methods allowing for rapid construction and testing at scale. Finally, the obtained results are fed back to the Gaussian Process Regression prediction algorithm in the Learn phase. n is the current design round and k is the maximum number of rounds allowed by time and/or money. In regards to the "Goal Met?" condition, the goal in our case was to find sequences with TIR significantly higher than benchmark, but the goal can be generalised to fit the requirements of the user.}
    \label{fig: Flowchart}
\end{figure}

The exact position of the RBS in the sequence upstream of the protein coding sequence (CDS) can be hard to pinpoint.
However, most previous studies place the RBS in the 20 bp directly preceding the coding sequence.
In our investigation, we are using an RBS which is known to have a very high TIR when expressing GFP and is present in the pBb series of plasmids \cite{Lee2011}.
This template RBS sequence is 20 base pairs (bp) long with the sequence TTTAAGA\textbf{AGGAGA}TATACAT, where the highlighted nucleotides constitute the core of the RBS.
As this is the sequence against which new RBS sequences will be benchmarked,
we will refer to this sequence as the \textit{benchmark sequence} hereafter.
Additionally, we have experimentally confirmed that modifying the core sequence is statistically more impactful on TIR than changes made outside of it (see Figure \ref{fig:core_vs_noncore}).
This hypothesis has been built based on reported biases towards certain bases present in the core of the RBS but absent outside of it.
For example, according to \cite{SHULTZABERGER2001} there is a strong bias towards A and G bases in the core region of the RBS.
Similarly, outside of the 6 bases of the core in the wider 20 bp context of the RBS there is no significant bias towards any particular base which suggest that these bases do not contribute to the overall TIR of a given RBS.
Focusing on the 6 to 8 bp core sequence is a common RBS design approach \cite{Jeschek2016}.
Hence, in our design, we focus on modification of the core at nucleotide positions -8 to -13 (relative to the start codon of the GFP; this is where the consensus Shine-Dalgarno AGGAGG sequence is usually found in wild type \textit{E. coli}) of the RBS and we keep other positions the same as the benchmark sequence, i.e. TTTAAGA + NNNNNN + TATACAT, where N can be any nucleotide (A, C, G, T).
The total experimental (variant) space is then $4^6$ = 4096.\\

In our genetic design, the investigated RBS controls the expression of green fluorescent protein (GFP).
By controlling expression of a fluorescent protein with the RBS we can quickly assess the perceived relative TIR by measuring fluorescence of cells harbouring the expression vector over time.
Finally, the mRNA is transcribed from an IPTG-inducible promoter pLlacO-1.
Inducible expression allows synchronization of the start of the GFP expression in all the cultures by addition of IPTG.
Since standardisation and comparative studies should be done in as similar genetic context as possible, the design of this device has been deliberately kept simple to make such studies easier \cite{Beal2021}.\\

\subsection{DESIGN: Performance of the recommendation algorithm}
\label{sec:ucb-results}


The Bandit recommendations were made using the batch Upper Confidence Bound multi-armed Bandit algorithm~\cite{desautels2014parallelizing}.
In short, this algorithm is a stochastic method of probing the experimental space, and is sometimes referred to as Bayesian optimization.
It maximises the reward (output) from testing a limited number of instances from a large pool which cannot be tested exhaustively due to limited resources (time, computational power, money).
It balances the exploration-exploitation paradigm, where exploration focuses on testing data points which maximise information gain and exploitation focuses on recommending RBSs with high predicted TIR.\\

Figure \ref{fig: Swarmplot and Quantplot}A shows the results for all the RBS groups tested experimentally.
In each experimental round, in addition to the new RBS designs, we measure the TIR of the benchmark RBS as the internal standard.
We then obtain the normalised TIR (called \textit{TIR ratio}) by taking the ratio between the raw TIR of a new design and the average TIR of benchmark sequences in each round (which are run in triplicate in each round).
Figure \ref{fig:rawswarmplots.} shows these results in terms of raw TIRs.\\

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/swarmplot.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/quantplot.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/swarmplot_proj.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/histogram.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.42]{plots/Main_Paper/tsneplot.pdf}
    \end{subfigure}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/quantplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot_proj.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/histogram.pdf}
    % \includegraphics[scale=0.4]{plots/Main_Paper/tsneplot.pdf}
    \caption{
    \textbf{TIRs of RBS groups examined in this study.}
    \textbf{A)} Swarm plot showing the obtained TIRs divided into RBS groups.
    BPS-NC: base-by-base changes in the non-core region.
    BPS-C: base-by-base changes in the core region.
    UNI: Randomly generated sequences with uniform distribution.
    PPM: Randomly generated sequences with distribution following the Position Probability Matrix for all natural RBS in \emph{E. coli}.
    Bandit-0/1/2/3 - Bandit algorithm generated results for Round 0, 1, 2 and 3 respectively.
    SD - Shine-Dalgarno sequence.
    Dashed line is set to 1 and represents the averaged benchmark sequence TIR for that group.
    BN - benchmark sequences for all plates.
    (not all are exactly 1 due to them being shown as separate samples rather than per round averages.)
    \textbf{B)} Line plot showing TIR obtained in a given quantile (Q) of results divided into groups as in A).
    % save the random groups which were shown together due to similar distributions.
    UNI and PPM are merged into Random group and BPS-NC is not shown due to changes being made outside the core in that group.
    \textbf{C)} Exploitation v.s. Exploration for Bandit 1-3. Blue-hued points represent exploitation, those hued red represent exploration.
    \textbf{D)} Histogram with kernel density estimations (KDE) showing distributions of TIRs for Bandit groups.
    \textbf{E)} t-SNE plot showing the relative distances between sequences in our design spaces as calculated by our kernel function (weighted degree kernel with shift).
    The Unlabeled represents the RBS sequences in design space but have not been tested.
    The area of the circle corresponds to the experimentally-obtained TIR value.
    The TIR results in all subplots are shown normalised to the respective benchmark sequence sample which acts as internal standard; the TIR of a given RBS is divided by TIR of the benchmark RBS run in the same plate. }
    \label{fig: Swarmplot and Quantplot}
\end{figure}

To generate the data set from which the algorithm would learn, we decided to characterise a total of 450 RBS variants, a little over 10\% of the whole experimental space. 
About a quarter of the designs are experimental controls to provide a baseline for comparison.
To fit our automated workflow, we divided the 450 variants into batches of 90, split into 4 design rounds.\\

In the zeroth Round we tested two batches of designs, giving a total of 180 variants split as below:

\begin{itemize}
    \item BPS-NC and BPS-C group: 60 RBS sequences which are subsequent single nucleotide variations of all 20 nucleotides of the original, benchmark sequence. This batch is designed to show us influence of such single nucleotide changes on the overall performance of the RBS and the potential impact of changes made beyond the core part (see Supplementary Figure \ref{fig:core_vs_noncore}).
    % \mengyan{put it in main paper?}
    \item UNI group: 30 RBS sequences that were uniformly randomised, i.e. equal probability of choosing any nucleotide for each position. This group shows the performance of RBSs generated randomly.
    \item PPM group: 30 RBS sequences randomised based on the position probability matrix (PPM) generated from all the naturally-occurring RBS sequences in  the \emph{E. coli} genome \cite{barrick1994quantitative}. This group shows the performance of RBSs generated randomly, but following the natural nucleotide distribution.
    \item Bandit-0: 60 RBS sequences recommended by our implementation of the recommendation algorithm based on a data set obtained from literature \cite{jervis2018machine}, which contains 113 non-repeated records for 56 unique RBS sequences with their respective TIRs.
    This data set has been used due to the perceived similarity of its goal to that of this work - prediction of TIR based on phenotypic output.
\end{itemize}

In the subsequent 3 rounds, with one batch each, all 90 designs were generated using our machine learning algorithm based on the data obtained from the previous rounds (these groups are called Bandit 1 to 3 respectively).\\

All Round 0 groups (BPS-NC, BPS-C, UNI, PPM, Bandit-0) performed worse than our benchmark sequence in terms of TIR.
The best-performing group was the BPS-NC, which is explained by the relatively small impact on the TIR of changes made outside the RBS core.
The Bandit-0 group's performance is similar to randomly generated designs, despite being machine learning-driven, due to being trained on approximate data.
% \hl{These results show that generating a strong RBS sequence by random mutations is a non-trivial task, when the tested data set is relatively small.}\\
Starting from Round 1, where the prediction and recommendation algorithms were fed data from Round 0, the results improved significantly, with a number of sequences performing better than the consensus Shine-Dalgarno sequence and in one case, better than the benchmark (by 8\%).
In Round 2 we observed further improvement by obtaining more sequences with TIRs similar to our benchmark sequence.
Finally, in Round 3 the algorithm identified two sequences that were 34\% and 15\% stronger than the benchmark sequence.\\

In summary, 120 out of 450 sequences (BPS-NC, BPS-C, UNI, PPM) are experimental controls created by sequence randomisation.
Only a few of the random sequences were promising, but they were still 20\% weaker than the benchmark sequence's TIR (Figure 3A).
In fact, these 80\% TIR ratio sequences were created by randomising the sequence outside of the core RBS region (BPS-NC), which was statistically shown not to be significantly impactful on the TIR (Note, this applies only to the 7 bps on either side of the core; see Supplementary Figure S1 and work by \mbox{\textcite{Jeschek2016}}). Sequences from randomization of the core RBS (BPS-C, UNI, PPM) are more appropriate controls, from which the best sequence achieved only about 65\% of the benchmark TIR.
These results show that generating a strong RBS sequence by random mutation is a non-trivial task, when the tested data set is relatively small.
In contrast, our Bandit-driven design gives much better results, with RBSs getting close to benchmark performance and even exceeding it.\\

Figure \ref{fig: Swarmplot and Quantplot}B shows the same results but divided into quantiles where the specific point for a given group shows the highest TIR for that quantile.
The gradual increase for all quantiles can be observed for all Bandit groups, suggesting that the algorithms have a better understanding of the experimental space given more data.
The decreased result in the 0.9th quantile compared to the maximum value for the Bandit 3 group can be attributed to the increased emphasis on exploitation that has been set for that round compared to others.
We see this effect in Figure \ref{fig: Swarmplot and Quantplot}C (with details shown in Supplementary \ref{subsec: Exploration-exploitation visualisation} and Figure  \ref{fig:exploitationvsexplorationdetail}), where we coloured the data points for Bandit 1-3 groups according to their relative exploration - exploitation affinity.
Those with a high predicted mean are coloured blue and represent exploitation, those coloured red are with high predicted uncertainty and represent exploration.
The implication of the fact that an RBS sequence chosen at random will have low TIR
(as shown by UNI and PPM), is that most of the exploration will result in low TIRs.
These results confirm that RBSs with high TIRs tend to come from exploitation of the design space,
whereas the exploration points give relatively low TIRs.
Note that the exploration is necessary to expand our knowledge to the unknown parts of the design space and in effect allow us to exploit it better.\\

Figure \ref{fig: Swarmplot and Quantplot}D shows the TIRs of RBSs tested in the Bandit groups divided into bins with width equal to a TIR ratio of 0.1.
KDE plots have been overlaid to depict the calculated density for each group.
The increase in prevalence of later Bandit groups in the higher bins is evident, especially for Bandit 2 and 3, constituting the bulk of results in the $>0.8$ TIR ratio bins.
Notably, the distributions calculated for all the groups are bimodal - we discuss the possible reasons for that later in the text.\\


In Figure \ref{fig: Swarmplot and Quantplot}E we show a t-distributed stochastic neighbour embedding (t-SNE) \mbox{\cite{tsne2008}} plot depicting the experimental space.
Each RBS is located on the plot according to its distance from other RBSs as calculated by our weighted degree kernel with shift (see Section \mbox{\ref{sec: method prediction with kernel}}).
The RBSs recommended by Bandit groups cover the majority of the design space.
Additionally, a number of clusters were especially targeted by the recommendation algorithm.
For example, the circled clusters labelled as "G-Rich Clusters" have been actively recommended by the algorithm.
More specifically, sequences with 4 or more guanines in any position constituted 10\% of the randomly selected sequences and 5, 9, 16 and finally 25\% in each of the 4 Bandit guided batches respectively.

\subsection{LEARN: Prediction of RBS performance}
\label{sec:gp-results}
\label{subsec: results LEARN}

In the Learn phase, we use a popular regression algorithm, Gaussian Process Regression (GPR). GPR has two benefits: (1) It provides predicted TIR and the confidence interval needed for the UCB algorithm in the Design phase, and (2) it provides a natural way to calculate similarities between genomic sequences via kernels.
% GPR, as a non-parametric Bayesian regression algorithm, assumes the reward function is sampled from a Gaussian Process and has been shown with good performance with a small amount of data in biological prediction tasks \cite{Rasmussen2004, desautels2014parallelizing}. Additionally, GPR has been studied with bandits algorithms, such as UCB and its variants, with both theoretical and empirical good recommendation performance (i.e. sub-linear regret upper bound) \cite{srinivas2012information, desautels2014parallelizing}.
Figure \ref{fig: Scatterplot} shows how our implementation of the Gaussian Process algorithm performed in terms of predictions (on an hold out test set) in each round. 
As expected, the use of approximate data lead to poor predictions in Round 0.
The predictions improved for the subsequent rounds, the Spearman correlation coefficient rose from 0.269 for Round 0 to 0.546 for Round 3.
% Figure \ref{fig:scatter abc1 TT.} shows the performance of predictions for individual rounds.
\\

We provide the evaluation of the Learn phase in terms of a ranking-based Spearman correlation coefficient, which has been shown to be a more suitable evaluation metric when the prediction is used for recommendation tasks than coefficient of determination ($R^2$) or Pearson correlation coefficient
\cite{Schober2018,  Kang2019}.
The regression task in the DBTL cycle is more challenging than the large-scale data-based regression tasks.
In the early iterations, we have a limited number of data points and relatively high variability due to the measurement noise.
However, since the predictions are only used for recommendations in the Design phase, instead of precise predictions of the mean for each RBS, we only need to provide a valid ranking for both the predicted mean TIR and uncertainties, and thus the ranking of the UCB scores.
This is because, in the Design part, we select RBSs based on the ranks of the TIR predictions instead of numerical predictions.
% in this task, we are maximising the TIR of RBS and so we are looking for RBS with high rank 1, regardless of its numerical TIR.
This would ensure that for each round we would be testing the required designs, even if we are not exactly accurate in terms of their numerical characteristics.
\\

The
% obtained values of the coefficient of determination and Spearman correlation coefficient
performance of the Learn phase
is also influenced by the exploration-exploitation balance in the Design phase.
In each round, some data points were selected for exploration of the areas in which we have few or no tested data points.
For those exploration points, since the predictor never learns their label distributions and patterns, there is no chance for the predictor to provide accurate predictions of TIR values,
% which means that when tested, they have a high chance of having a experimentally-obtained mean different from what was predicted,
thus hurting the prediction performance in the short term.
However, this is useful information for future predictions as it allows us to understand the whole underlying space instead of focusing on local sub-optimal data points.
In other words, we are intentionally sacrificing the accuracy of our predictions in each round to improve them in the future rounds.
The effect of exploration of the space is the ability to find high TIR RBSs even with relatively low prediction performance.\\


\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_0.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_1.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_2.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_3.pdf}
    \end{subfigure}
    \caption{\textbf{Performance of the prediction algorithm (no kernel normalisation)}. The scatter plots A-D show the performance of the prediction algorithm calculated after each round.
    Note that the TIR values are normalised according to the standardisation described in section \ref{sec: method data pre-procesing}, which is different from the TIR ratio reported in Figure \ref{fig: Swarmplot and Quantplot}.
    X-axis and y-axis are respectively: the true measured TIR, and the predicted TIR.
    In (A)(B)(C), we show at round $t = 1,2,3$ respectively, we train our predictor based on the previously obtained data (round 0 to $t-1$) and show the predictions on both the training data (orange) and recommendations suggested by the Design phase (i.e. test data, blue).
    In D), the predictor was trained on a randomly chosen 80\% subset of all available data, and tested on the remaining 20\% data.
    The Spearman correlation coefficient (with corresponding p-value; calculated using test data only) are provided in each plot's title on test data only.
    The p-value here is for the null hypothesis stating that two sets of data are uncorrelated, providing strong evidence that the predicted TIR are accurate.
    }
    \label{fig: Scatterplot}
\end{figure}

\subsection{BUILD \& TEST: Characteristics of the tested sequences}
\label{sec:characteristics-of-library}

% Taken together, our data can be viewed as a reliable (low coefficients of variation) and extensive (high number of entries compared with widely used iGEM RBS collections, for example \cite{IGEM}) library of RBS sequences for \emph{E. coli}, some characteristics of which are shown in Table 1.\\

We present some important characteristics of the tested RBSs in Table 1, demonstrating the effectiveness of our Build and Test phases.
Figure \ref{fig:Library characteristics}A shows the sequence logo calculated for the Top 30 sequences (Figure \ref{fig: All_logo} shows the logo generated for all tested sequences).
It is generally understood that guanine-rich sequences promote strong transcription.
This expected bias towards guanine is clearly visible for all positions in our Top 30 RBSs.
This result combined with the Bandit algorithm's bias towards the G-rich cluster shown in Figure \ref{fig: Swarmplot and Quantplot}D reinforces the notion that our algorithm successfully identified G-rich sequences as the ones with higher probability of having a high TIR value.\\

\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \caption{}
         \includegraphics[scale=1.2]{plots/Main_Paper/TOP30_logo.pdf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \caption{}
         \includegraphics[scale=0.43]{plots/Main_Paper/heatmap.pdf}
     \end{subfigure}
     \caption{\textbf{Characteristics of strong RBSs.} A) Sequence logo calculated for the Top 30 tested sequences. B) Heatmap showing the edit (Hamming) distance required for positive change in TIR for RBSs with high and medium TIRs. The temperature scale shows the difference between a given RBS on the y-axis and the RBS with the strongest TIR at the given distance. Every second RBS is labelled for increased legibility.}
     \label{fig:Library characteristics}
\end{figure}

\begin{table}[!h]
\centering
\begin{minipage}[c]{0.6\textwidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Characteristics of the library}                                                       & \textbf{Statistics} \\ \hline
Total experimental space                                                                      & 4096                \\ \hline
Planned constructs                                                                            & 450                 \\ \hline
Successfully constructed                                                                      & 445                 \\ \hline
Sequences with CV\textless{}40\%                                                              & 79\%                \\ \hline
Sequences with CV\textless{}20\%                                                              & 27\%                \\ \hline
\begin{tabular}[c]{@{}c@{}}Efficiency ratio of Bandit design \\ (compared with random)\end{tabular} & 2                   \\ \hline
Raw TIR range                                                                                     &      [4.93, 105.38]               \\ \hline
TIR ratio range                                                                                     &      [0.06, 1.34]               \\ \hline
\end{tabular}
\end{minipage}
\begin{minipage}[c]{0.38\textwidth}
\centering
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{tabular}{|c|c|}
\hline
\textbf{Top RBS Core} & \textbf{TIR Ratio} \\ \hline
GGGGGC                & 1.34             \\ \hline
GGGGGT                & 1.15             \\ \hline
GGCTAT                & 1.08              \\ \hline
\textbf{AGGAGA}                & 1                  \\ \hline
GGCGTT                & 0.98            \\ \hline
GGGGGG                & 0.98             \\ \hline
GGCGAC                & 0.98             \\ \hline
CAGGAG                & 0.96             \\ \hline
GGCGAG                & 0.95             \\ \hline
\textbf{AGGAGG}       & 0.39            \\ \hline
\end{tabular}
\end{minipage}
\caption{\textbf{Characteristics of the library.}
Left table presents some of the characteristics of our library.
Right table presents 10 RBS sequences with their corresponding TIR ratios; the first 9 are the strongest sequences including the benchmark sequence (AGGAGA) and the last is the Shine-Dalgarno sequence (AGGAGG).
CV is coefficient of variation (standard deviation (STD) of a sample divided by its mean; see details in Figures \ref{fig: SDhist} and \ref{fig:variation of biological replicates}).
Efficiency of the Bandit design is calculated by dividing the highest TIR found using machine learning by the highest TIR found using random sequence generation.}
\end{table}

We discovered an interesting phenomenon of large edit distances between RBSs with high TIRs.
In other words, given an RBS with high TIR, multiple sequence edits need to occur to reach another RBS with high TIR (we assume here that our data set includes most of the strongest RBSs for each distance - sequence combination).
We define the edit distance as Hamming distance, that is, how many positions have to be changed to get from one sequence to the other (Hamming distance of 0 means that the sequences are identical and 6 means that they are two completely different sequences).
Figure \ref{fig:Library characteristics}B shows the edit distance that is required for positive change in TIR for an RBS with TIR $>0.75$.
For RBSs with high TIRs ($>1$), the minimum distance that is required to increase the TIR is 2, with edit distances between 2 and 5 giving similar results.
For RBSs with medium TIRs ($<1$), a distance of 1 is enough to produce a meaningful increase in TIR.
\\

This means that as the TIRs of examined RBSs increase, exploring sequences which are increasignly dissimilar to the current candidates tends to give more meaningful improvement.
As long as this does not impact targeted methods like machine learning-guided design, this implies that the low rate of natural mutations will be very slow to explore more dissimilar sequences on such a short distance \cite{Lee2012}, which indicates that methods like Adaptive Laboratory Evolution may not be able to find very strong RBSs with a limited budget.
In other words, because the examined sequence is relatively short (6 bp in a wider 20 bp context) the time required to accumulate 2 or more changes in the RBS region required for meaningful increase in TIR might be prohibitively long (Figure \ref{fig: Swarmplot and Quantplot}A shows comparison between efficiency of random versus directed sequence generation).
In such cases, a directed process, like the one described here, should be strongly encouraged.
This observation is in line with approaches seen in other disciplines, e.g. protein engineering, where more directed changes yield better results than random changes \cite{Jackel2008}.\\

Finally, while our strong sequences showed some affinity for the anti-sense sequence of the ribosome known to bind to RBS, they did not show any obvious secondary structures that could explain their TIRs (see Figure \ref{fig:structures}).
This result combined with the unexpectedly bimodal nature of KDEs in Figure \ref{fig: Swarmplot and Quantplot} reinforces the notion, based on the previously reported literature \cite{Saito2020,EspahBorujeni2016}, that there may be a number of different mechanisms governing the probability of effective RBS-ribosome binding.\\



\section{Discussion}

In this work, we show how a machine learning guided approach, and high-throughput, automated laboratory methods can be jointly applied to efficiently optimise a small genetic part, in this case maximising the TIR of a bacterial RBS.
In the Learn phase, we used Gaussian process regression to predict the TIR mean and uncertainty of that prediction.
To represent RBSs and capture the similarities between them, we choose to use
the Weighted Degree Kernel with Shift method, which fits well with Gaussian processes.
In the Design phase we used an Upper Confidence Bound multi-armed Bandit algorithm to recommend sequences to be tested in batches.
In the Build and Test phase, we performed our experiments using laboratory
automation to increase their speed, reliability and reproducibility.
Using our proposed workflow and testing 450 RBS variants in 4 DBTL cycles,
we designed and experimentally-validated RBSs with high translation initiation rates equalling or exceeding the currently known strong RBSs in this genetic context by up to 34\%.
Furthermore, we have generated an extensive library of diverse RBSs that can be used as a basis for future studies.
In the rest of the section, we first revisit our overall experimental goal and challenges encountered in each phase of the
DBTL cycle (Figure~\ref{fig: Flowchart}).
We then link our proposed methods with related work and discuss the potential generalisation of our framework.
% Then we will discuss the lessons learned for 
We further discuss our design choices and open questions in this line of research.
We refer \cite{zhang_synbio2021} as more detailed discussion.\\ 

Our goal is to show the power of the machine learning guided DBTL cycle on RBS optimisation.
Our approach has shown that this combination of the two machine learning algorithms is able to correctly detect and exploit rules of biological design that otherwise require substantial time and experiments to uncover.
We focus on the part-centric optimisation \cite{Hollerer2020}, which is an important task in synthetic biology. 
Understanding how to design the individual parts also allows us to extend the framework to better strain design \cite{Jervis2019}, where the part is optimised with a wider goal of strain optimisation.
% metabolic pathways are made up of individual parts and understanding their performance by themselves allows for better strain design.
% It is important to make a distinction between works focusing on the part itself like \textcite{Hollerer2020} and works where the part is optimised with a wider goal of strain optimisation like in \textcite{Jervis2019}
\\

The machine learning guided DBTL framework has good potential to be generalised to multi-gene pathway
design as proposed by \textcite{hamedirad2019towards} and recently reviewed by \textcite{LAWSON2021}.
For example, the optimisation goal can be adjusted to address combinatorial optimisation for multi-gene and RBS scenarios; since this would be a large-scale data task, the current Gaussian Process regression prediction model could be updated to a deep Gaussian Process regression approach and the current Bandit algorithm could be optimised towards querying large design space to reduce computational complexity~\cite{Shahriari2016}.\\

In the Design phase, we focus on maximising the TIR of RBSs, by gradually moving our emphasis from exploration to exploitation as we progress through the design rounds.
While maximisation could be the appropriate payoff function for optimising RBSs
or other small parts, other payoffs may be better in more complex cases.
For example, when considering multi-gene metabolic engineering, maximising expression of individual genes may result in excessive metabolic burden, 
which could be achieved within the bandit framework by 
% changing the objective of the algorithm to maximising the range of the black-box function, or identifying the part of function above a threshold~\cite{gotovos2013active}, or 
combining different goals into a multi-objective method~\cite{shu2020new}.
We constrain our design space over 6-core parts of RBS sequences in this study, the design space will increase exponentially when designing a longer sequence. 
The computational complexity of calculating and sorting acquisition functions (e.g. UCB scores) can be reduced by discretizing the space adaptively and hierarchically \cite{shekhar2018GPBanditDiscre, zhang2021GPOO}.\\

In the Learn phase, our approach has correctly identified the correlation between high guanine content in the RBS and high TIR.
We have achieved this despite the relatively low Spearman scores for our predictions. This observation of useful recommendations despite low prediction scores corroborates recent evidence from other studies~\cite{Radivojevic2020, Opgenorth2019}.
% Additionally, it has been shown that when the prediction is used for recommendation tasks like the one considered in this paper, ranking-based metric like Spearman correlation coefficient is a better evaluation metric
% \cite{Schober2018,  Kang2019}.
Finally, our predictor Gaussian Process regression model, compared with previously described calculators using a deterministic thermodynamic approach, is able to show the uncertainty of the predictions, which can be used by our bandit algorithm to give better recommendations.\\

To further test the performance of our predictions and recommendations we have generated a TIR label for each of the 4096 RBS sequences using the RBS calculator \cite{Salis2009} and used our approach to simulate the search for the strongest designs in limited number of rounds.
We show these results in Supplementary Figure \ref{fig: Swarmplot and Quantplot Salis}, but in short, our approach was able to  identify above 75\% of the sequences stronger than the benchmark in up to 2 rounds and 99\% of them in up to 3 rounds.
This surprisingly high efficiency can be partially attributed to the source of the TIR predictions, which being generated algorithmically are easier to predict by our algorithm than real world data.
Nevertheless, it shows that the algorithm can efficiently learn the sequence to TIR relationship and exploit it subsequently in a recommendation task.\\

In this study we have limited the number of design rounds to four.
There were a number of reasons for this, including limitations on time and money, but also because the results obtained showed that we have achieved our goal of generating very strong RBS designs.
There is a possibility that increasing the number of experimental rounds would enable us to improve the results further, however, this has to be put in the context of limited resources.
For example, scanning the whole space would surely achieve the best results, i. e. would enable us to find the strongest possible RBS, but that would require unreasonable use of resources.
Compared to solutions like the one reported by Hollerer \cite{Hollerer2020}, our solution can be used when a high-volume method for data-generation is not available (for example where there is no fluorescent readout available), while still providing the required results (optimised part). \\

There are still open questions that need to be addressed for the application of machine learning in synthetic biology.
Firstly, we would like to understand how we can extract more biologically-important information from the decisions made by our algorithms.
We have shown that the algorithms are able to exploit them, but it will be important to create tools that will enable their reliable extraction from the results obtained.
Secondly, given the small number of RBS sequences tested, how can machine learning algorithms provide more accurate predictions and uncertainty quantification?
Thirdly, the generalisability of the method is unknown.
We believe that the method described here would be useful for designing other small genetic parts, but the complexity of the task quickly increases with the size of the analysed sequence, so the method's applicability might be impacted at some point.
Similarly, the re-usability of the obtained data set is currently unknown.
The TIR of an RBS is dependent on its genetic context, but our Hamming distance and TIR impact of nucleotides outside of the core (Figures \ref{fig:Library characteristics}B and \ref{fig:core_vs_noncore} respectively) studies indicate that as long as the changes in the genetic context are small the obtained data set could serve as a basis for similar design efforts.
For example, the data could be used to teach the algorithm for the first round of new designs, which in turn could decrease the number of rounds required to obtain the required characteristics.
Finally, the practical optimal exploration-exploitation balance between rounds and samples is still an open question.
In our work we have steered the recommendation process towards exploitation as we progressed through the rounds, in line with other works addressing this balance \mbox{\cite{Radivojevic2020}}.
%While Radivojevic \textcite{Radivojevic2020} provides a Bayesian ensemble framework, we focus on Gaussian process and provides methods to design batch recommendation better.}
%In this research we have decided to gradually move the balance from exploration to exploitation across the rounds, but it is not yet clear what is the optimal way of conducting this change.
\\
% \section{Conclusion}

We have found our approach of bringing machine learning and synthetic biology experts together very fruitful.
Synthetic biology is promoting standardised and normalised testing in biology and naturally pairs with machine learning, which can leverage the high quality biological data sets generated when the correct design rules are observed.
The addition of machine learning to synthetic biology also adds an additional layer of scrutiny to the generated data sets through the advanced statistical methods that can be used to design and analyse the experiments.
On top of that, the use of automation has helped us to produce more reliable results, which gave us the required confidence in our predictions and recommendations.
We envision that pairing machine learning with high-throughput automation will keep delivering a high number of good quality data sets and improved methods for biological engineering.\\

In future we hope to extend the algorithm to other more complicated genetic elements, including promoters and terminators.
However, it is important to reiterate that the experimental space grows exponentially with the number of examined positions, so the space becomes increasingly hard to cover with experiments and so the percentage of the space that is measured will inevitably decrease, which might impact the expected recommendation results.
To solve this problem, different algorithms or experimental techniques might be needed, but the general workflow can be reused.\\


%! TEX root=./main.tex
\section{Materials and Methods}

\subsection{Laboratory experimental design}

\subsubsection{BUILD: Construction of genetic devices}

\textbf{Plasmid Design.}
The pBbB6c-GFP plasmid was used for all our designs.
This plasmid contains the GFP mut3b CDS, expression of which can be induced by the addition of isopropyl $\beta$-D-1-thiogalactopyranoside (IPTG) (Merck, Darmstadt, Germany, catalogue no. I5502).
The original RBS for the GFP CDS was replaced using a combination of PCR and isothermal assembly.
Primer sequences and the assembly strategy were generated using the Teselagen DESIGN software (Teselagen Biotechnology, San Francisco, CA).

\textbf{PCR.}
PCR amplification of the cloning inserts was done using Q5 High-Fidelity 2X Master Mix (NEB, Ipswich, MA, catalogue no. M0492L).
20 $\mu$L reactions were prepared by dispensing 1 $\mu$L of each 10 $\mu$M reverse primer into the wells of a 96-well PCR plate using the Echo liquid handler (Beckman Coulter, Brea, CA).
A mastermix consisting of polymerase premix, plasmid DNA template (pBbB6c, 5-10 ng per reaction), and the single 10 $\mu$M forward primer was prepared and dispensed using the FeliX liquid handler (Analytik Jena, Jena, Germany) or electronic multi-channel pipette. Reactions were run using Touchdown PCR or standard PCR cycling methods in C1000 thermal cyclers (Bio-Rad, Hercules, CA).
Capillary electrophoresis of PCR products was performed using the ZAG DNA Analyzer system (Agilent Technologies, Santa Clara, CA).
2 $\mu$L of each PCR reaction was electrophoresed using the ZAG 130 dsDNA Kit (75-20000 bp) or ZAG 110 dsDNA Kit (35-5000 bp) (Agilent Technologies, catalogue no. ZAG-110-5000; ZAG-130-5000). ZAG sample plates were prepared using the Sciclone G3 liquid handler (Perkin Elmer, Waltham, MA).
ProSize Data Analysis Software (Agilent Technologies) was used to generate gel images from the sample chromatograms, and amplicon sizes were estimated by reference to the upper and lower DNA markers spiked into each sample and a DNA ladder run in well H12 of each sample plate. 


\textbf{Isothermal DNA Assembly.}
Constructs were assembled using NEBuilder HiFi DNA Assembly Master Mix (NEB, catalogue no. E2621L).
Reactions consisting of approximately equal amounts of the common fragment and the variable fragment were prepared using the FeliX liquid handler or electronic multi-channel pipette, to a final volume of 5 or 10 $\mu$L.
Assemblies were run in the thermal cycler for 1 hour at 50$^{\circ}$C, followed by an infinite hold step at 4$^{\circ}$C.
Finally, samples were incubated with of 50 nL of DpnI (NEB, catalogue no. R0176S) at 37$^{\circ}$C for 90 minutes to degrade any residual template DNA.

\textbf{\textit{E. coli} transformation.}
The DH5$\alpha$ cell line (Thermo Fisher Scientific, Waltham, MA, catalogue no. 18265017) was made chemically competent using the Mix $\&$ Go \textit{E. coli} Transformation Kit $\&$ Buffer Set (Zymo Research, Irvine, CA, catalogue no. T3001).
20 $\mu$L of cells was aliquoted into each well of a cold 96-well PCR plate and stored at -80$^{\circ}$C for later use.
Plates of cells were thawed on a -20$^{\circ}$C cold block before 3 $\mu$L of the assembly product was added and mixed using the FeliX liquid handler.
Cells were incubated on a cold block for 2-5 minutes before being plated in a 96 (12 x 8) grid on Omnitrays containing LB (BD, Franklin Lakes, NJ, catalogue no. 244610) and 15 g/L agar (Merck, catalogue no. A1296) with 34 $\mu$g/mL chloramphenicol (Merck, catalogue no. C1919).
Plates were incubated overnight at 37$^{\circ}$C. Cells were plated using the FeliX liquid handler.

\textbf{Automated colony picking and culturing.}
A PIXL colony picker (Singer Instruments, Roadwater, United Kingdom) was used to select individual colonies from the transformation plates using the 490-510 nm (cyan) light filter.
Each selected colony was used to inoculate 1 mL of selective medium in a 2 mL square well 96 plate.
They were then cultured overnight at 37$^{\circ}$C with shaking (300 rpm).

\textbf{Glycerol stock preparation.}
100 $\mu$L of sterile 80\% (v/v) glycerol (Chem-Supply, Gillman, Australia, catalogue no. GA010) and 100 $\mu$L of overnight culture were combined in the wells of a 96 deep (2 mL) round well plate using the FeliX liquid handler or electronic multi-channel pipette
They were then sealed with a 96-well silicone sealing mat and transferred to a -80$^{\circ}$C freezer. 

\textbf{Sequencing}
Strains that gave GFP fluorescence intensity readings similar to that of the original RBS were selected for sequence confirmation by capillary electrophoresis sequencing (CES) (Macrogen, Inc., Seoul, South Korea).
The strains transformed with each of the selected constructs were grown to saturation in 5 mL LB medium with chloramphenicol selection (34 $\mu$g/mL).
Plasmids were extracted from the cultures using the QIAprep Spin Miniprep Kit (QIAGEN, Hilden, Germany, catalogue no. 27106) according to the manufacturer's instructions.
Plasmid concentrations were quantified using the Cytation 5 plate reader with the Take3 Micro-Volume Plate (BioTek, Winooski, VT) and all fell in the range of 100-200 ng/$\mu$L.
Samples of 20 $\mu$L of undiluted plasmid DNA were sequenced using a single primer (5'-CGATATAGGCGCCAGCAA-3') that binds approximately 150 bp upstream of the RBS.
Reads were aligned with the template sequence in the Teselagen software.

\subsubsection{TEST: Culture analysis}

\textbf{Test strain culture.}
Overnight cultures (six biological (restarted every time from glycerol stock) replicates for each batch) were started by inoculating 1 mL of LB medium supplemented with 34 $\mu$g/mL chloramphenicol with 2 $\mu$L of the glycerol stock in a 96 deep (2 mL) round well plate, using the FeliX liquid handler or electronic multi-channel pipette.
Cultures were incubated at 37$^{\circ}$C with shaking (300 rpm) for 17 hours. 
The following morning, 20 $\mu$L of each overnight culture was added to 980 $\mu$L of fresh selection medium and these cultures were grown at 37$^{\circ}$C with shaking in a 96 deep (2 mL) round well plate. 
After 90 minutes, cultures were induced with IPTG to a final concentration of 0.5 mM. This was done by transferring 1.0 $\mu$L of 0.1 M IPTG to each well of a flat-bottom clear polystyrene 96-well plate using the Echo liquid handler, then adding 300 $\mu$L of culture to each well using an electronic multi-channel pipette.


\textbf{Microplate spectrophotometry.}
The plates were monitored in the Cytation 5 microplate reader immediately after the addition of IPTG.
Cytation 5 acquisition and incubation/shaking settings were as follows: length of run: 8 hours; interval: 10 min; continuous orbital shake at 237 rpm and slow orbital speed; excitation wavelength: 490/10 nm; emission wavelength: 515/10 nm; bottom read; gain: 60; read height: 7 mm; read speed: Sweep.

\subsection{Machine learning experimental design}

Two types of machine learning algorithms have to be applied to drive the experimental design workflow shown in Figure \ref{fig: Flowchart}.
% To automatically design the RBS sequences in batch using machine learning, we can logically divided the workflow into two parts: 
One type of machine learning algorithm is a prediction algorithm (\textbf{LEARN}), which helps us learn the function of TIR with respect to RBS sequence. The other type of machine learning algorithm is a recommendation algorithm (\textbf{DESIGN}), which recommends RBS sequences to query (test) in each round (in the sense of a single pass through the DBTL cycle) based on the predictions from LEARN. \\

% 1) \textbf{LEARN}: A regression algorithm which takes the RBS sequences as input features and TIR scores as labels, trains on sequences with known labels and returns the predicted TIR scores and the respective confidence intervals.
% 2) \textbf{DESIGN}: An online learning approach which recommends the RBS sequences based on the predicted TIR scores and confidence intervals.
In Round $t$, prediction and design are based on the results obtained in all previous rounds. 
Our implementation of the machine learning algorithms was tested in Python 3.6 and used the scikit-learn library \cite{scikit-learn}. 
In the following paragraphs, we describe our machine learning pipeline (which is applicable to design Rounds 1-3, denoted as Bandit 1-3) in the following order: data pre-processing, prediction and kernels, and finally recommendation. 
The pipeline for our 0th Round of machine learning designs, denoted as Bandit-0, is reported in Supplementary \ref{sec: ML design pipeline}.


\subsubsection{Data pre-processing}
\label{sec: method data pre-procesing}

For each RBS sequence, we measured the TIR of 6 biological replicates.
We measure GFP fluorescence of the culture in the log phase as it is the most representative one.
% \textit{Log growth phase} is a phase in the growth of bacterial culture, the culture in general starts slow which is called \textit{lag phase}, then it grows exponentially which is called log phase after which it plateaus.
We also measure the Optical Density at 600 nm (OD600) of the culture every 10 minutes, which reflects how dense (how many cells) the culture is. Denote our measuring time as $t \in \{t_{min}, \ldots, t_{max}\}$, where ${t_{min}}$ start and end of the log growth phase.
At $t_{min}$, the $\frac{GFP}{OD_{600}}$ value is at its minimum.
We choose the time range to be 4 hours, i.e. $t_{max} - t_{min} = 4h$.
In our experiment, TIR is calculated as an averaged sum of GFP fluorescence divided by OD600 of the culture over time (calculated using values from all the measured data points),
$$TIR=\frac{1}{t_{max} - t_{min}} \sum_{t = t_{min}}^{t_{max}}\frac{GFP(t)}{OD_{600}(t)}.$$

% where TIR is calculated as an averaged sum of GFP fluorescence divided by Optical Density at 600 nm (OD600) of the culture over time (calculated using values from all the measured data points), following the formula:

% $$\mathnormal{TIR=\frac{\sum_{t_{min}}^{t_{max}}\frac{GFP(t)}{OD_{600}(t)}}{\Delta t}}$$

% where ${t_{min}}$ is the time at which the $\frac{GFP}{OD_{600}}$ value is at its minimum denoting the start of the log growth phase, ${t_{max}}$ is the time at which the logarithmic phase ends (on average, for our samples that was 4 hours from ${t_{min}}$ so that was the time point used for TIR calculations for all samples), ${\Delta t}$ is the difference between ${t_{max}}$ and ${t_{min}}$.\\

For label pre-processing, we first adjust the TIR values in each round using the round-wise reference values.
The reference value is the TIR of the benchmark sequence that is run in triplicate in each round.
Specifically, in Round $t$, we subtract the TIR mean of the benchmark RBS measured in Round $t$ from all TIR values measured in the same round, for each replicate separately. 
We then normalised the data by performing a logarithm transformation and standardisation on the adjusted TIR label for each replicate separately.
After normalisation, each replicate has zero mean and unit variance.
Furthermore, we also normalised the kernel matrix used for prediction by centring and unit-norm normalisation, which is reviewed in details in Supplementary \ref{supp: Normalisation of Kernel}.
% Since the design space is known, we consider the transductive approach \cite{} for the kernel pre-processing with centering and unit-norm normalisation.


\subsubsection{Prediction: Gaussian Process Regression with String Kernel}
\label{sec: method prediction with kernel}


To find RBS sequences with the highest possible TIR score after a total number of rounds $N$, we consider our experimental design problem as a sequential optimisation of an unknown reward function $f: \mathcal{D} \rightarrow \mathbb{R}$, where $\mathcal{D}$ is the set containing all RBS sequence points in the design space, and $f(\mathbf{x})$ is the TIR score of the 6-base core sequence of the RBS $\mathbf{x} \in \{A,C,G,T\}^{6}$.
In each Round $t$, we choose a set of $m$ points $\mathcal{S}_t \subset \mathcal{D}$ and observe the function value at each point in the selected set $\mathcal{S}_t$, i.e. $y_i = f(\mathbf{x}_i) + \epsilon$, for all $i \in \mathcal{S}_t$, where $\epsilon$ is the Gaussian random noise with unknown mean and standard deviation. \\
% This noise is influenced by the accuracy of the RBS predictor and other experimental sources of interference (e.g. time, temperature, operator, etc.). 
% REVIEW:
% \mengyan{we should discuss measurement noise somewhere, maybe with a sample example for the dataset we used.}

For the regression model, we have used a Bayesian non-parametric approach called \textit{Gaussian Process Regression (GPR)} \cite{Rasmussen2004,srinivas2012information, romero_navigating_2013}.
We model $f$ as a sample from a \textit{Gaussian Process} $\mathcal{G} \mathcal{P}(\mu(\mathbf{x}), k(\mathbf{x}, \mathbf{x'}))$, which is specified by the mean function $\mu(\mathbf{x})=\mathbb{E}[f(\mathbf{x})]$ and the kernel function (also called \textit{covariance function}) $k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbb{E}[(f(\mathbf{x})-\left.\mu(\mathbf{x}))\left(f\left(\mathbf{x}^{\prime}\right)-\mu\left(\mathbf{x}^{\prime}\right)\right)\right]$.
%As illustrated in Figure \ref{fig: Gaussian Process Regression Example.}, 
GPR can predict the posterior mean of the RBS sequences in the design space. At the same time, it can provide the posterior standard deviation, which represents the level of uncertainty for the prediction.

We review the predictions of GPR in Supplementary \ref{supp: GPR}.\\
% We assume the observations are noisy, so even when a data point in observed then confidence interval is still bigger than 0.
% As we can see, the true function is in the confidence interval when there are data points are observed.

% \begin{figure}[t]
%     \centering
%     \includegraphics[scale=0.7]{plots/Prediction with Initial Observations.pdf}
%     \caption{Gaussian Process Regression Example. This plot shows the GPR prediction with confidence interval based on 4 initial observations. The confidence interval are shown in predicted mean $\pm$ 1.96 standard deviation.}
%     \label{fig: Gaussian Process Regression Example.}
% \end{figure}

The choice of kernel function is critical for accurate predictions, since it controls smoothness and amplitude of the function to be modelled.
For Bandit designs in Round 0, since we only had access to a limited number of data points from the literature, we chose to use one of the basic string kernels, the \textit{spectrum kernel} \cite{leslie2001spectrum} to process the core 6 bp and dot product kernel \cite{Rasmussen2004} (with one-hot embedding) to process the 7 bp flanking sequences both upstream and downstream of the core sequence.
Among various string kernels which have been studied by \mbox{\textcite{Ben-Hur2008}} and we will introduce briefly in Supplementary \mbox{\ref{sec: Choices of Kernels}}, we chose the one with the best reported performance, the \textit{weighted degree kernel with shift} (WDS) \mbox{\cite{ratsch_rase_2005_wds, Ben-Hur2008}},  
to represent the RBS sequences in subsequent rounds and specify the kernel function of $GP$. 
WDS is a type of a string kernel, which takes two sequences as inputs and outputs a scalar value which represents the similarities between the two sequences.  
WDS kernel does this by counting the matches of substrings of a certain length (i.e. kmers) that constitute the sequence.
The maximum substring length is specified by $\ell$.
The WDS takes into account the positional information by counting substrings starting from different positions, where the start position is specified by $l$.
Additionally, the WDS kernel considers the shifting of substrings, with the maximum shift specified by $s$.
This could be useful when there is a shift between two sequences.
For example, two sequences A\textbf{CCTGA} and \textbf{CCTGA}A are in 1-shift. \\
% there is a common part \textbf{CCTGA} which is begins at the 2nd nucleotide in sequence 1 and at the 1st nucleotide in sequence 2, hence the calculated shift would be 1.

We now define WDS kernel.
Let $\mathbb{I}$(A) be the indicator function, which equals 1 if $A$ is true and 0 otherwise.
Then $\mathbb{I}(\mathbf{x}_{[l+s:l+s+d]} = \mathbf{x}_{[l:l+d]}^\prime)$ indicates whether two substrings of length $d$ match, between $\mathbf{x}$ starting from position $l+s$ and $\mathbf{x}^\prime$ starting from position $l$.
This is similarly done for $\mathbb{I}(\mathbf{x}_{[l:l+d]}= \mathbf{x}_{[l+s:l+s+d]}^\prime)$.
By having these two terms considering substrings of two sequences with starting positions differing by $s$ characters, the WDS can measure shifted positional information.
When $s = 0$, the kernel function counts the matches with no shift between sequences.
Let $\mathbf{x}, \mathbf{x}^\prime$ be two RBS sequences with length $L$, the WDS kernel is defined as
\begin{align}
        k_\ell^{WDS}(\mathbf{x}, \mathbf{x}^\prime)
        %&= \sum_{d=1}^{\ell} \beta_d \sum_{l=1}^{L-d+1} \gamma_l \sum_{s = 0, s + l \leq L}^{S(l)} \delta_s
        %\left(k_d^{Spec}(\mathbf{x}_{[l+s:l+s+d]}, \mathbf{x}_{[l:l+d]}^\prime) + (k_d^{Spec}(\mathbf{x}_{[l:l+d]}, \mathbf{x}_{[l+s:l+s+d]}^\prime)\right)\\
        = \sum_{d=1}^{\ell} \beta_d \sum_{l=1}^{L-d+1} \gamma_l \sum_{s = 0, s + l \leq L}^{S(l)} \delta_s
        \left(\mathbb{I}(\mathbf{x}_{[l+s:l+s+d]} = \mathbf{x}_{[l:l+d]}^\prime) + \mathbb{I}(\mathbf{x}_{[l:l+d]}= \mathbf{x}_{[l+s:l+s+d]}^\prime)\right),
\end{align}
where
$\beta_d = \frac{2(\ell - d + 1)}{\ell(\ell+1)}, \delta_s = \frac{1}{2(s+1)}$, $\gamma_l$ is a weighting parameter over the position in the
sequence, where we chose to use a uniform weighting over the sequences, i.e. $\gamma_l = 1/L$ ($L = 20$ in our design).

$S(l)$ determines the maximum shift at position $l$. 
Furthermore, we normalise the kernel with centring and unit-norm in terms of all sequences in the design space. 
% In our design, the length of RBS sequence $L = 20$.

The hyperparameters for kernel, including maximum substring length $\ell = 6$, maximum shift length $S(l) = 1$, and the standard deviation of GP noise $\alpha = 1$ were chosen based on 10-repeat 5-fold cross validation.
The values of hyperparameters considered were $\ell \in \{3,4,5,6\}$, $S(l) = \{0,1,2\}$, $\alpha = [0.5, 1, 2]$.
% ($\ell=6$, and $S(l)= 1$, $\alpha = 2$).




%  For recommendations, we consider the \textit{Upper Confidence Bound (UCB)} type algorithms.
%  As one popular type of the bandit algorithms \cite{lattimore2020bandit}, the UCB type of algorithms are based on the \textit{optimism in the face of uncertainty},
 %provide various approaches to sequential design where an agent adaptively chooses one or more options among several actions based on certain policies. In our work we used the Upper Confidence Bound version of that algorithm, which is based on the \textit{optimism in the face of uncertainty}. The UCB algorithm, as the name suggests,
%  which basically select RBS sequences with the maximum upper confidence bound constructed by the sum of the predicted mean and $n$ standard deviation ($n > 0$), i.e. $\operatorname{argmax}_{\mathbf{x}_i \in \mathcal{D}} \left( \mu_t(\mathbf{x}_i) + \beta_t \sigma_t(\mathbf{x}_i)\right)$,
%     where $\beta_t$ is a hyperparameter balancing the exploitation and exploration,
%     $\mu_t(\mathbf{x}_i), \sigma_t(\mathbf{x}_i)$ are the predicted mean and standard deviation at round $t$ for the sequence $\mathbf{x}_i$.

\subsubsection{Recommendation: Upper Confidence Bound multi-armed Bandit algorithm}

To recommend the RBS sequences to query in the next round, we have used the \textit{Upper Confidence Bound (UCB)} batch algorithm
\cite{lattimore2020bandit}
% \cite{srinivas2012information}
, which provides  $\textit{exploration-exploitation balance}$.
On one hand, UCB exploits the function in terms of the design space, that is to pinpoint sequences that are believed to have high labels (i.e. high predicted mean); 
on the other hand, UCB explores the design space where we have little information and sequences have a chance to have high labels (i.e. high predicted standard deviation).
More precisely, the UCB algorithm selects RBS sequences $\mathbf{x}_i \in \mathcal{D}$ with the maximum upper confidence bound at Round $t$, i.e.
\begin{align}
\label{Eq: GPUCB}
    \operatorname{argmax}_{\mathbf{x}_i \in \mathcal{D}} \left( \mu_{t-1}(\mathbf{x}_i) + \beta_t \sigma_{t-1}(\mathbf{x}_i)\right),
\end{align}
where $\beta_t$ is a hyperparameter balancing the exploration and exploitation, 
$\mu_t(\mathbf{x}_i), \sigma_t(\mathbf{x}_i)$ are the predicted mean and standard deviation (that is uncertainty of the prediction) at Round $t$ for the sequence $\mathbf{x}_i$.
We call $\mu_{t-1}(\mathbf{x}_i) + \beta_t \sigma_{t-1}(\mathbf{x}_i)$ the \textit{UCB score} of sequence $\mathbf{x}_i$ at Round $t-1$.\\

Since experimentally labelling sequences is time-consuming, it is unrealistic to recommend sequences sequentially (i.e. one-by-one) and then wait for the label to be tested and used to improve the model.
Instead, we can recommend RBS sequences in a batch of size $m$.
%One naive approach is to
%train GP on sequences with known TIR labels and
However, this approach may end up recommending similar sequences in the same local maximum (e.g. $x = 2, x =2.5$ in this example). 
Since GPR assumes similar sequences would have similar labels (e.g. by knowing $x=2$ we can gain information about $x=2.5$ as well), we prefer to not waste time and money on labelling sequences with high similarities in the same batch.
To counter this, we use a batch recommendation strategy that is designed to avoid recommending such highly similar sequences in the same batch, described below. \\

% \begin{figure}[t]
% \centering
% \subfloat{\label{main:c}\includegraphics[scale=.4]{plots/Recommend_2_Data_Point_Top_2_UCB.pdf}}\par\medskip
% \begin{minipage}{.5\linewidth}
% \centering
% \subfloat{\label{main:a}\includegraphics[scale=.4]{plots/GP-BUCB_Recommend_1st_Data_Point.pdf}}
% \end{minipage}%
% \begin{minipage}{.5\linewidth}
% \centering
% \subfloat{\label{main:b}\includegraphics[scale=.4]{plots/GP-BUCB_Recommend_2nd_Data_Point.pdf}}
% \end{minipage}
% \caption{Batch Recommendation. We use the batch size of 2, with 5 initial observations. The design (recommendation) space is 24 uniformly distributed points in the range [-2,10], i.e. {-2, -1.5, -1, ..., 9.5, 10}.
% The confidence interval are shown with predicted mean of $\pm$ 1.96 standard deviation.
% (a) Top UCB recommendations. The recommendations are 2 data points with top UCB scores, constructed with GP predictions.
% (b)(c) GP-BUCB recommendations. (b) shows the first recommended sequence, (c) shows the new predicted confidence interval and the second recommendation based on that.}
% \label{fig:batch rec}
% \end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=.4]{plots/Main_Paper/Recommend_2_Data_Points_Top_2_UCB.pdf}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/GP-BUCB_Recommend_1st_Data_Point.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/GP-BUCB_Recommend_2nd_Data_Point.pdf}
    \end{subfigure}
    \caption{\emph{Batch Recommendation Illustration.} We use the batch size of 2, with 5 initial observations. The design space is 24 uniformly distributed points in the range [-2,10], i.e. {-2, -1.5, -1, ..., 9.5, 10}.
    The confidence intervals are shown with predicted mean $\pm$ 1.96 standard deviation.
    (A) Top UCB recommendations. The recommendations are 2 data points with top UCB scores, chosen with GP predictions.
    (B)(C) Batch UCB recommendations. (B) shows the first recommended sequence, (C) shows the new predicted confidence interval and the second recommendation based on that updated interval.}
    \label{fig:batch rec}
\end{figure}


A key property of Gaussian Process regression is that the predicted standard deviation depends only on features, not on the labels. 
% We illustrate this fact by comparing the prediction with the true function value of the new recommended point (Figure \ref{fig: Recommend 1 Data Point (New Observation)}) and the prediction with the predicted value of the new recommended point (Figure \ref{fig: Recommend 1 Data Point (Predicted Mean)}).
% As we can see, the predicted confidence interval is the same for the two plots.
One can make use of this property to design a batch upper confidence bound (BUCB) algorithm \cite{desautels2014parallelizing}.
The strategy here is to recommend RBS sequences one-by-one by sequentially adding the newly recommended RBSs' predicted means (without testing them) into the training data and updating UCB scores.\\

% Figure \ref{fig: Recommend 2 Data Points: GP-BUCB.} shows an example of batch UCB recommendation. 
% The plot shows the predicted mean and standard deviation after observing 4 data points. 
As illustrated in Figure \ref{fig:batch rec}B, the algorithm recommends the data point with maximum UCB score based on the predictions over initial 5 observations.  
We then add the recommended data point ($x = 2$) into the training data set with the predicted mean of that point as the label (note it is not the true label), and update the predicted standard deviation, then we finally update the UCB scores. 
The second data point is then recommended based on the new UCB scores.   
Figure \ref{fig:batch rec}C shows that since we assume we have observed $x = 2$, the new predicted standard deviation of the data points in design space around $x =2$  decreases, so instead of recommending a similar data point $x = 2.5$, the algorithm recommends $x = 8$, which is in another local maximum design area. 
In this way, the batch recommendation can potentially cover more local maximum areas than the sequential design. 
In summary, the exploration efficiency is improved since the recommended sequences in one batch will tend to be in different design areas so that the information gain is maximised. 
In our design, we have set the batch size to 90, to fit the experimental batch.
Finally, we set $\beta_t = 2$ for Round 0-3 and $\beta_t = 0$ for the last round to allow for more exploitation.


\section*{Code, data and material availability}

All code and data required to reproduce the results are available at Github (San Francisco, CA): \url{https://github.com/mholowko/Solaris/tree/master/synbio_rbs}.
All the processed and raw data are included in the repository.
The pBbB6c plasmid is available on Addgene (Watertown, MA). Other strains and plasmids are available on request from the authors; MTAs will need to be negotiated between the parties.


\section*{Author Contribution}
M. Zhang and C. S. Ong  designed and implemented the machine learning algorithms and workflow. M. B. Holowko and H. Hayman Zumpe have designed and performed the laboratory experiments. M. B. Holowko and C. S. Ong conceived and planned the project. All authors analysed the data, contributed to and reviewed the manuscript.

\section*{Competing interests}
The authors declare no competing interests.

\section*{Supporting Information}

A detailed text description of the machine learning methods, including Gaussian Process regression, choices of kernels, batch recommendation, experimental settings, additional results and model evaluation on RBS calculator data; Figures S1-S12; Tables S1-S2. Sequences of plasmids and oligos and assembly reports used in this study.


\section*{Acknowledgments}
The authors would like to acknowlege CSIRO's Machine Learning and Artificial Intelligence, and Synthetic Biology Future Science Platforms for providing funding for this research. The authors would also like to thank CSIRO BioFoundry for help with performing the experiments.


\newpage

\printbibliography

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{paper/plots/Supplementary/TOC.pdf}
    \caption{\textbf{For Table of Contents Only.} }
    \label{fig: TOC}
\end{figure}

\clearpage

\setcounter{figure}{0}
\setcounter{table}{0}
\makeatletter
\renewcommand{\thefigure}{S\@arabic\c@figure}
\renewcommand{\thetable}{S\@arabic\c@table}
\makeatother
\pagenumbering{arabic}% resets `page` counter to 1
\renewcommand*{\thepage}{S\arabic{page}}
\appendix


%! TEX root=./main.tex
\title{Supplementary of Machine learning guided\\
batched design of a bacterial Ribosome Binding Site}
\maketitle

\section{Machine Learning Methods}

In this section, we describe the machine learning methods in details. 

\subsection{Gaussian Process Regression}
\label{supp: GPR}

A \textit{Gaussian process} is a collection of random variables, any finite number of which have a joint Gaussian distribution. 
We define mean function $\mu(\mathbf{x})$  and covariance function $k(\mathbf{x}, \mathbf{x}^\prime)$ of a real process $f(\mathbf{x})$ as
\begin{align}
    \mu(\mathbf{x}) &= \mathbb{E}[f(\mathbf{x})]\\
    k(\mathbf{x}, \mathbf{x}^\prime) &= \mathbb{E}[(f(\mathbf{x}) - \mu(\mathbf{x}))(f(\mathbf{x}^\prime) - \mu(\mathbf{x}^\prime))].
\end{align}

A Gaussian process is specified by its mean function and covariance function as $f(\mathbf{x}) \sim \mathcal{G} \mathcal{P}\left(\mu(\mathbf{x}), k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right)$.
We consider the case where the observations are noisy, i.e. $(X, \mathbf{y}) = \{(\mathbf{x}_i, y_i)| i = 1, \dots, n\}$, where label $y_i = f(\mathbf{x}_i) + \epsilon$ with $\epsilon \sim \mathcal{N}(0, \alpha^2)$. 
The Gaussian noise is independent identically distributed, and the covariance of the prior on the noisy observations is then $\operatorname{cov}\left(y_{p}, y_{q}\right)=k\left(\mathbf{x}_{p}, \mathbf{x}_{q}\right)+\alpha^{2} \delta_{p q}$,
where $\delta_{pq}$ is a Kronecker delta which is one if $p = q$ and zero otherwise.
It is equivalent to a diagonal matrix $\alpha^2 I$ added on the kernel matrix evaluated on the training points.

For $n_\ast$ test points $X_\ast$, we assume the prior over the functions values as a random Gaussian vector $\mathbf{f}_\ast \sim \mathcal{N}(\mathbf{0}, K(X_\ast, X_\ast))$.
Then the joint distribution of the observed target values and the function values at the test points under the prior as 
\begin{align}
    \left[\begin{array}{l}\mathbf{y} \\ \mathbf{f}_{*}\end{array}\right] \sim \mathcal{N}\left(
        \left[\begin{array}{cc}
        \bm{\mu}  \\
        \bm{\mu}_\ast
        \end{array}\right],
    \left[\begin{array}{cc}K(X, X)+\alpha^{2} I & K\left(X, X_{*}\right) \\ K\left(X_{*}, X\right) & K\left(X_{*}, X_{*}\right)\end{array}\right]\right)
\end{align}
where
$\bm{\mu}, \bm{\mu}_\ast$ are mean vectors for $\mathbf{y}, \mathbf{f}_{*}$ respectively,
$K(X, X_\ast)$ denotes the $n \times n_\ast$ covariance/Kernel matrix evaluated at all pairs of training and testing points, similarly for other kernel matrices.
Then the posterior of the test points (i.e. predictive distributions) is given by the conditional distribution $\mathbf{f}_\ast | X, \mathbf{y}, X_\ast \sim \mathcal{N}(\bar{\mathbf{f}}_\ast, cov(\mathbf{f}_\ast))$, where
\begin{align}
   \overline{\mathbf{f}}_{*} & \triangleq \mathbb{E}\left[\mathbf{f}_{*} \mid X, \mathbf{y}, X_{*}\right]=
   \bm{\mu}_\ast + K\left(X_{*}, X\right)\left[K(X, X)+\alpha^{2} I\right]^{-1} (\mathbf{y} - \bm{\mu})\\
   \label{Eq: predicted variance}
   \operatorname{cov}\left(\mathbf{f}_{*}\right) &=K\left(X_{*}, X_{*}\right)-K\left(X_{*}, X\right)\left[K(X, X)+\alpha^{2} I\right]^{-1} K\left(X, X_{*}\right) 
\end{align}
For noisy test targets $\mathbf{y}_\ast = f(\mathbf{x}_\ast) + \epsilon$ with $\epsilon \sim \mathcal{N}(0, \alpha^2)$, we can compute the predictive distribution by adding $\alpha^2 I$ to the variance term $cov(\mathbf{f}_\ast)$ in Eq. (\ref{Eq: predicted variance}),
i.e. $\overline{\mathbf{y}}_{*} = \overline{\mathbf{f}}_{*}$, $\operatorname{cov}\left(\mathbf{y}_{*}\right) = \operatorname{cov}\left(\mathbf{f}_{*}\right) + \alpha^2 I$.


% We now introduce the marginal likelihood (or evidence) $p(\mathbf{y}|X)$, is which the integral of the likelihood times the prior 
% \begin{align}
%     p(\mathbf{y} \mid X)=\int p(\mathbf{y} \mid \mathbf{f}, X) p(\mathbf{f} \mid X) d \mathbf{f}
% \end{align}


\subsection{Choices of Kernels}
\label{sec: Choices of Kernels}

The choice of covariance function is critical for the performance of Gaussian process regression. 
Except the weighted degree kernel with shift described in Section \ref{sec: method prediction with kernel}, we show two closely related string kernels tested in this study below.

\begin{itemize}
    \item \textit{Spectrum Kernel.}
    \begin{align}
        k_\ell^{\text{Spec}}(\mathbf{x}, \mathbf{x}^\prime) =\left\langle\phi_{\ell}^{\mathrm{Spec}}(\mathbf{x}), \phi_{\ell}^{\mathrm{Spec}}\left(\mathbf{x}^{\prime}\right)\right\rangle = \phi_{\ell}^{\mathrm{Spec}}(\mathbf{x})^T \phi_{\ell}^{\mathrm{Spec}}\left(\mathbf{x}^{\prime}\right).
    \end{align}
     where $\mathbf{x}, \mathbf{x}^\prime$ are two RBS sequences in $\mathcal{D}$ over an alphabet $\Sigma$. We denote the number of letters in the alphabet as $|\Sigma|$. 
     In our case, $\Sigma = \{A,C,G,T\}$ and $|\Sigma| = 4$.
    $\phi_{\ell}^{\mathrm{spec}}(\mathbf{x})$ maps the sequence $\mathbf{x}$ into a $|\Sigma|^\ell$ dimensional feature space, where each dimension is the count of the number of one of the $|\Sigma|^\ell$ possible strings $s$ of length $\ell$. 
    For example, if $\mathbf{x} = ACACAG$ and $\ell = 3$, then $\phi_{\ell}^{\mathrm{spec}}(\mathbf{x})$ maps $\mathbf{x}$ into $4^3$ dimensional feature space, where each dimension corresponds to $AAA, AAC, AAT, AAG, ACA, ..., TTT$. Among those, the dimension corresponds to $ACA, CAC, CAG$ are 2,1,1 and others are 0. 
    Let $X, X^\prime$ be two metrics which include $n$ sequences, and $\Phi_\ell^{Spec}(X) \in \mathbb{R}^{n \times |\Sigma|^{\ell}}$, then the spectrum kernel over metrics is 
    \begin{align}
         K_\ell^{\text{Spec}}(X, X^\prime) = \Phi_{\ell}^{\mathrm{Spec}}(X) \Phi_{\ell}^{\mathrm{Spec}}\left(X^{\prime}\right)^T.
    \end{align}
    
    % \item \textit{Sum of Spectrum Kernel,} considers weighted sum over different parts of the string. 
    
    % \item \textit{Mixed Spectrum Kernel,} considers weighted sum over different substring length, with $\beta_d = \frac{2(\ell - d + 1)}{\ell(\ell+1)}$,
    %     \begin{align}
    %         k_\ell^{MixedSpec}(\mathbf{x}, \mathbf{x}^\prime) 
    %         = \sum_{d=1}^{\ell} \beta_d k_d^{Spec}(\mathbf{x}, \mathbf{x}^\prime)
    %     \end{align}
    \item \textit{Weighted Degree Kernel,} considers positional information. WD kernel counts the match of kmers at corresponding positions in two sequences.
    For sequences with fixed length $L$ and weighted degree kernel considers substrings starting at each position $l = 1, ..., L$, with $\beta_d = \frac{2(\ell - d + 1)}{\ell(\ell+1)}$, \\
    \begin{align}
        k_\ell^{WD}(\mathbf{x}, \mathbf{x}^\prime) 
        &= \sum_{d=1}^{\ell} \beta_d \sum_{l=1}^{L-d+1} \gamma_l k_d^{Spec}(\mathbf{x}_{[l:l+d]}, \mathbf{x}_{[l:l+d]}^\prime)\\
        &= \sum_{d=1}^{\ell} \beta_d \sum_{l=1}^{L-d+1} \gamma_l \phi_d^{Spec}(\mathbf{x}_{[l:l+d]})^T \phi_d^{Spec}(\mathbf{x}_{[l:l+d]}^\prime)\\
        &= \sum_{d=1}^{\ell} \beta_d \sum_{l=1}^{L-d+1} \gamma_l \mathbb{I}(\mathbf{x}_{[l:l+d]} = \mathbf{x}_{[l:l+d]}^\prime),
    \end{align}
    where $\mathbb{I}(\text{true}) = 1$ and 0 otherwise. 
    
    % \item \textit{Weighted Degree Kernel With Shift.}
    % \begin{align}
    %     k_\ell^{WDS}(\mathbf{x}, \mathbf{x}^\prime) 
    %     &= \sum_{d=1}^{\ell} \beta_d \sum_{l=1}^{L-d+1} \gamma_l \sum_{s = 0, s + l \leq L}^{S(l)} \delta_s
    %     \left(k_d^{Spec}(\mathbf{x}_{[l+s:l+s+d]}, \mathbf{x}_{[l:l+d]}^\prime) + (k_d^{Spec}(\mathbf{x}_{[l:l+d]}, \mathbf{x}_{[l+s:l+s+d]}^\prime)\right)\\
    %     &= \sum_{d=1}^{\ell} \beta_d \sum_{l=1}^{L-d+1} \gamma_l \sum_{s = 0, s + l \leq L}^{S(l)} \delta_s
    %     \left(\mathbb{I}(\mathbf{x}_{[l+s:l+s+d]} = \mathbf{x}_{[l:l+d]}^\prime) + (\mathbb{I}(\mathbf{x}_{[l:l+d]}= \mathbf{x}_{[l+s:l+s+d]}^\prime)\right),
    % \end{align}
    % where $\beta_d = \frac{2(\ell - d + 1)}{\ell(\ell+1)}, \delta_s = \frac{1}{2(s+1)}$, $\gamma_l$ is a weighting over the position in the
    % sequence, where we choose to use a uniform weighting over the sequences, i.e. $\gamma_l = 1/L$. $S(l)$ determines the shift
    % range at position $l$.
\end{itemize}

% \textbf{From kernel to distance}:
% $$d(\mathbf{x}, \mathbf{x}^\prime) = \sqrt{k(\mathbf{x}, \mathbf{x}) + k(\mathbf{x}^\prime, \mathbf{x}^\prime) - 2 k(\mathbf{x}, \mathbf{x}^\prime)} $$

\subsubsection{Normalisation of Kernel}
\label{supp: Normalisation of Kernel}

As part of data pre-processing,
the range of all features should be normalised so that each feature contributes approximately proportionately to the predictive model. 
The kernel matrix is represented by the inner product of the underlying feature vectors, it needs to be normalised before being used in the downstream regression models. 
Up-scaling (down-scaling) features can be understood as down-scaling (up-scaling) regularizers such that they penalise the features less (more). 

Here we consider two approaches for kernel normalisation: centering and unit norm. 
We will show how to convert the normalisation in terms of feature vectors to normalisation in terms of kernel matrices. 
As defined before, consider $\mathbf{x}, \mathbf{x}^\prime$ are two RBS sequences in $\mathcal{D}$ over an alphabet $\Sigma$.
We denote $\phi(\mathbf{x})$ as a column feature vector of sequence $\mathbf{x}$, 
where a feature function $\phi: \mathbf{x} \rightarrow \mathbb{R}^d$,
with $d$ as the dimension of features.
One example of $\phi$ can be $\phi_\ell^{spec}(\mathbf{x})$ in the spectrum kernel.
Recall the corresponding kernel can be defined as $k(\mathbf{x}, \mathbf{x}^\prime) = \phi(\mathbf{x})^T \phi(\mathbf{x}).$

Assume there is total of $n$ sequences in the data $X$ ($n'$ sequences in the data $X'$). 
We illustrate centering and unit norm normalisation below. 

\begin{itemize}
    \item Centering. 
    Defining the mean vector as $\bar{\Phi}(X) = \frac{1}{n} \sum_{s = 1}^n \phi(\mathbf{x}_s) \in \mathbb{R}^d$, the centered feature vector $\phi^C(\mathbf{x}_i) \in \mathbb{R}^d$ of $\mathbf{x}_i$ is
    \begin{align}
        \phi^{C}(\mathbf{x}_i) = \phi(\mathbf{x}_i) - \bar{\Phi}(X) = \phi(\mathbf{x}_i) - \frac{1}{n'} \sum_{s = 1}^{n'} \phi(\mathbf{x}_s).
    \end{align}
    The corresponding centering kernel value between $\mathbf{x}_i$ and $\mathbf{x}_j$ is then 
    \begin{align}
        k^C(\mathbf{x}_i, \mathbf{x}_j) =& <\phi^C(\mathbf{x}_i), \phi^C(\mathbf{x}_j)>\\
        =& \left( \phi(\mathbf{x}_i) - \frac{1}{n} \sum_{s = 1}^n \phi(\mathbf{x}_s)\right)^T \left( \phi(\mathbf{x}_j) - \frac{1}{n'} \sum_{s' = 1}^{n'} \phi(\mathbf{x}_{s'})\right)\\
        =& \phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j) - \left( \frac{1}{n} \sum_{s = 1}^n \phi(\mathbf{x}_s)\right)^T \phi(\mathbf{x}_j) - \phi(\mathbf{x}_i)^T \left(\frac{1}{n} \sum_{s' = 1}^{n'} \phi(\mathbf{x}_{s'})\right) +\\
        &\left( \frac{1}{n} \sum_{s = 1}^n \phi(\mathbf{x}_s)\right)^T \left(\frac{1}{n'} \sum_{s' = 1}^{n'} \phi(\mathbf{x}_{s'})\right)\\
        =& k(\mathbf{x}_i, \mathbf{x}_j) - \frac{1}{n} \sum_{s=1}^n k(\mathbf{x}_s, \mathbf{x}_j) - \frac{1}{n'} \sum_{s'=1}^{n'} k(\mathbf{x}_i, \mathbf{x}_{s'}) + \frac{1}{n^2} \sum_{s = 1}^n \sum_{s'=1}^{n'} k(\mathbf{x}_s, \mathbf{x}_{s'})
    \end{align}
    
    \item Unit Norm. Define the ($l_2$) norm of a feature vector as $||\phi(\mathbf{x})|| = \sqrt{\sum_{m = 1}^d \phi_m(\mathbf{x})^2} = \sqrt{k(\mathbf{x}, \mathbf{x})} \in \mathbb{R}^+$, then the unit norm feature vector $\phi^{UN}(\mathbf{x}_i) \in \mathbb{R}^d$ of $\mathbf{x}_i$ is 
    \begin{align}
        \phi^{UN}(\mathbf{x}_i) = \frac{\phi(\mathbf{x}_i)}{||\phi(\mathbf{x}_i)||}.
    \end{align}
    The corresponding unit norm kernel value between $\mathbf{x}_i$ and $\mathbf{x}_j$ is then 
    \begin{align}
        k^{UN}(\mathbf{x}_i, \mathbf{x}_j) &= <\frac{\phi(\mathbf{x}_i)}{||\phi(\mathbf{x}_i)||}, \frac{\phi(\mathbf{x}_j)}{||\phi(\mathbf{x}_j)||}>\\
        &= \frac{\phi(\mathbf{x}_i)^T \phi(\mathbf{x}_j)}{||\phi(\mathbf{x}_i)|| \times ||\phi(\mathbf{x}_j)||}\\
        &= \frac{k(\mathbf{x}_i, \mathbf{x}_j)}{\sqrt{k(\mathbf{x}_i, \mathbf{x}_i)  k(\mathbf{x}_j, \mathbf{x}_j)}}
    \end{align}
    
     \item Unit Variance. 
    After the centering and unit norm normalisation, the kernel matrix is unit variance as well. 
    In the following, we show transformations of the unit variance (with centering) normalisation.
    Define the variance vector ${Var}(\Phi(X)) = \frac{1}{n} \sum_{s=1}^n ||\phi(\mathbf{x}_s) - \bar{\Phi}(X)||^2 = \frac{1}{n} \sum_{s=1}^n ||\phi(\mathbf{x}_s) - \sum_{s'=1}^n \left(\phi(\mathbf{x}_s')\right)||^2 = \frac{1}{n} \sum_{s=1}^n  k^C(\mathbf{x}_s, \mathbf{x}_s)  \in \mathbb{R}$, the unit variance feature vector $\phi^{UV}(\mathbf{x}_i) \in \mathbb{R}^d$ of $\mathbf{x}_i$ is
    \begin{align}
        \phi^{UV}(\mathbf{x}_i) = \frac{\phi(\mathbf{x}_i)}{\sqrt{Var(\Phi(X))}}.
    \end{align}
    The corresponding kernel representation is 
    \begin{align}
        k^{UV}(\mathbf{x}_i, \mathbf{x}_j) &= <\frac{\phi(\mathbf{x}_i)}{\sqrt{Var(\Phi(X))}}, \frac{\phi(\mathbf{x}_j)}{\sqrt{Var(\Phi(X'))}}>\\
        &= \frac{\phi(\mathbf{x}_i)^T \mathbf{x}_j}{\sqrt{Var(\Phi(X)) Var(\Phi(\mathbf{X'}))}}\\
        &= \frac{k(\mathbf{x}_i, \mathbf{x}_j)}{\sqrt{ \frac{1}{n} \sum_{s=1}^n  k^C(\mathbf{x}_s, \mathbf{x}_s)  \frac{1}{n} \sum_{s'=1}^{n'}  k^C(\mathbf{x}_{s'}, \mathbf{x}_{s'})}}
    \end{align}
    After centering and unit norm, $ \frac{1}{n} \sum_{s=1}^n  k^C(\mathbf{x}_s, \mathbf{x}_s) = k(\mathbf{x}_i, \mathbf{x}_i)$, which implies that after centering and unit norm, the kernel matrix is already unit variance normalised. 
\end{itemize}
For the Gaussian Process regression, we make of use of two kernel matrices: the kernel function between the training data itself, i.e. $K(X_{train}, X_{train})$; and
the kernel function taking the training data and testing data as inputs, i.e. $K(X_{test}, X_{train})$. 
%It is straightforward to normalise a square kernel which same input, i.e. $n = n'$ and $k(\mathbf{x}_i, \mathbf{x}_i),  k(\mathbf{x}_j, \mathbf{x}_j)$ taken from the diagonal of the matrix. 
%The second one (between train and test) is a little bit tricky. The different is not only that $n \neq n'$. 
We will state two ways of normalisation those two kind of matrices:
\begin{itemize}
    \item Normalise training and testing data separately.
    This approach is preferred for most of the machine learning algorithms since it follows the rule that we have no information about testing data while training.
    Then for centering, one should subtract the mean vector over the training data for both kinds of matrices.
    For unit norm normalisation, when one calculates $K^{UN}(X_{test}, X_{train})$, the two terms inside of square root: $k(\mathbf{x}_i, \mathbf{x}_i)$ is taken from $K(X_{test}, X_{test})[i,i]$, and $k(\mathbf{x}_j, \mathbf{x}_j)$ is taken from $K(X_{train}, X_{train})[j,j]$.
    
    \item Normalise training and testing data together, i.e. normalise $K(X_{train+test}, X_{train+test})$, then extract the parts we need from the normalised matrix. 
    This approach is suitable in a case where one already knows the whole of testing features. 
    For centering, one should subtract the mean vector over the whole matrix $\Phi(X_{train+test})$. 
    The unit norm normalisation is the same as in the previous case. 
\end{itemize}

For our experiment, we fix the design space before training, i.e. the testing features are already known before testing. 
So we choose to normalise the kernel matrix over the training and testing data together (for Round 2-3, normalise over all RBS sequence in the design space),
by first applying centering and then unit norm normalisation. 

\subsection{Batch Recommendation}

We consider recommending sequences in batch and using Gaussian Process Batch Upper Confidence Bound (GP-BUCB) algorithm  \cite{desautels2014parallelizing}.
We show technical details of GP-BUCB in the following.
With batches of size $B$, the feedback mapping $fb[t] = \lfloor(t-1) / B\rfloor B$, i.e. 
\begin{align}
    \mathrm{fb}[t]=\left\{\begin{array}{cl}
    0 & : t \in\{1, \ldots, B\} \\
    B & : t \in\{B+1, \ldots, 2 B\} \\
    2 B & : t \in\{2 B+1, \ldots, 3 B\} \\
    & \vdots
    \end{array}\right.
\end{align}


A key property of Gaussian Process regression is that the predictive variance in Eq. (\ref{Eq: predicted variance}) only depends on observed points (i.e. features), but not on the labels of these observed points. 
So one can compute the posterior variance without actually observing the labels. 
The GP-BUCB policy is to select sequences that
\begin{align}
    \operatorname{argmax}_{\mathbf{x}_i \in \mathcal{D}} \left( \mu_{fb[t]}(\mathbf{x}_i) + \beta_t \sigma_{t-1}(\mathbf{x}_i)\right).
\end{align}
And only update $y_{t^{\prime}}=f\left(\boldsymbol{x}_{t^{\prime}}\right)+\varepsilon_{t^{\prime}} \text { for } t^{\prime} \in\{\mathrm{fb}[t]+1, \ldots, \mathrm{fb}[t+1]\}$ at the end of each batch ($\mathrm{fb}[t]<\mathrm{fb}[t+1]$). 
This is equivalent to sequential GP-UCB with \textit{hallucinated observations} $$\boldsymbol{y}_{\mathrm{fb}[t]+1: t-1}=\left[\mu_{\mathrm{fb}[t]}\left(\boldsymbol{x}_{\mathrm{fb}[t]+1}\right), \ldots, \mu_{\mathrm{fb}[t]}\left(\boldsymbol{x}_{t-1}\right)\right],$$
while the posterior variance decreases. 

\subsection{Bandit-0 Design}
\label{sec: ML design pipeline}
% \mengyan{explain somewhere why we have a different pipeline for round 0.}
% \mengyan{add some analysis on literature data?
% to verify 1) our library is "better" 2) it's hard to do ML on available library. some analysis in my mind (for literature data): number of data points, number of replicates (I remembered there only one replicates, if not, then std), label histogram, coverage of RBS.
% And we need to address the label in literature data and ours is "different", in the way that our pipeline is different for bandit-0 design}

The design of Round-0 is based on the literature data \cite{jervis2018machine}.
We first normalise the raw TIR to values between 0 and 1. 
We applied the Gaussian Process Regression with noise parameter $\alpha = 1e-10$. 
We chose to use one of the basic string kernels, the \textit{spectrum kernel} \cite{leslie2001spectrum} to process the core 6bp and dot product kernel \cite{Rasmussen2004} (with one-hot embedding) to process the 7bp flanking sequences both upstream and downstream of the core sequence.
The design size is 60 with UCB parameter $\beta = 1$.

\subsection{Additional Results}

As stated in Section \ref{subsec: results LEARN}, we merely aimed to provide a valid ranking in Learn part due to the small amount of noisy data, which can be captured by the Spearman correlation coefficient. 
We show $R^2$ score for the reader's interest, which quantifies value-based prediction evaluation. The $R^2$ increases along with rounds increase. 

\begin{table}[h!]
\caption{$R^2$ score for predictions in Figure \ref{fig: Scatterplot}.}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
                  & \textbf{\begin{tabular}[c]{@{}c@{}}Train 0\\ Test 1\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Train 0-1\\ Test 2\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Train 0-2\\ Test 3\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Train 80\%\\ Test 20\%\end{tabular}} \\ \hline
\textbf{$R^2$}       & 0.067                                                             & 0.226                                                               & 0.213                                                               & 0.269                                                                   \\ \hline
\textbf{Spearman} & 0.269                                                             & 0.468                                                               & 0.546                                                               & 0.479                                                                   \\ \hline
\end{tabular}
\end{table}

\subsection{Exploration-exploitation visualisation}
\label{subsec: Exploration-exploitation visualisation}

In this section, we show our method of exploration-exploitation visualisation displayed in Figure \ref{fig: Swarmplot and Quantplot}C.
The UCB score of a sequence $\mathbf{x}_i$ at Round $t$ is defined as 
\begin{align*}
% \label{Eq: UCB}
    \mu_{t-1}(\mathbf{x}_i) + \beta_t \sigma_{t-1}(\mathbf{x}_i),
\end{align*}
where $\beta_t$ is a hyperparameter balancing the exploration and exploitation, 
$\mu_{t-1}(\mathbf{x}_i), \sigma_{t-1}(\mathbf{x}_i)$ are the predicted TIR and standard deviation (STD) at Round $t-1$ for the sequence $\mathbf{x}_i$.
The sequences with high predicted TIR address exploitation, while the sequences with high predicted STD address exploration. 
We show the predicted TIR (x-axis) v.s. predicted STD (y-axis) in Figure \ref{fig:exploitationvsexplorationdetail} as orange points. 

To compare the predicted TIR and STD in the same dimension (basis), we project the (predicted TIR, predicted STD) pair to the diagonal line (the grey dashline) set between (0,1) and (1,0) coordinates.
The slope of the diagonal line is selected to weight the predicted TIR and STD equally. 
Each point on the diagonal then, corresponds to an orthogonal projection (known as \emph{vector projection}) of an orange point.
As a result, the higher the predicted TIR the closer the point is to the (1,0) coordinate, and the higher the predicted STD the closer the point is to the (0,1) coordinate.
Finally, we colour those projected points using the RdBu spectrum, where the points with high STD (exploration) are shown in red and those with high mean TIR (exploitation) are shown in blue.
Finally, these colours correspond to ones shown in Figure \mbox{\ref{fig: Swarmplot and Quantplot}C}.

% \mengyan{mengyan to re-write it}
% For a given round all the tested sequences are then projected along the diagonal line crossing (0,1) and (1,0) according to their predicted STD and mean TIR (the higher the predicted mean the closer the point is to the (1,0) coordinate and the higher the predicted STD the closer the point is to the (0,1) coordinate).
% Each point on the diagonal then, corresponds to an orthogonal projection (known as \emph{vector projection}) of an orange point.
% We do this to transform our predictions into a common basis (where the basis is the diagonal) and so by calculating where a given point falls on the diagonal we can show if its mean TIR or STD is higher.
% Finally, we colour those projected points using the RdBu spectrum where the points with high STD (exploration) are shown in red and those with high mean TIR (exploitation) are shown in blue.

   
\begin{figure}[!ht]
   \centering
    \includegraphics[scale=0.35]{plots/Supplementary/proj_scatter_abc1_FF_0.pdf}
    \includegraphics[scale=0.35]{plots/Supplementary/proj_scatter_abc1_FF_1.pdf}
    \includegraphics[scale=0.35]{plots/Supplementary/proj_scatter_abc1_FF_2.pdf}
   \caption{\textbf{Exploration-Exploitation scores illustration.} We show the predictions for each sequence from Round 1-3 in orange (with the same settings as Figure \mbox{\ref{fig: Scatterplot}}) and their corresponding projections to (0,1), (1,0) diagonal coloured in RdBu spectrum.
    We explain in details how the colours are generated in Section \ref{subsec: Exploration-exploitation visualisation} .
   }
   \label{fig:exploitationvsexplorationdetail}
\end{figure}

\subsection{Model evaluation on RBS calculator data}
\label{subsec: Model evaluation on RBS calculator data}

To test the reliability of our proposed model, we have generated TIR predictions for all 4096 RBS sequences using the RBS  calculator ver. 2.0 \mbox{\cite{Salis2009}} and evaluated our model using that data.
We normalised the TIR values by dividing them by the TIR of the benchmark sequence, and show the TIR ratio mapping between the RBS calculator and TIRs obtained experimentally for this work in Figure \mbox{\ref{fig:TIR ratio salis micro}}.
We observe that the RBS calculator values are not well aligned to the measured values, which underlines the need for development of new prediction and design methods. 
Among all 4096 RBS sequences, 38 RBS were predicted to have higher TIR than the benchmark sequence by the RBS calculator. We call those sequences the \textit{optimal set}, and the goal is to find as many sequences from the optimal set as permitted by the budget (450).

To do this, we have applied our approach to this data set - first we have chosen 90 sequences at random and after teaching the algorithm using the obtained numbers we have generated 90 further sequences to be tested (using UCB for recommendations) and did this for a total of 4 rounds.
We have repeated this experiment 100 times to increase our confidence in the results, by doing so we have found that our model can identify above 75\% of the optimal set in up to 2 rounds and 99\% of them in up to 3 rounds.
We reported the detailed statistics for the 100 runs in Table \mbox{\ref{table: Important statistics for the 100 synthetic experiments}}.
Figure \mbox{\ref{fig: Swarmplot and Quantplot Salis}} shows plot analogues to these in Figure \mbox{\ref{fig: Swarmplot and Quantplot}}  but for one of the runs using the RBS calculator data.
Our algorithm has successfully found most of the sequences in optimal set in the first and second rounds, thus the recommended TIRs have an apparent drop in the following rounds (as there are no good sequences left to recommend).
This might suggest that it might be valuable to add an additional stopping condition for the DBTL cycle - decreasing round performance.

\begin{figure}
    \centering
    \includegraphics[scale = 0.6]{paper/plots/Supplementary/TIR_ratio_salis_micro.png}
    \caption{TIR ratio of RBS calculator ver. 2.0 \cite{Salis2009} plotted against TIR ratios obtained experimentally in this work. The red point represents the benchmark sequence.}
    \label{fig:TIR ratio salis micro}
\end{figure}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Supplementary/swarmplot_salis.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Supplementary/quantplot_salis.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Supplementary/swarmplot_proj_salis.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Supplementary/histogram_salis.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.42]{plots/Supplementary/tsneplot_salis.pdf}
    \end{subfigure}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/quantplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot_proj.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/histogram.pdf}
    % \includegraphics[scale=0.4]{plots/Main_Paper/tsneplot.pdf}
    \caption{
    \textbf{Model evaluation on RBS calculator data.}
    \textbf{A)} Swarm plot showing the obtained TIRs divided into RBS groups.
    Random: randomly chosen sequences.
    Bandit-0/1/2/3 - Bandit algorithm generated results for Round 0, 1, 2 and 3 respectively.
    Dashed line is set to TIR of the benchmark sequence.
    \textbf{B)} Line plot showing TIR obtained in a given quantile (Q) of results divided into groups as in A)
    \textbf{C)} Exploitation v.s. Exploration for Bandit 0-3. Blue-hued points represent exploitation, those hued red represent exploration.
    \textbf{D)} Histogram with kernel density estimations (KDE) showing distributions of TIRs for Bandit groups.
    \textbf{E)} t-SNE plot showing the relative distances between sequences in our design spaces as calculated by our kernel function (weighted degree kernel with shift).
    The Unlabeled represents the RBS sequences in design space but have not been tested.
    The area of the circle corresponds to the TIR value.
    The TIR results in all subplots are shown normalised to the  benchmark sequence.}
    \label{fig: Swarmplot and Quantplot Salis}
\end{figure}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Supplementary/scatter_abc1_FF_0_salis.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Supplementary/scatter_abc1_FF_1_salis.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Supplementary/scatter_abc1_FF_2_salis.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Supplementary/scatter_abc1_FF_3_salis.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Supplementary/salis_scatter_abc1_FF_4.pdf}
    \end{subfigure}
    \caption{\textbf{Performance of the prediction algorithm (no kernel normalisation) on the RBS calculator data}. The scatter plots A-D show the performance of our prediction algorithm calculated after each round using the RBS calculator data.
    E shows the performance on all collected data.
    Note that the TIR values are normalised according to the standardisation described in section \ref{sec: method data pre-procesing}, which is different from the TIR ratio reported in Figure \ref{fig: Swarmplot and Quantplot}.
    X-axis and y-axis are the TIR predicted by the RBS calculator and the TIR predicted by our model respectively.
    The Spearman correlation coefficient (with corresponding p-value; calculated using test data only) are provided in each plot's title on test data only.
    The p-value here is for the null hypothesis stating that two sets of data are uncorrelated.
    }
    \label{fig: Scatterplot_Salis}
\end{figure}

\begin{table}[h!]
\caption{\textbf{Important statistics for the 100 synthetic experiments run using the RBS calculator data.}
Each round contains 90 samples, where Round 0 is uniformly random sampled from the whole space, and Rounds 1-4 follow the bandits algorithm. 
In column \textit{Percent of the Optimal Set Found} and \textit{Standard Deviation}, we reported the average percent of the \textit{optimal set} found in each round and the standard deviation of this value averaged over the 100 experiments. 
In the last column, we reported the average percent of the (\textit{optimal set}) found in total after each round. 
We can observe that our algorithm can find the majority of the (\textit{optimal set}) sequences within three rounds.
}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
                  \textbf{Round} & \textbf{\begin{tabular}[c]{@{}c@{}}Percent of\\ the Optimal Set Found\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Standard\\ Deviation\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Total Percent of\\ the Optimal Set Found\end{tabular}} \\ \hline
 \textbf{0}  & 0.022  & 0.026 & 0.022  \\ \hline
  \textbf{1}  & 0.744  & 0.085 & 0.766  \\ \hline
   \textbf{2}  & 0.222  & 0.077 & 0.988  \\ \hline
    \textbf{3}  & 0.012  & 0.026 & 1.000  \\ \hline
     \textbf{4}  & 0.000  & 0.000 & 1.000  \\ \hline

\end{tabular}
\label{table: Important statistics for the 100 synthetic experiments}
\end{table}


\newpage

\section{Supplementary Plots}

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.4]{plots/Supplementary/core_vs_noncore.pdf}
    \caption{\textbf{Comparison of base change impact on TIR in core versus non-core region.} 
    The core region is highlighted in light green and the lines are rolling averages (in this case mean of the 5 points centred over the indicated position) for each base.
    The top dotted line shows the TIR for the benchmark sequence, where dots represent a change at a given position to a given base, which is colour coded.
    Since only changes from the original base at each position are shown, the dotted lines start and end at different positions depending on how many changes to the respective base are at positions adjacent to the indicated one.
    The value of Welch's t-test between the mean TIR in core and non-core groups is -4.8780 with p-value $<$ 0.0001 and 34 degrees of Freedom.}
    \label{fig:core_vs_noncore}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[scale = 0.4]{plots/Supplementary/swarmplot_raw.pdf}
    \caption{
    \textbf{Raw TIRs of RBS groups examined in this study.}
    Raw TIR is calculated as a derivative of GFP fluoresence divided by OD600 of culture over 4h counting from the start of log phase of growth. 
    BPS-NC: base-by-base changes in the non-core region. 
    BPS-C: base-by-base changes in the core region. 
    UNI: Randomly generated sequences with uniform distribution. 
    PPM: Randomly generated sequences with distribution following the PPM for all natural RBS in \emph{E. coli}. 
    Bandit-0/1/2/3 - Bandit algorithm generated results for Round 0, 1, 2 and 3 respectively.
    SD - Shine-Dalgarno sequence.
    BN - benchmark sequences for all plates. 
    }
    \label{fig:rawswarmplots.}
\end{figure}

\begin{figure}[!ht]
   \centering
    \includegraphics[scale=0.5]{plots/Supplementary/tsneplot_hamming.pdf}
   \caption{\textbf{t-SNE plot based on Hamming distances.} This t-SNE plot has been drawn based on Hamming distances between sequences. 
    }
   \label{fig: TSNE hamming}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[scale = 0.35]{plots/Supplementary/scatter_bc1_TTF_0.pdf}
    \includegraphics[scale = 0.35]{plots/Supplementary/scatter_abc1_TT_1.pdf}
    \includegraphics[scale = 0.35]{plots/Supplementary/scatter_abc1_TT_2.pdf}
    % \includegraphics[scale = 0.4]{plots/Supplementary/scatter_abc1_TT_3.pdf}
    \caption{\textbf{Predictions in design pipeline in Round 1-3.} For Round 1, the kernel matrix is normalised based on known RBS sequences in Round 0.
    For Round 2-3, the kernel matrix is normalised based on all RBS sequences in the design space.
    Compared to the results shown in Figure \mbox{\ref{fig: Scatterplot}}, here we have applied kernel normalisation described in Section \mbox{\ref{supp: Normalisation of Kernel}}. Although the precise value prediction looks different from Figure \mbox{\ref{fig: Scatterplot}}, the rankings provided in two figures are similar, which can be reflected by the similar Spearman correlation coefficients.
    }
    \label{fig:scatter abc1 TT.}
\end{figure}

% \begin{figure}[!ht]
%     \centering
%     \includegraphics[scale = 0.3]{plots/Supplementary/Data_Similarity_and_Prediction_Sorted_by_TIR_Label.pdf}
%     \includegraphics[scale = 0.3]{plots/Supplementary/Data_Similarity_and_Prediction_Sorted_by_RBS_Similarity.pdf}
%     \caption{Kernel Heatmap and Predictions. Sequences are grouped as Consensus (1-2), BPS-C (3-20), BPS-NC (21-61), UNI (62-90), PPM (91-118), Bandit-0 (119-177), Bandit-1 (178-265). Inside of each group, sequences are clustered and sorted in terms of TIR labels (left) or RBS similarity (right). The first row shows the similarity measured by weighted degree kernel with shift, the second shows the predicted mean and uncertainty (1.95 standard deviation).}
% \end{figure}

\begin{figure}[!ht]
   \centering
   \includegraphics[width=0.5\textwidth]{plots/Supplementary/All_logo.pdf}
   \caption{\textbf{Sequence logo for all tested sequences.} Compared to Figure \ref{fig:Library characteristics}A there are no significant biases at each position.}
   \label{fig: All_logo}
\end{figure}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.25]{plots/Supplementary/Structure_GGGGGG.pdf}
        \label{fig:GGGGGG}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.25]{plots/Supplementary/Structure_GGGGGC.pdf}
        \label{fig:GGGGGC}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.25]{plots/Supplementary/Structure_CGGGAC.pdf}
        \label{fig:CGGGAC}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.25]{plots/Supplementary/Structure_CACGTT.pdf}
        \label{fig:CACGTT}
    \end{subfigure}
       \caption{\textbf{Folding predictions for four different RBS.} Here we show the predicted, energetically most favourable structures for four of our RBS with the following cores: GGGGGG, GGGGGC, CGGGAC, CACGTT and their immediate upstream and downstream background sequence. There is no discerbile and consistent difference between strong ones (the first two) and weak ones (the last two).}
       \label{fig:structures}
\end{figure}



\begin{figure}[!ht]
   \centering
    \includegraphics[scale=0.4]{plots/Supplementary/SDhist.pdf}
   \caption{\textbf{Histogram of standard deviations.}
   For each RBS sequence, we tested 6 biological replicates. This plot shows the histogram of the STD of raw TIR values of the 6 replicates for each RBS sequence. }
   \label{fig: SDhist}
\end{figure}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Supplementary/SD.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.6]{plots/Supplementary/SDvsTIR.pdf}
    \end{subfigure}
    \caption{\textbf{Distribution of standard deviation of tested samples} A) Joint plot showing the distribution of relative error calculated as the average TIR from 6 replicates divided by the given samples standard deviation (coefficient of variation). B) Scatter plot showing the standard deviation for each RBS plotted against its averaged TIR. Standard deviation and average TIR is given in terms of raw number.}
    \label{fig:variation of biological replicates}
\end{figure}



\end{document}
% However large data sets may not be available for a genetic part of interest, for instance the RBS. Hence there is a need to develop methods for training predictors on smaller data sets in the Learn phase of DBTL.\\
