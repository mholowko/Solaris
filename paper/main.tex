\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[maxcitenames=1,style=numeric, sorting = none]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk}
\usepackage{soul}
\usepackage{bm}
\usepackage{lineno}

% \usepackage[table,xcdraw]{xcolor}
% \usepackage{subfloat}

\linenumbers
\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in


\newcommand{\cheng}[1]{\textcolor{purple}{{\bf Cheng:~}#1}}
\newcommand{\mengyan}[1]{\textcolor{magenta}{#1}}
\newcommand{\maciej}[1]{\textcolor{blue}{#1}}

% \captionsetup[subfigure]{position=top, labelfont=bf,textfont=normalfont,singlelinecheck=off,justification=raggedright}
\captionsetup[subfigure]{font={bf,small}, skip=1pt, singlelinecheck=false}
\renewcommand{\thesubfigure}{\Alph{subfigure}}

\addbibresource{ref.bib}

%\title{Machine Learning guided Design\\
%of Ribosome Binding Sites}

\title{Multi-armed bandit \mengyan{(Bayesian?)} optimisation\\
of bacterial Ribosome Binding Site}

\author[1,2,3]{Zhang Mengyan}
\author[4,5]{Holowko Maciej Bartosz}
\author[4,5]{Hayman Zumpe Huw}
\author[1,2,3,*]{Ong Cheng Soon}
\affil[1]{Machine Learning and Artificial Intelligence Future Science Platform, CSIRO}
\affil[2]{Department of Computer Science, Australian National University}
\affil[3]{Data61, CSIRO}
\affil[4]{Synthetic Biology Future Science Platform, CSIRO}
\affil[5]{Land and Water, CSIRO}
\affil[*]{email: cheng-soon.ong@data61.csiro.au}

\date{\today{}}


\bibliography{ref.bib}
% \DeclareUnicodeCharacter{2212}{-}
\begin{document}

\maketitle

\section*{Abstract}

Optimisation of gene expression levels is an essential part of the organism design process.
Fine control of this process can be achieved through engineering transcription and translation control elements, including the ribosome binding site (RBS).
Unfortunately, design of specific genetic parts can still be challenging due to lack of reliable design methods.
\mengyan{I still feel "reliable design methods" is too vague. e.g. what does reliable mean? why lack such methods? what are the challenging parts exactly?}
To address this problem, we have created a machine learning (ML) guided Design-Build-Test-Learn (DBTL) cycle for the experimental design of bacterial RBSs to show how small genetic parts can be reliably designed.
We used Gaussian Process Regression for the Learn part of cycle and the Upper Confidence Bound multi-armed Bandit algorithm for the Design of genetic designs to be tested in vivo.
We have integrated these machine learning algorithms with laboratory automation and high-throughput processes for a more reliable data generation .
The algorithms were able to identify RBS sequence rules conducive to high protein expression.
Notably, by testing only 450 RBS variants, we have designed and experimentally-validated RBSs with high translation initiation rates equalling or exceeding our benchmark RBS by up to 34\%.
Overall, our results show that machine learning is a powerful tool for designing RBSs, and they pave the way for the design of more complicated genetic devices.\\


Keywords: machine learning, optimisation, genetic part design, ribosome binding site

\newpage

\section{Introduction}

One of the main tenets of synthetic biology is the design, evaluation and standardisation of genetic parts \cite{Brophy2014,Canton2008,Stanton2014}.
Part design \mengyan{what is part design?} is a central problem of synthetic biology as it is the genetic parts that are ultimately combined into more complicated genetic circuits that produce desired functions in the target organisms. \mengyan{make the previous sentence shorter?}
This is usually done in terms of the Design-Build-Test-Learn (DBTL) cycle, where the given genetic part or organism is continually improved through an iterative process.
This cycle involves designing the DNA sequence in computer-aided design (CAD) software and then physically testing it in a laboratory.
Additionally, computer modelling can be used to predict characteristics of a genetic part \cite{Yeoh2019,Nielsen2016}.
Most of these computer models are based on either the thermodynamic properties of the involved molecules (DNA, RNA, proteins, etc.) or empirically-obtained values describing a relevant design property, like translation initiation rate (TIR) in the case of ribosome binding sites (RBS) \cite{Xia1998,Chen2013,Reeve2014}.
However, de novo design of small genetic elements is still challenging due to unknown relationships between their sequence and performance.
\\

The biggest gap in the DBTL cycle is at the interface between Learn and Design - it is hard to translate obtained results into new designs. \mengyan{what does "translate" mean? it feels like recommendation, but in the following we are discussing predictions.}
For example, according to  \textcite{Reeve2014} there are three main RBS calculators, all predicting the TIR based on the thermodynamic properties of the RBS and the ribosome \cite{Seo2013,Na2010,Salis2009}. 
Reported predictions from all of these models range from relatively good ($R^2 >0.8$) to low ($R^2 <0.2$) depending on the data set \cite{Reis2020}.
This may be due to number of reasons: 
i) they rely on calculations of free energies which can be difficult to estimate with high precision, 
ii) in general, one of the best ways to improve the models' accuracy is by increasing the number of phenomena taken into account, which can lead paradoxically to decreased model accuracy due to accumulation of errors \cite{EspahBorujeni2016}, and 
iii) by using deterministic coefficients to calculate energies, one disregards the often stochastic nature of processes in cells which can potentially increase prediction error \cite{Goss1998}. 
There is also evidence that binding energy calculations may be poor predictors of RBS strength \cite{Saito2020,Sherer1980}. 
This is reinforced by studies suggesting that RNA secondary structure is potentially a more important feature in TIR determination \cite{DESMIT1994,EspahBorujeni2016}.
\hl{This suggests that there is a number of interactions determining the mRNA-ribosome binding which can be hard to capture in a single, universal model}.\\

The volume of experimental data in synthetic biology is increasing exponentially. \cite{Freemont2019}. 
\mengyan{this is not the case for our study. The amount of data we have is quite small in ML view. And the small amount of data motivates the DBTL approach. }
Recent work has combined these data sets with machine learning algorithms to generate new models and predictors for use in synthetic biology, vastly improving the DBTL cycle's performance \cite{Camacho2018,Radivojevic2020,LAWSON2021}. 
For example, Jervis \emph{et al.} used support vector machines and neural networks to optimise production of monoterpenoids in \emph{Escherichia coli} \cite{Jervis2019}.
Similarly, Costello \emph{et al.} have used a number of machine learning approaches to analyse time-series multiomics data to predict metabolic pathway behaviour \cite{Costello2018}. 
Deep learning techniques have also been successfully used to analyse large synthetic biology data sets \cite{Alipanahi2015,Angermueller2016,Hollerer2020}.\\

\hl{In genetic design practice, the designers often have to fine tune characteristics of the used parts to meet the specific metrics of the designed strains.
For instance, if the goal is to produce a protein at a high yield, the designer might want to increase the TIR of the RBS responsible for translation of the said protein.
To do that, one could use the existing RBS predictors, but they can be unreliable in the investigated genetic context.}
Importantly, this work focuses on RBS part design and characteristic prediction, rather than looking at its wider genetic context and impact on the general performance of the cell.
As the RBS is one of the key genetic elements controlling protein expression and has a relatively short sequence, it is a suitable target for establishing a workflow that could be potentially translated to more complicated systems.\\

Our overall experimental goal was to maximise the TIR by building and testing batches of RBS sequences while minimising the number of DBTL cycle iterations.
We demonstrate how machine learning (ML) algorithms can be incorporated into the DBTL cycle to predict (Learn) and recommend (Design) variants of a bacterial RBS with the goal of optimising associated protein expression level \hl{in the specific genetic context (i.e. with these specific bases upstream and downstream of the investigated RBS sequences)}. 
Two types of ML algorithms are applied in the DBTL cycle. 
For the Learn part, the goal is design a predictor to predict the protein expression level by learning from the logged data. 
We used a Bayesian, non-parametric regression algorithm, called Gaussian Process Regression \cite{Rasmussen2004} in our pipeline. 
For the Design part, the goal is to design a policy to recommend RBS sequences to query sequentially in batches so that we can identify the optimal RBS sequences within a given budget. 
A variant of the Upper Confidence Bound multi-armed Bandit algorithms \cite{desautels2014parallelizing} was applied in our pipeline to guide our recommendations. 
The two types of algorithms cooperate with each other and provides powerful tools for DBTL cycle in genetic parts design.
\hl{This way, we were able to rationally increase production of our target protein compared to our very strong benchmark RBS by up to 35\%}. \\


\section{Results}

We present our RBS-optimising DBTL workflow that uses machine learning in Section~\ref{sec:dbtl-workflow}.
Machine learning is used in two different ways: i) we show the efficacy of the ML recommendations
in the Design stage (Section~\ref{sec:ucb-results}),
and ii) we demonstrate that the ML predictions are accurate in the Learn stage (Section~\ref{sec:gp-results}).
We present our new RBS sequence library in Section~\ref{sec:characteristics-of-library} and describe some interesting
characteristics of the discovered sequences,
as well as show the effectiveness of the automated laboratory workflow.

\subsection{The experimental workflow}
\label{sec:dbtl-workflow}

Our DBTL workflow, which uses machine learning to optimise protein expression, is shown in Figure \ref{fig: Flowchart}.
Build and Test are driven chiefly by choices made by human researchers and the use of automated methods.
Machine learning algorithms are applied in Learn and Design.
In the Learn phase, we use the Gaussian Process regression algorithm to predict the TIR of RBS sequences comprising the experimental space.
In the Design phase, we use the Bandit algorithm to recommend new RBS sequences based on the predictions from Learn.\\

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{plots/Main_Paper/flowchart.pdf}
    \caption{\textbf{Flowchart of the machine learning-based experimental design.} The RBS design is recommended by the Upper Confidence Bound multi-armed Bandit algorithm. After generating the recommendations, the RBSs are built and tested using automated laboratory methods allowing for rapid construction and testing at scale. Finally, the obtained results are fed back to the Gaussian Process Regression prediction algorithm in the Learn phase. n is the current design round and k is the maximum number of rounds allowed by time and/or money. In regards to the "Goal Met?" condition, the goal in our case was to find sequences with TIR significantly higher than benchmark, but the goal can be generalised to fit the requirements of the user.} 
    \label{fig: Flowchart}
\end{figure}

\hl{The exact position of the RBS in the sequence upstream of the CDS can be hard to pinpoint, however most previous studies put it in the 20 bp directly preceding the coding sequence.} 
In our investigation, we are using an RBS which is known to have a very high TIR when expressing GFP and is present in the pBb series of plasmids \cite{Lee2011}.
This template RBS sequence is 20 base pairs (bp) long with the sequence TTTAAGA\textbf{AGGAGA}TATACAT, where the highlighted nucleotides constitute the core of the RBS.
Since this is the sequence against which new RBS sequences will be benchmarked,
we will refer to this sequence as the \textit{benchmark sequence} hereafter.
Additionally, we have experimentally confirmed that modifying the core sequence is statistically more impactful on TIR than changes made outside of it (see Figure \ref{fig:core_vs_noncore}).
This hypothesis has been built based on reported biases towards certain bases present in the core of the RBS but absent outside of it.
For example, according to \cite{SHULTZABERGER2001} there is a strong bias towards A and G bases in the core region of the RBS.
Similarly, outside of the 6 bases of the core in the wider 20 bp context of the RBS there is no significant bias towards any particular base which suggest that these bases do not contribute to the overall TIR of a given RBS. 
Focusing on the 6 to 8 bp core sequence is a common RBS design approach \cite{Jeschek2016}.
Hence, in our design, we focus on modification of the core at nucleotide positions -8 to -13 (relative to the start codon of the GFP; this is where the consensus Shine-Dalgarno AGGAGG sequence is usually found in wild type \textit{E. coli}) of the RBS and we keep other positions the same as the benchmark sequence, i.e. TTTAAGA + NNNNNN + TATACAT, where N can be any nucleotide (A, C, G, T). 
The total experimental (variant) space is then $4^6$ = 4096.\\

In our genetic design, the investigated RBS controls expression of green fluorescent protein (GFP).
By controlling expression of a fluorescent protein with the RBS we can quickly assess the perceived relative TIR by measuring fluorescence of cells harbouring the expression vector over time.
Finally, the mRNA is transcribed from an IPTG-inducible promoter pLlacO-1.
Inducible expression allows one to synchronise the start of the GFP expression in all the cultures with addition of IPTG.
Since standardisation and comparative studies should be done in as similar genetic context as possible, the design of this device has been deliberately kept simple to make such studies easier \cite{Beal2021}.\\

\subsection{DESIGN: Performance of the recommendation algorithm}
\label{sec:ucb-results}


The Bandit recommendations were made using the batch Upper Confidence Bound multi-armed Bandit algorithm.
In short, this algorithm is a stochastic method of probing the experimental space. 
It aims at maximising the reward (output) from testing a limited number of instances from a large pool which cannot be tested exhaustively due to limited resources (time, computational power, money).
It balances the exploration-exploitation paradigm, where exploration focuses on testing data points which maximise information gain and exploitation focuses on recommending RBSs with high predicted TIR.\\

Figure \ref{fig: Swarmplot and Quantplot}A shows the results for all the RBS groups tested experimentally.
In each experimental round, in addition to the new RBS designs, we measure the TIR of the benchmark RBS as the internal standard.
We then obtain the normalised TIR (called \textit{TIR ratio}) by taking the ratio between the raw TIR of a new design and the average TIR of benchmark sequences in each round (which are run in triplicate in each round).
Figure \ref{fig:rawswarmplots.} shows these results in terms of raw TIRs.\\

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/swarmplot.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/quantplot.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/swarmplot_proj.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/histogram.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.42]{plots/Main_Paper/tsneplot.pdf}
    \end{subfigure}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/quantplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot_proj.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/histogram.pdf}
    % \includegraphics[scale=0.4]{plots/Main_Paper/tsneplot.pdf}
    \caption{
    \textbf{TIRs of RBS groups examined in this study.}
    \textbf{A)} Swarm plot showing the obtained TIRs divided into RBS groups.
    BPS-NC: base-by-base changes in the non-core region. 
    BPS-C: base-by-base changes in the core region. 
    UNI: Randomly generated sequences with uniform distribution. 
    PPM: Randomly generated sequences with distribution following the Position Probability Matrix for all natural RBS in \emph{E. coli}. 
    Bandit-0/1/2/3 - Bandit algorithm generated results for Round 0, 1, 2 and 3 respectively.
    SD - Shine-Dalgarno sequence.
    Dashed line is set to 1 and represents the averaged benchmark sequence TIR for that group. 
    BN - benchmark sequences for all plates. 
    (not all are exactly 1 due to them being shown as separate samples rather than per round averages.)
    \textbf{B)} Line plot showing TIR obtained in a given quantile (Q) of results divided into groups as in A).
    % save the random groups which were shown together due to similar distributions.
    UNI and PPM are merged into Random group and BPS-NC is not shown due to changes being made outside the core in that group.
    \textbf{C)} Exploitation v.s. Exploration for Bandit 1-3. Blue-hued points represent exploitation, those hued red represent exploration.
    \textbf{D)} Histogram with kernel density estimations (KDE) showing distributions of TIRs for Bandit groups.
    \textbf{E)} t-SNE plot showing the relative distances between sequences in our design spaces as calculated by our kernel function (weighted degree kernel with shift). 
    The area of the circle corresponds to the experimentally-obtained TIR value.
    The TIR results in all subplots are shown normalised to the respective benchmark sequence sample which acts as internal standard; the TIR of a given RBS is divided by TIR of the benchmark RBS run in the same plate. }
    \label{fig: Swarmplot and Quantplot}
\end{figure}

To generate the data set from which the algorithm would learn, we decided to characterise a total of 450 RBS variants, little over 10\% of the whole experimental space. 
To fit our automated workflow, we divided the 450 variants into batches of 90, split into 4 design rounds.\\

In the zeroth Round we tested two batches of designs, giving a total of 180 variants split as below: 

\begin{itemize}
    \item BPS-NC and BPS-C group: 60 RBS sequences which are subsequent single nucleotide variations of all 20 nucleotides of the original, benchmark sequence. This batch is designed to show us influence of such single nucleotide changes on the overall performance of the RBS and the potential impact of changes made beyond the core part (see Supplementary Figure \ref{fig:core_vs_noncore}).
    % \mengyan{put it in main paper?}
    \item UNI group: 30 RBS sequences that were uniformly randomised, i.e. equal probability of choosing any nucleotide for each position. This group shows the performance of RBSs generated randomly.
    \item PPM group: 30 RBS sequences randomised based on the position probability matrix (PPM) generated from all the naturally-occurring RBS sequences in  the \emph{E. coli} genome \cite{barrick1994quantitative}. This group shows the performance of RBSs generated randomly, but following the natural nucleotide distribution.
    \item Bandit-0: 60 RBS sequences recommended by our implementation of the recommendation algorithm based on a data set obtained from literature \cite{jervis2018machine}, which contains 113 non-repeated records for 56 unique RBS sequences with their respective TIRs.
    This data set has been used due to the perceived similarity of its goal to that of this work - prediction of TIR based on phenotypic output.
\end{itemize}

In the subsequent 3 rounds, with one batch each, all 90 designs were generated using our machine learning algorithm based on the data obtained from the previous rounds (these groups are called Bandit 1 to 3 respectively).\\

All Round 0 groups (BPS-NC, BPS-C, UNI, PPM, Bandit-0) performed worse than our benchmark sequence in terms of TIR. 
The best-performing group was the BPS-NC, which is explained by the relatively small impact on the TIR of changes made outside the RBS core. 
The Bandit-0 group's performance is similar to randomly generated designs, despite being machine learning-driven, due to being trained on approximate data.
% \hl{These results show that generating a strong RBS sequence by random mutations is a non-trivial task, when the tested data set is relatively small.}\\
Starting from Round 1, where the prediction and recommendation algorithms were fed data from Round 0, the results improved significantly, with a number of sequences performing better than the consensus Shine-Dalgarno sequence and in one case, better than the benchmark (by 8\%).
In Round 2 we observed further improvement by obtaining more sequences with TIRs similar to our benchmark sequence.
Finally, in Round 3 the algorithm identified two sequences that were 34\% and 15\% stronger than the benchmark sequence.\\

In summary, out of 450 tested sequences, around 40\% (BPS-NC, BPS-C, UNI, PPM) were created through some kind of sequence randomisation. Out of these randomised sequences, only a few got close to the benchmark sequence's TIR and were still 20\% weaker than it is (Figure 3A). In fact, these 80\% TIR ratio sequences were created by randomising the sequence outside of the core RBS region (BPS-NC), which was statistically shown not to be significantly impactful on the TIR (see Supplementary Figure S1 and, for example, work by \mbox{\textcite{Jeschek2016}}). More representative would be sequences from other random groups (BPS-C, UNI, PPM), from which the best sequence achieved only about 65\% of the benchmark TIR. 
These results show that generating a strong RBS sequence by random mutation is a non-trivial task, when the tested data set is relatively small.
Contrasted with this, non-randomized, our Bandit-driven design batch gave much better results, with RBSs getting close to benchmark performance and even exceeding it.\\

Figure \ref{fig: Swarmplot and Quantplot}B shows the same results but divided into quantiles where the specific point for a given group shows the highest TIR for that quantile.
The gradual increase for all quantiles can be observed for all Bandit groups, suggesting that the algorithms have a better understanding of the experimental space given more data.
The decreased result in the 0.9th quantile compared to the maximum value for the Bandit 3 group can be attributed to the increased emphasis on exploitation that has been set for that round compared to others.
We see this effect in Figure \ref{fig: Swarmplot and Quantplot}C (with details shown in Figure  \ref{fig:exploitationvsexplorationdetail}), where we coloured the data points for Bandit 1-3 groups according to their relative exploration - exploitation affinity.
Those with a high predicted mean are coloured blue and represent exploitation, those coloured red are with high predicted uncertainty and represent exploration.
\hl{Since we expect that an RBS sequence chosen at random will have low TIR
(as shown by UNI and PPM), this implies that most of the exploration will result in low TIR.
Our results confirm that
RBSs with high TIRs tend to come from exploitation of the design space,
whereas the exploration points give relatively low TIRs. Note that the exploration is necessary to expand our knowledge of unknown parts of the design space.}\\

Figure \ref{fig: Swarmplot and Quantplot}D shows the TIRs of RBSs tested in the Bandit groups divided into bins with width equal to a TIR ratio of 0.1.
KDE plots have been overlaid to depict the calculated density for each group.
The increase in prevalence of later Bandit groups in the higher bins is evident, especially for Bandit 2 and 3, constituting the bulk of results in the $>0.8$ TIR ratio bins.
Notably, the distributions calculated for all the groups are bimodal - we discuss the possible reasons for that later in the text.\\


In Figure \ref{fig: Swarmplot and Quantplot}E we show a t-distributed stochastic neighbour embedding (t-SNE) \mbox{\cite{tsne2008}} plot depicting the experimental space.
Each RBS is located on the plot according to its distance from other RBSs as calculated by our weighted degree kernel with shift (see Section \mbox{\ref{sec: method prediction with kernel}}).
The RBSs recommended by Bandit groups have covered the majority of the design space. 
Additionally, a number of clusters were especially targeted by our recommendation algorithm.
For example, the circled clusters labelled as "G-Rich Clusters" have been actively recommended by the algorithm.
More specifically, sequences with 4 or more guanines in any position constituted 10\% of the randomly selected sequences and 5, 9, 16 and finally 25\% in each of the 4 Bandit guided batches respectively.

\subsection{LEARN: Prediction of RBS performance}
\label{sec:gp-results}


Figure \ref{fig: Scatterplot} shows how our implementation of the Gaussian Process algorithm performed in terms of predictions in each round. 
As expected, the predictions in Round 0 were poor due to use of approximate data. 
The predictions improved for the subsequent rounds, from R\textsuperscript{2} (coefficient of determination) of 0.067 for Round 0 to R\textsuperscript{2} of 0.213 for Round 3.
Similarly, the Spearman correlation coefficient rose from 0.269 for Round 0 to 0.546 for Round 3.
Figure \ref{fig:scatter abc1 TT.} shows the performance of predictions for individual rounds.\\

The obtained values of the coefficient of determination and Spearman correlation coefficient are influenced by a number of factors.
The primary one is our choice of the exploration-exploitation balance.
In each round, we select a number of data points for exploration, which means that when tested, they have a high chance of having a experimentally-obtained mean different from what was predicted, thus decreasing the R\textsuperscript{2} values.
However, this is very useful information for future predictions as it allows us to understand the underlying space better.
\hl{In other words, we are intentionally sacrificing accuracy of our predictions in each round to improve them in the future rounds}. 
The effect of better exploration of the space is the ability to find high TIR RBSs even with relatively low R\textsuperscript{2} values.\\


\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_0.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_1.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_2.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_3.pdf}
    \end{subfigure}
    \caption{\textbf{Performance of the prediction algorithm (no kernel normalisation)}. The scatter plots A-D show the performance of our prediction algorithm calculated after each round.
    Note that the TIR values are normalised according to the standardisation described in section \ref{sec: method data pre-procesing}, which is different from the TIR ratio reported in Figure \ref{fig: Swarmplot and Quantplot}.
    The $R^2$ value and Spearman correlation coefficient (with corresponding p-value) are provided for each plot.
    The p-value here is for the null hypothesis stating that two sets of data are uncorrelated.
    }
    \label{fig: Scatterplot}
\end{figure}

\subsection{BUILD \& TEST: Characteristics of the tested sequences}
\label{sec:characteristics-of-library}

% Taken together, our data can be viewed as a reliable (low coefficients of variation) and extensive (high number of entries compared with widely used iGEM RBS collections, for example \cite{IGEM}) library of RBS sequences for \emph{E. coli}, some characteristics of which are shown in Table 1.\\

\hl{We present some important characteristics of the tested RBSs in Table 1}.
Figure \ref{fig:Library characteristics}A shows the sequence logo calculated for the Top 30 sequences (Figure \ref{fig: All_logo} shows the logo generated for all tested sequences).
It is generally understood that guanine-rich sequences promote strong transcription.
This expected bias towards guanine is clearly visible for all positions in our Top 30 RBSs.
This result combined with the Bandit algorithm's bias towards the G-rich cluster shown in Figure \ref{fig: Swarmplot and Quantplot}D reinforces the notion that our algorithm successfully identified G-rich sequences as the ones with high TIR probability.\\

\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \caption{}
         \includegraphics[scale=1.2]{plots/Main_Paper/TOP30_logo.pdf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \caption{}
         \includegraphics[scale=0.43]{plots/Main_Paper/heatmap.pdf}
     \end{subfigure}
     \caption{\textbf{Characteristics of strong RBSs.} A) Sequence logo calculated for the Top 30 tested sequences. B) Heatmap showing the edit (Hamming) distance required for positive change in TIR for RBSs with high and medium TIRs. The temperature scale shows the difference between a given RBS on the y-axis and the RBS with the strongest TIR at the given distance. Every second RBS is labelled for increased legibility.}
     \label{fig:Library characteristics}
\end{figure}

\begin{table}[!h]
\centering
\begin{minipage}[c]{0.6\textwidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Characteristics of the library}                                                       & \textbf{Statistics} \\ \hline
Total experimental space                                                                      & 4096                \\ \hline
Planned constructs                                                                            & 450                 \\ \hline
Successfully constructed                                                                      & 445                 \\ \hline
Sequences with CV\textless{}40\%                                                              & 79\%                \\ \hline
Sequences with CV\textless{}20\%                                                              & 27\%                \\ \hline
\begin{tabular}[c]{@{}c@{}}Efficiency of Bandit design \\ (compared with random)\end{tabular} & 2                   \\ \hline
Raw TIR range                                                                                     &      [4.93, 105.38]               \\ \hline
TIR ratio range                                                                                     &      [0.06, 1.34]               \\ \hline
\end{tabular}
\end{minipage}
\begin{minipage}[c]{0.38\textwidth}
\centering
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{tabular}{|c|c|}
\hline
\textbf{Top RBS Core} & \textbf{TIR Ratio} \\ \hline
GGGGGC                & 1.34             \\ \hline
GGGGGT                & 1.15             \\ \hline
GGCTAT                & 1.08              \\ \hline
\textbf{AGGAGA}                & 1                  \\ \hline
GGCGTT                & 0.98            \\ \hline
GGGGGG                & 0.98             \\ \hline
GGCGAC                & 0.98             \\ \hline
CAGGAG                & 0.96             \\ \hline
GGCGAG                & 0.95             \\ \hline
\textbf{AGGAGG}       & 0.39            \\ \hline
\end{tabular}
\end{minipage}
\caption{\textbf{Characteristics of the library.}
Left table presents some of the characteristics of our library.
Right table presents 10 RBS sequences with their corresponding TIR ratios; the first 9 are the strongest sequences including the benchmark sequence (AGGAGA) and the last is the Shine-Dalgarno sequence (AGGAGG).
CV is coefficient of variation (standard deviation (STD) of a sample divided by its mean; see details in Figures \ref{fig: SDhist} and \ref{fig:variation of biological replicates}).
Efficiency of the Bandit design is calculated by dividing the highest TIR found using machine learning by the highest TIR found using random sequence generation.}
\end{table}

Another interesting characteristic uncovered by our research is the perceived editing distance between two sequences required for  improvement in the TIR when the given RBS' TIR is already high.
We define the edit distance as Hamming distance, that is, how many positions have to be changed to get from one sequence to the other (Hamming distance of 0 means that the sequences are identical and 6 means that they are two completely different sequences).
Figure \ref{fig:Library characteristics}B shows the edit distance is required for positive change in TIR for an RBS with TIR $>0.75$.
For RBSs with high TIRs ($>1$), the minimum distance that is required to increase the TIR is 2, with edit distances between 2 and 5 giving similar results.
For RBSs with medium TIRs ($<1$), a distance of 1 is enough to produce a meaningful increase in TIR.\\

This means that as the TIRs of examined RBSs increase, exploring sequences which are more dissimilar to the current candidates tends to give more meaningful improvement. 
As long as this does not impact targeted methods like machine learning-guided design, it does imply that the low rate of natural mutations will be very slow to explore more dissimilar sequences on such a short distance \cite{Lee2012}, which indicates that methods like Adaptive Laboratory Evolution may not be able to find very strong RBSs with a limited budget.  
In other words, because the examined sequence is relatively short (6 bp in a wider 20 bp context) the time required to accumulate 2 or more changes in the RBS region required for meaningful increase in TIR might be prohibitively long.
In such cases, a directed process, like the one described here, should be strongly encouraged.
This observation is in line with approaches seen in other disciplines, e.g. protein engineering, where more directed changes yield better results than wide random changes \cite{Jackel2008}.\\

\hl{Finally, while our strong sequences showed some affinity towards the anti-sense sequence of the ribosome known to bind to RBS, they did not show any obvious secondary structures that could explain their TIRs} (see Figure \ref{fig:structures}).
This result combined with the unexpectedly bimodal nature of KDEs in Figure \ref{fig: Swarmplot and Quantplot} reinforces the notion, based on the previously reported literature \cite{Saito2020,EspahBorujeni2016}, that there may be a number of different mechanisms governing the probability of effective RBS-ribosome binding.\\



\section{Discussion and Conclusion}

In this work, we have shown how machine learning and high-throughput, automated laboratory methods can be jointly applied to efficiently \hl{optimise a small genetic part, in this case maximising TIR of a bacterial RBS}. 
We have used Gaussian Process regression to predict the TIR function to be optimised and an Upper Confidence Bound multi-armed Bandit algorithm to recommend sequences to be tested.
To represent RBSs and capture the similarities between them, we chose to use the Weighted Degree Kernel with Shift method, which fits well into our prediction method.
In building and testing, we have performed the bulk of our experiments using automation to increase their speed, reliability and reproducibility.
By using our workflow and testing only 450 RBS variants, we have designed and experimentally-validated RBSs with high translation initiation rates equalling or exceeding the currently known strong RBSs \hl{in this context} by up to 34\%.
Furthermore, we have generated an extensive library of diverse RBSs that can be used \hl{as a basis} in future studies.\\

In our work we have focused on maximising the TIR of the RBS, by gradually moving our emphasis from exploration to exploitation as we progressed through the design rounds.
\hl{As long as maximisation could be the appropriate function for optimising RBSs or other small parts}, it may have to be modified in more complex cases, especially involving multi-gene metabolic engineering since maximising expression of genes would most probably result in excessive metabolic burden.
This change could be achieved within the Bayesian optimisation framework by changing the objective of the algorithm to maximising the range of the black-box function, or identifying the part of function above a threshold \mbox{\cite{gotovos2013active}}, or combining different goals into a multi-objective method, for example \mbox{\cite{shu2020new}}.\\
% \hl{
% While we focus our optimisation framework on the RBS optimisation task, our framework has good potential to be generalised to multi-gene pathway design, as reviewed by \mbox{\textcite{LAWSON2021}} and proposed by \mbox{\textcite{hamedirad2019towards}}. For example, the optimisation goal can be adjusted to address combinatorial optimisation for multi-gene and RBS scenarios; since this would be a large-scale data task, the current Gaussian Process Regression (GPR) prediction model could be updated to a deep GPR approach and the current Bandit algorithm could be optimised towards querying large design space to reduce computational complexity \mbox{\cite{shahriari2015taking}}.  
% }

Further, it is important to make a distinction between works focusing on the part itself like \mbox{\textcite{Hollerer2020}} and works where the part is optimised with a wider goal of strain optimisation like in \mbox{\textcite{Jervis2019}}.
The part-centric optimisation, like the one considered in this paper, is an important task in synthetic biology since metabolic pathways are made up of individual parts and understanding their performance in separation allows for better strain design. 
While we focus our optimisation framework on the RBS optimisation task, this framework has good potential to be generalised to multi-gene pathway design as reviewed by \mbox{\textcite{LAWSON2021}} and proposed by \mbox{\textcite{hamedirad2019towards}}. 
For example, the optimisation goal can be adjusted to address combinatorial optimisation for multi-gene and RBS scenarios; since this would be a large-scale data task, the current Gaussian Process regression prediction model could be updated to a deep Gaussian Process regression approach and the current Bandit algorithm could be optimised towards querying large design space to reduce computational complexity \mbox{\textcite{Shahriari2016}}.  
\\

Our approach has shown that machine learning is able to correctly detect and exploit rules of biological design that otherwise require substantial time and experiments to uncover.
For example, our algorithm has correctly identified the correlation of high guanine content in the RBS with high TIR.
We have achieved this despite the relatively low R\textsuperscript{2} values for our predictions.
Notably, other machine learning methods used in synthetic biology on small data sets were also shown to produce useful recommendations despite low R\textsuperscript{2} values \cite{Radivojevic2020, Opgenorth2019}.
Additionally, it has been shown that when the prediction is used for recommendation tasks like the one considered in this paper, ranking-based metric like Spearman correlation coefficient is a better evaluation metric
\cite{Schober2018,  Kang2019}.
Finally, our predictor Gaussian Process regression model, compared with previously described calculators using a deterministic thermodynamic approach, is able to show the uncertainty of the predictions, which can be used by our Bandit algorithm to give better recommendations.\\

We have found our approach of bringing machine learning and synthetic biology experts together very powerful.
We envision that pairing machine learning with high-throughput automation will keep delivering a high number of good quality data sets and improved methods for biological engineering.
Synthetic biology is promoting standardised and normalised testing in biology and naturally pairs with machine learning, which can leverage the high quality biological data sets generated when the correct design rules are observed.
The addition of machine learning to synthetic biology also adds an additional layer of scrutiny to the generated data sets through the advanced statistical methods that can be used to design and analyse the experiments.
On top of that, the use of automation has helped us to produce more reliable results, which gave us the required confidence in our predictions and recommendations.\\

In this study we have limited the number of design rounds to four.
There were a number of reasons for this, including limitations on time and money, but also the results obtained showed that we have achieved our goal of generating very strong RBS designs.
There is a possibility that increasing the number of experimental rounds would enable us to improve the results further, however this has to be put in the context of limited resources.
For example, scanning the whole space would surely achieve the best results, i. e. would enable us to find the strongest possible RBS, but that would require unreasonable use of resources.
Compared to solutions like the one reported by Hollerer \cite{Hollerer2020}, our solution can be used when a high-volume method for data-generation is not available, while still providing the required results (optimised part). \\

There are still open questions that need to be addressed for applying machine learning in synthetic biology.
Firstly, we would like to understand how we can extract more biologically-important information from the decisions made by our algorithms.
We have shown that the algorithms are able to exploit them, but it will be important to create tools that will enable their reliable extraction from the results obtained.
Secondly, given the small number of RBS sequences tested, how can machine learning algorithms provide more accurate predictions and uncertainty measurements? 
Thirdly, the generalisability of the method is unknown.
\hl{Since the model has been taught on data relating to this specific genetic context its applicability for other context will be limited and new data will have to be obtained.}
We believe that the method described here would be useful for designing other small genetic parts, but the complexity of the task quickly increases with the size of the analysed sequence, so the method's applicability might be impacted at some point.
% \hl{Additionally, while we focus our optimisation framework on the RBS part optimisation task, this framework has good potential to be generalised to multi-gene pathway design.}
% \hl{Since such optimisation would be a large-scale data task, the current GPR prediction model could be updated to a deep GPR approach and the current bandits algorithm could be optimised towards querying large design space to reduce computational complexity } \cite{Shahriari2016}.
Finally, the optimal exploration-exploitation balance between rounds and samples is still to be determined.
In this research we have decided to gradually move the balance from exploration to exploitation across the rounds, but it is not yet clear what is the optimal way of conducting this change.\\

In the future, we hope to extend the algorithm to other more complicated genetic elements, including promoters and terminators.
However, it is important to reiterate that the complexity of the task quickly increases with the length of the sequence.
This is because the experimental space grows exponentially with the number of examined positions, so the space becomes increasingly hard to cover with experiments.
To solve this problem, different algorithms or experimental techniques might be needed, but the general workflow can be reused.\\

\input{methods.tex}

\section*{Code, data and material availability}

All code and data required to reproduce the results are available at Github: \url{https://github.com/mholowko/Solaris/tree/master/synbio_rbs}.
All the processed and raw data are included in the repository.
Sequences of plasmids and oligos and assembly reports used in this study are available in the supplementary information as a separate file.
The pBbB6c plasmid is available on Addgene. Other strains and plasmids are available on request from the authors; MTAs will need to be negotiated between the parties.


\section*{Contributions}
Zhang M. and Ong C. S. designed and implemented the machine learning algorithms and workflow. Holowko M. B. and Hayman Zumpe H. have designed and performed the laboratory experiments. Holowko M. B. and Ong C. S. conceived and planned the project. All authors analysed the data, contributed to and reviewed the manuscript.

\section*{Competing interests}
The authors declare no competing interests.

\section*{Acknowledgments}
The authors would like to acknowlege CSIRO's Machine Learning and Artificial Intelligence, and Synthetic Biology Future Science Platforms for providing funding for this research. The authors would also like to thank CSIRO BioFoundry for help with performing the experiments.


\newpage

\printbibliography

\clearpage

\setcounter{figure}{0}
\makeatletter
\renewcommand{\thefigure}{S\@arabic\c@figure}
\makeatother
\appendix
\input{supplementary}
\end{document}
