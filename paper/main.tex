\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[maxcitenames=1,style=numeric]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{thmtools}   
\usepackage{thm-restate}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk}
% \usepackage[table,xcdraw]{xcolor}
% \usepackage{subfloat}

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in


\newcommand{\cheng}[1]{\textcolor{purple}{{\bf Cheng:~}#1}}
\newcommand{\mengyan}[1]{\textcolor{magenta}{#1}}
\newcommand{\maciej}[1]{\textcolor{blue}{#1}}

% \captionsetup[subfigure]{position=top, labelfont=bf,textfont=normalfont,singlelinecheck=off,justification=raggedright}
\captionsetup[subfigure]{font={bf,small}, skip=1pt, singlelinecheck=false}
\renewcommand{\thesubfigure}{\Alph{subfigure}}

\addbibresource{ref.bib}

\title{Machine Learning guided workflow\\
for Ribosome Binding Site engineering}

\author[1,2,4]{Zhang M.}
\author[3]{Holowko M. B.}
\author[3]{Hayman Zumpe H.}
\author[1,2,4]{Ong, C. S.}
\affil[1]{Machine Learning and Artificial Intelligence Future Science Platform, CSIRO}
\affil[2]{Department of Computer Science, Australian National University}
\affil[3]{CSIRO Synthetic Biology Future Science Platform, CSIRO Land and Water}
\affil[4]{Data61, CSIRO}

\date{\today{}}

\bibliography{ref.bib}
% \DeclareUnicodeCharacter{2212}{-}
\begin{document}

\maketitle

\section*{Abstract}

Fine control of gene expression can be achieved through engineering transcriptional and translation control elements, including the Ribosome Binding Site (RBS).
Unfortunately, RBSs are not understood at the level of finesse required for reliable design. 
To address this problem, we have created a machine learning (ML) enabled workflow for the design of bacterial RBSs.
We used Gaussian Process Regression for prediction and the Upper Confidence Bound-based Bandit algorithm for recommendation of genetic designs to be tested in vitro.
We have integrated the ML algorithms with laboratory automation and high-throughput processes, creating a robust workflow for the design of custom RBSs.
Using our workflow, we generated a novel library of diverse RBSs with a wide range of expression levels.
Notably, a high number of these sites demonstrate translation initiation rates equalling or exceeding the currently known strong RBSs.

\section{Introduction}

One of the main tenets of synthetic biology is the design, evaluation and standardisation of genetic parts \cite{Brophy2014,Canton2008,Stanton2014}.
This is usually done in terms of the Design-Build-Test-Learn (DBTL) cycle, where the given genetic part or organism are continually improved by going through a number of turns of the said cycle.
This normally involves designing the DNA sequence in Computer Aided Design (CAD) software and then physically testing it in a laboratory. 
Additionally, computer modelling and prediction of part behaviour based on the designed DNA sequence or design of DNA sequence based on expected function can be used \cite{Yeoh2019,Nielsen2016}.
Most of these models are based on either the thermodynamic properties of the involved molecules (DNA, RNA, proteins, etc.) or empirically obtained values describing a relevant to a given design property, like Translation Initiation Rate (TIR) in the case of Ribosome Binding Sites (RBS) \cite{Xia1998,Chen2013,Reeve2014}.
However, de-novo design of small genetic elements is still challenging due to unknown relationships between sequence and performance of such elements.\\

The biggest gap in the DTBL cycle, at present, is at the Learn and Design interface - it is hard to translate obtained results into new designs.
For example, according to  \textcite{Reeve2014} there are three main RBS calculators, all predicting the TIR based on the thermodynamic properties of the RBS and the ribosome \cite{Seo2013,Na2010,Salis2009}. 
Reported predictions from all of these models are relatively good ($R^2 >0.8$), 
but they come with a number of caveats: i) they rely on calculations of free energies that can be hard to estimate with high precision ii) in general, one of the best ways to improve the models' accuracy is by increasing the number of phenomenons taken into account, but this can lead to paradoxically decreased model accuracy due to accumulation of errors \cite{EspahBorujeni2016} and iii) by using deterministic coefficients to calculate energies one disregards often stochastic nature of processes in the cells which again increases perceived prediction error \cite{Goss1998}. 
There are also sources showing that binding energy calculations may be poor predictors of RBS strength \cite{Saito2020,Sherer1980}. This is reinforced by studies suggesting that RNA secondary structure is potentially a more important feature in TIR determination \cite{DESMIT1994,EspahBorujeni2016}.\\

Synthetic biology is currently going through a phase of exponential increase in volume of data produced during experiments \cite{Freemont2019}. 
These new data sets can be combined with data reliant machine learning algorithms to generate new models and predictors for use in synthetic biology, vastly improving the DBTL cycle's performance \cite{Camacho2018,Radivojevic2020,LAWSON2021}. 
For example, Jervis \emph{et al.} used support vector machines and neural network to optimise production of monoterpenoid in \emph{Esherichia coli} \cite{Jervis2019}.
Similarly, Costello \emph{et al.} have used a number of machine learning approaches to analyse time-series multiomics data to predict metabolic pathway behaviour \cite{Costello2018}.
There were also successful attempts at using deep learning techniques for analysis of big synthetic biology data sets \cite{Alipanahi2015,Angermueller2016}.\\

In this work, we present how machine learning algorithms can be used as part of the DBTL cycle to predict (Learn) and recommend (Design) variants of bacterial RBS with the goal of optimising associated protein expression level. 
RBS being one of the key genetic elements controlling protein expression and at the same time having a relatively short sequence is a perfect target for establishing workflows that can be later translated to more complicated systems.
In this work we have used Gaussian Process Regression \cite{Rasmussen2004} and Upper Confidence Bound multi-armed Bandits algorithms \cite{desautels2014parallelizing} for prediction and recommendation respectively to analyse and optimise the initiation rates of the designed RBS .
Our overall experimental goal was to maximise the Translation Initiation Rate (TIR) by building and testing batches of RBS sequences while minimising the number of DBTL cycle turns that we had to do.
This way, we were able to build an extensive, reliable library of novel RBSs with diverse sequences.
At the same time we were able to discover new RBS sequences with very high TIRs from between 95 to 135\% of TIR of our chosen very strong benchmark RBS. 

\section{Results}

We present our RBS optimising DBTL workflow that uses machine learning in Section~\ref{sec:dbtl-workflow}.
Machine learning (ML) is used in two different ways: we show the efficacy of the ML recommendations
in the DESIGN stage (Section~\ref{sec:ucb-results}),
and we demonstrate that the ML predictions are accurate in the LEARN stage (Section~\ref{sec:gp-results}).
We present our new RBS sequence library in Section~\ref{sec:characteristics-of-library} and describe some interesting
characteristics of the discovered sequences, 
as well as show the effectiveness of the automated laboratory workflow stages.

\subsection{The experimental workflow}
\label{sec:dbtl-workflow}

We show our DBTL workflow, that uses machine learning to optimise protein expression, in Figure \ref{fig: Flowchart}.
BUILD and TEST are driven chiefly by choices made by human researchers and use of automated methods.
Machine learning algorithms are applied in LEARN and DESIGN.
In LEARN phase, we use the Gaussian Process regression algorithm to predict the TIR of RBS sequences comprising the experimental space.
In DESIGN phase, we use Upper Confidence Bound multi-armed Bandit algorithm to recommend new RBS designs based on the predictions from LEARN.\\

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{plots/Main_Paper/flowchart.pdf}
    \caption{\textbf{Flowchart of the machine learning based experimental design.} The RBS design is recommended by the Upper Confidence Bound Bandit algorithm. After generating the recommendations the RBS are built and tested using automated laboratory methods allowing for rapid construction and testing at scale. Finally, the obtained results are fed back to the prediction algorithm in the learn phase. }
    \label{fig: Flowchart}
\end{figure}

In our genetic design, the investigated RBS controls expression of the Green Fluorescent Protein (GFP). 
By controlling expression of a fluorescent protein with the RBS we can quickly assess the perceived relative TIR by measuring fluorescence of cells harbouring plasmid with the device over time.
Finally, the mRNA is transcribed from an IPTG-inducible promoter pLlacO-1. 
By making the whole device inducible we can synchronise the start of the expression of the GFP in all the cultures by inducing them at the same time with addition of IPTG.\\

Our template RBS sequence is 20 bps long with the sequence TTTAAGA\textbf{AGGAGA}TATACA, where the highlighted nucleotides constitute the core of the RBS.
This sequence is known to have a very high TIR and comes with the pBb series plasmids \cite{Lee2011}. 
Since this is the sequence against which new RBS sequences will be benchmarked,
we will refer to this sequence as the \textit{benchmark sequence} from now on.
We have experimentally confirmed that modifying the core sequence is statistically more impactful on TIR than changes made outside of it (see Supplementary materials).
In effect, in our design we focus on modification of the core at positions -8 to -13 (relative to the start codon of the GFP) nucleotides of the RBS and fix others to be the same as the benchmark sequence, i.e. TTTAAGA + NNNNNN + TATACAT, where N can be any nucleotide (A, C, G, T). 
The total experimental (variant) space is then $4^6$ = 4096.

\subsection{DESIGN: Performance of the recommendation algorithm}
\label{sec:ucb-results}

The design recommendations were made using the Multi-armed Bandit algorithm.
In short, this algorithm is a stochastic method of probing of the experimental space. 
This algorithm aims at maximising the reward (output) from testing a limited number of instances from a big pool which cannot be wholly tested due to limited resources (time, computational power, capital). \\

\mengyan{Maciej: do we want to put the group description into Fig 2 as we discussed last Fri?}
\maciej{I think we have decided to put it in the text, but within the description of Fig 2A as it is now. Otherwise the caption would be suuuppppppeeeerrrr long}
Figure \ref{fig: Swarmplot and Quantplot}A shows the results for all the examined groups. 
In each experimental round, we measure the TIR of benchmark RBS as the internal standard. 
We then obtain the normalised TIR (called \textit{TIR ratio}) by taking the ratio between the raw TIR and the average TIR of benchmark sequences in each round (which are run in triplicate in each round).\\

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/swarmplot.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/quantplot.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/swarmplot_proj.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/histogram.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.42]{plots/Main_Paper/tsneplot.pdf}
    \end{subfigure}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/quantplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot_proj.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/histogram.pdf}
    % \includegraphics[scale=0.4]{plots/Main_Paper/tsneplot.pdf}
    \caption{
    \textbf{TIRs of RBS groups examined in this study.} 
    \textbf{A)} Swarm plot showing the obtained TIRs divided into RBS groups.
    BPS-NC: base-by-base changes in the non-core region. 
    BPS-C: base-by-base changes in the core region. 
    UNI: Randomly generated sequences with uniform distribution. 
    PPM: Randomly generated sequences with distribution following the PPM for all natural RBS in \emph{E. coli}. 
    Bandit-0/1/2/3 - Bandit algorithm generated results for Round 0, 1, 2 and 3 respectively.
    SD - Shine-Dalgarno sequence.
    Dash line is set to 1 and represents the averaged benchmark sequence TIR for that group. 
    BN - benchmark sequences for all plates. 
    They are not all exactly 1 due to them being shown as separate samples rather than per round averages.
    \textbf{B)} Line plot showing TIR obtained in a given quantile of results divided into groups as in A).
    % save the random groups which were shown together due to similar distributions.
    UNI and PPM are merged into Random group and BPS-NC is not shown due to changes being made outside the core in that group. 
    \textbf{C)} Exploitation v.s. Exploration for Bandit 1-3. Blue-hued points represent exploitation, those hued red represent exploration. 
    \textbf{D)} Histogram with kernel density estimations (KDE) showing distributions of TIRs for Bandit groups.
    \textbf{E)} t-SNE plot showing the relative distances between sequences in our design spaces as calculated by our kernel function (weighted degree kernel with shift). 
    The area of the circle corresponds to the the experimentally obtained TIR value.
    The TIR results in all subplots are shown normalised to the respective benchmark sequence sample which acts as internal standard, that is TIR of a given RBS is divided by TIR of the benchmark RBS run in the same plate. }
    \label{fig: Swarmplot and Quantplot}
\end{figure}

To generate the data set that the algorithm could learn from we have decided to characterise a total of 450 RBS variants, which constitutes a little over 10\% of the whole experimental space. 
To fit into our automated workflow, we have divided the 450 variants into batches of 90.\\

In the zeroth round we have tested two batches of designs, for total of 180 variants split as below: 

\begin{itemize}
    \item BPS-NC and BPS-C group: 60 RBS sequences which are subsequent single nucleotide variations of all 20 nucleotides of the original, consensus sequence. This batch is designed to show us influence of such single nucleotide changes on the overall performance of the RBS and the potential impact of changes made beyond the core part (see Supplementary Figure \ref{fig:core_vs_noncore}).
    % \mengyan{put it in main paper?}
    \item UNI group: 30 RBS sequences that were  uniformly randomised, i.e. equal probability of choosing any nucleotide for each position. 
    \item PPM group: 30 RBS sequences randomised based on the position probability matrix (PPM) generated from all the naturally occurring RBS sequences in \emph{E. coli} genome \cite{Stormo1982}.
    \item Bandit-0: 60 RBS sequences recommended by our implementation of the recommendation algorithm based on a data set obtained from literature \cite{jervis2018machine}, which contains 113 non-repeated records for 56 unique RBS sequences with the respective TIRs.
    This data set has been used due to perceived similarity of its goal to the one of this work - prediction of TIR based on phenotypic output.
\end{itemize}

In the subsequent 3 rounds, with one batch each, all 90 designs were generated using our machine learning algorithm based on the data obtained from the previous rounds (these groups are called Bandit 1 to 3 respectively).\\

All Round 0 groups (BPS-NC, BPS-C, UNI, PPM, Bandit-0) have performed worse than our benchmark sequence in terms of TIR. 
The Bandit-0 group performed poorly, despite being machine learning driven, due to being trained on only approximate data.\\

However, starting from Round 1, where the bandit algorithm was fed data from the Round 0 the results improve significantly, with a number of sequences that perform better than the consensus Shine-Dalgarno sequence and in one case better than the benchmark (by 8\%).
In round 2 we have observed further improvement by getting more sequences that showed TIR on levels similar to our benchmark sequence.
Finally, in round 3 the algorithm identified two sequences that were 34\% and 15\% stronger than the benchmark sequence.\\

Figure \ref{fig: Swarmplot and Quantplot}B shows the same results but divided into quantiles where the specific point for a given group is showing the highest TIR for that quantile.
The gradual increase for all quantiles can be observed for all Bandit groups suggesting algorithms' better understanding of the experimental space with more data.
The decreased result in the 0.9th qunatile compared to the max value for Bandit 3 group can be attributed to the increased emphasis on exploitation that has been set for the that round compared to others.
We see that effect in Figure \ref{fig: Swarmplot and Quantplot}C, where we coloured the data points for Bandit 1-3 groups according to their relative exploration - exploitation affinity.
Those with high predicted mean are coloured blue and represent exploitation, those hued red are with high predicted uncertainty and represent exploration.
We can see that the RBSs with high TIRs tend to come from exploitation of the design space whereas the explorative points give relatively low TIR, but expand our knowledge about the unknown part of the design space.\\

Figure \ref{fig: Swarmplot and Quantplot}D shows the TIRs of RBSs tested in the Bandit groups divided into bins of width of 0.1 TIR ratio.
KDE plots have been overlaid to depict the calculated density for each group.
The increase of prevalence of later bandit groups in the higher bins is evident, especially for Bandit 2 and 3 constituting the bulk of results in the $>0.8$ TIR ratio bins.
Notably, the distributions calculated for all the groups show bivariate distribution, we discuss the possible reasons for that further in the text.\\

In \ref{fig: Swarmplot and Quantplot}E we show a t-SNE plot depicting the experimental space.
Each RBS is located on the plot according to its distance to other RBSs as calculated by our embedding function.
We can see the RBSs recommended by Bandit groups have covered majority of the design space. 
Additionally, a number of clusters were especially targeted by our recommendation algorithm.
For example, the circled clusters labelled as ``G-Rich Clusters" have been actively recommended by the algorithm.
More specifically, sequences with more or equal to 4 guanines in any position constituted 10\% of the randomly selected sequences and 5, 9, 16 and finally 25\% in each of the 4 Bandit guided batches respectively.

\subsection{LEARN: Prediction of RBS performance}
\label{sec:gp-results}

Figure \ref{fig: Scatterplot} shows how our implemetation of the Gaussian Process algorithm performed in terms of predictions in each round. 
As expected, the predictions in Round 0 were poor due to use of approximated data. 
The predictions improved for the subsequent rounds, from R\textsuperscript{2} of 0.065 for round 0 to R\textsuperscript{2} of 0.27 for round 3.
Similarly, the Spearman correlation coefficient rose from 0.27 for Round 0 to 0.48 for Round 3.\\

One important point to note is that the predictions are also influenced by our recommendation choices. 
In each round, we select a number of data points for exploration, which means that these data points, when tested, have a high chance of having real mean different to what was predicted.
However, this is still very useful information for future predictions as it allows us to understand the underlying space better.\\

    
\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_0.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_1.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_2.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_3.pdf}
    \end{subfigure}
    \caption{\textbf{Performance of the prediction algorithm.} The scatter plots A-D are showing the performance of our prediction algorithm calculated after each round.
    Note that the TIR values are normalised according to the standardisation described in section \ref{sec: method data pre-procesing}, which is different from the TIR ratio reported in the Figure \ref{fig: Swarmplot and Quantplot}.
    The values of $R^2$ and Spearman correlation coefficient (with corresponding p-value) are provided for each plot.
    The p-value here is for null hypothesis stating that two sets of data are uncorrelated.
    }
    \label{fig: Scatterplot}
\end{figure}

\subsection{BUILD \& TEST: Characteristics of the tested sequences}
\label{sec:characteristics-of-library}

Our data set taken together can be viewed as a reliable library of RBS sequences for \emph{E. coli}, some characteristics of which are shown in Table 1.\\

Figure \ref{fig:Library characteristics}A shows the sequence logo calculated for the Top 30 sequences (in terms of TIR ratio).
It is generally understood that guanine rich sequences are promoting strong transcription.
This expected bias towards guanine is clearly visible for all positions in our Top 30 RBSs.
This result combined with the Bandits' algorithm bias towards the G rich cluster shown in Figure \ref{fig: Swarmplot and Quantplot}D reinforces the notion that our algorithm successfully identified G rich sequences as the ones with high TIR probability.\\

\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \caption{}
         \includegraphics[scale=1.2]{plots/Main_Paper/TOP30_logo.pdf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \caption{}
         \includegraphics[scale=0.5]{plots/Main_Paper/Hd_Heatmap.pdf}
     \end{subfigure}
     \caption{\textbf{Characteristics of strong RBSs.} A) Sequence logo calculated for the Top 30 tested sequences. B) Heatmap showing what edit (Hamming) distance is required for positive change in TIR for RBS with high and medium TIR. The temperature scale shows the difference between a given RBS on y axis and the RBS with strongest TIR at the given distance. Every second RBS is labelled for increased legibility. }
     \label{fig:Library characteristics}
\end{figure}

\begin{table}[!h]
\centering
\begin{minipage}[c]{0.6\textwidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Characteristics of the library}                                                       & \textbf{Statistics} \\ \hline
Total experimental space                                                                      & 4096                \\ \hline
Planned constructs                                                                            & 450                 \\ \hline
Successfully constructed                                                                      & 445                 \\ \hline
Sequences with CV\textless{}40\%                                                              & 79\%                \\ \hline
Sequences with CV\textless{}20\%                                                              & 27\%                \\ \hline
\begin{tabular}[c]{@{}c@{}}Efficiency of bandit design \\ (compared with random)\end{tabular} & 2                   \\ \hline
Raw TIR range                                                                                     &      [4.926, 105.377]               \\ \hline
TIR ratio range                                                                                     &      [0.063, 1.339]               \\ \hline
\end{tabular}
\end{minipage}
\begin{minipage}[c]{0.38\textwidth}
\centering
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{tabular}{|c|c|}
\hline
\textbf{Top RBS Core} & \textbf{TIR Ratio} \\ \hline
GGGGGC                & 1.339             \\ \hline
GGGGGT                & 1.154             \\ \hline
GGCTAT                & 1.084              \\ \hline
\textbf{AGGAGA}                & 1                  \\ \hline
GGCGTT                & 0.981            \\ \hline
GGGGGG                & 0.979             \\ \hline
GGCGAC                & 0.976             \\ \hline
CAGGAG                & 0.963             \\ \hline
GGCGAG                & 0.952             \\ \hline
\textbf{AGGAGG}                & 0.394            \\ \hline
\end{tabular}
\end{minipage}
\caption{\textbf{Characteristics of the library.}
Left table presents some of the characteristics of our library.
Right table presents 10 RBS sequences with their corresponding TIR ratios, the first 9 being the strongest sequences including the benchmark sequence (AGGAGA) and the last being the Shine-Dalgarno sequence (AGGAGG).
CV is coefficient of variation (STD of a sample divided by its mean).
Efficiency of the bandit design is calculated by dividing the highest TIR found using machine learning by the highest TIR found using random sequence generation. }
\end{table}

Another interesting characteristic uncovered by our research is the perceived editing distance between two sequences required for  improvement in the TIR when the given RBS' TIR is already high. 
We define the edit distance as Hamming distance, that is, how many positions have to be changed to get from one sequence to the other (Hamming distance of 0 means that the sequences are identical and 6 means that they are two completely different sequences).
Figure \ref{fig:Library characteristics}B shows what edit distance is required for positive change in TIR for RBS with TIR $>0.75$.
For RBS with high TIR ($>1$), the minimum distance that is required for increase of TIR is 2, with edit distance between 2 and 5 giving similar results.
For RBS with medium TIR ($<1$), a distance of 1 is enough to produce a meaningful increase in TIR.
That means that as the TIR of examined RBSs increases, exploring sequences which are more dissimilar to the current candidates tends to give more meaningful improvement. 
This also implies that the low rate of natural mutations will be very slow to explore more dissimilar sequences on such a short distance \cite{Lee2012}, which indicates that methods like Adaptive Laboratory Evolution may not be able to find very strong RBSs within limited budget.  
In other words, because the examined sequence is relatively short (6bp in a wider 20bp context) the time to accumulate 2 or more changes in the RBS region required for meaningful increase in TIR might be prohibitively long.
In such cases, a directed process should be strongly encouraged.
This is in line with common practices in e.g. protein engineering, where similar approaches, that is making more directed changes, are often observed \cite{Jackel2008}.\\

Finally, our strong sequences did not show neither strong binding to the anti-sense sequence of the ribosome known to bind to RBS nor any obvious secondary structures that could explain their TIRs (see Supplementary).
This result combined with the unexpectedly bimodal nature of KDEs in Figure \ref{fig: Swarmplot and Quantplot} reinforces the notion based on the previously reported literature \cite{Saito2020,EspahBorujeni2016} that there may be a number of different mechanisms governing the probability of effective RBS-ribosome binding.\\


\section{Discussion}

In this work, we have shown how machine learning and high-throughput, automated laboratory methods can be used to efficiently generate a library of small parts, in this case bacterial RBS. 
We have used Gaussian Process regression to predict the shape of our function and Upper Confidence Bound Bandit algorithm to recommend sequences to be tested.
We have investigated a number of methods of digitising the DNA sequence, finally settling on Weighted Degree Kernel with Shift method, which fit well into our prediction method.
In building and testing, we have performed bulk of our experiments using automation to increase their speed, reliability and reproducibility.
By using our workflow, we have found very strong RBS sequences and we have generated an extensive library of diverse RBS that can be used in the future studies.
We have achieved this despite the relatively low accuracy of our predictions, which means that the presented algorithms are robust and able to identify the right signal in a noisy environment. \\

We have found our approach bringing machine learning and synthetic biology experts very powerful.
We envision that pairing of machine learning with high-throughput automation will keep delivering a high number of good quality data sets and improved methods for biological engineering.\\

There are still open questions that need to be addressed for applying machine learning in synthetic biology.
For one, we would like to understand how can we extract more biologically important information from the decisions made by our algorithms.
Secondly, given a small amount of RBS sequences tested, how can machine learning algorithm provide more accurate predictions and uncertainty measurement? 
Thirdly, the generalisability of the method is unknown.
We believe that the method would be useful for use in design of other small genetic parts, but the complexity of the task quickly increases with the size of the analysed sequence and so the method's applicability might be impacted at some point.
Finally, the optimal balance between exploration-exploitation between rounds and samples is still to be determined.
In this research we have decided to gradually move the balance from exploration to exploitation over the rounds, but it is not yet clear what is the method protocol for this change.\\

In the future, we hope to extend the algorithm to other, more complicated genetic elements.
This could include, for example, promoters and terminators.
However, its important to note that the complexity of the task quickly increases with the length of the sequence.
This is because the experimental space grows exponentially with the number of examined positions and so the space becomes increasingly hard to cover with experiments.
To solve this problem, a different algorithms or experimental techniques might be needed, but the general workflow can be reused.\\

\input{methods.tex}

\section*{Code and data availability}

All code and data required to reproduce the results is available at Github: \url{https://github.com/mholowko/SynbioML} .
All the processed and raw data is included in the repository.
Sequences of plasmids and oligos and assembly reports used in this study are available in supplementary as a separate file.

\section*{Contributions}
Zhang M. and Ong C. S. designed and implemented the machine learning algorithms and workflow. Holowko M. B. and Hayman Zumpe H. have designed and performed the laboratory experiments. Holowko M. B. and Ong C. S. conceived and planned the project. All authors analysed the data, contributed to and reviewed the manuscript.

\section*{Competing interests}
The authors declare no competing interests.

\section*{Acknowledgments}
The authors would like to acknowlege CSIRO's Machine Learning and Artificial Intelligence, and Synthetic Biology Future Science Platforms for providing funding for this research. The authors would also like to thank CSIRO BioFoundry for help with performing the experiments.


\newpage

\printbibliography

\clearpage

\setcounter{figure}{0}
\makeatletter 
\renewcommand{\thefigure}{S\@arabic\c@figure}
\makeatother
\appendix
\input{supplementary}
\end{document}
