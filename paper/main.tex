\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[maxcitenames=1,style=numeric, sorting = none]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{thmtools}   
\usepackage{thm-restate}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk}
% \usepackage[table,xcdraw]{xcolor}
% \usepackage{subfloat}

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in


\newcommand{\cheng}[1]{\textcolor{purple}{{\bf Cheng:~}#1}}
\newcommand{\mengyan}[1]{\textcolor{magenta}{#1}}
\newcommand{\maciej}[1]{\textcolor{blue}{#1}}

% \captionsetup[subfigure]{position=top, labelfont=bf,textfont=normalfont,singlelinecheck=off,justification=raggedright}
\captionsetup[subfigure]{font={bf,small}, skip=1pt, singlelinecheck=false}
\renewcommand{\thesubfigure}{\Alph{subfigure}}

\addbibresource{ref.bib}

\title{Machine Learning guided Design\\
of Ribosome Binding Sites}

\author[1,2,4]{Zhang M.}
\author[3]{Holowko M. B.}
\author[3]{Hayman Zumpe H.}
\author[1,2,4]{Ong, C. S.}
\affil[1]{Machine Learning and Artificial Intelligence Future Science Platform, CSIRO}
\affil[2]{Department of Computer Science, Australian National University}
\affil[3]{CSIRO Synthetic Biology Future Science Platform, CSIRO Land and Water}
\affil[4]{Data61, CSIRO}

\date{\today{}}

\bibliography{ref.bib}
% \DeclareUnicodeCharacter{2212}{-}
\begin{document}

\maketitle

\section*{Abstract}

% Fine control of gene expression can be achieved through engineering transcription and translation control elements, including the ribosome binding site (RBS).
The ribosome binding site (RBS), as one of the transcription and translation control elements, allows fine control of gene expression.
% Unfortunately, RBSs are not understood at the level of finesse required for reliable design.
We consider the problem of designing RBSs at the nucleotide level to optimise gene expression with a fixed budget. 
To address this problem, we have created a machine learning (ML) guided Design-Build-Test-Learn (DBTL) cycle for the experimental design of bacterial RBSs.
We used Gaussian Process Regression for prediction and the batch Upper Confidence Bound-based Bandit algorithm for recommendation of genetic designs to be tested in vivo.
We have integrated these machine learning algorithms with laboratory automation and high-throughput processes for more reliable data generation.
The algorithms were able to identify RBS sequences rules conducive to high protein expression.
Notably, by testing only 450 RBS variants, we have discovered 
% designed and experimentally-validated 
RBSs with high translation initiation rates equalling or exceeding the currently known strong RBSs by up to 34\%.
Overall, our results show that machine learning is a very powerful tool for designing RBSs, and they pave the way for the design of more complicated genetic devices.


\section{Introduction}

One of the main tenets of synthetic biology is the design, evaluation and standardisation of genetic parts \cite{Brophy2014,Canton2008,Stanton2014}.
This is usually done in terms of the Design-Build-Test-Learn (DBTL) cycle, where the given genetic part or organism is continually improved through an iterative process.
This cycle involves designing the DNA sequence in computer-aided design (CAD) software and then physically testing it in a laboratory. 
Additionally, computer modelling can be used to predict characteristics of a genetic part \cite{Yeoh2019,Nielsen2016}.
Most of these computer models are based on either the thermodynamic properties of the involved molecules (DNA, RNA, proteins, etc.) or empirically-obtained values describing a relevant design property, like translation initiation rate (TIR) in the case of ribosome binding sites (RBS) \cite{Xia1998,Chen2013,Reeve2014}.
However, de-novo design of small genetic elements is still challenging due to unknown relationships between their sequence and performance.\\

The biggest gap in the DTBL cycle, at present, is at the interface between Learn and Design - it is hard to translate obtained results into new designs.
For example, according to  \textcite{Reeve2014} there are three main RBS calculators, all predicting the TIR based on the thermodynamic properties of the RBS and the ribosome \cite{Seo2013,Na2010,Salis2009}. 
Reported predictions from all of these models are relatively good ($R^2 >0.8$), 
but they come with a number of caveats: i) they rely on calculations of free energies which can be difficult to estimate with high precision, ii) in general, one of the best ways to improve the models' accuracy is by increasing the number of phenomena taken into account, which can lead paradoxically to decreased model accuracy due to accumulation of errors \cite{EspahBorujeni2016}, and iii) by using deterministic coefficients to calculate energies, one disregards the often stochastic nature of processes in cells which increases perceived prediction error \cite{Goss1998}. 
There is also evidence that binding energy calculations may be poor predictors of RBS strength \cite{Saito2020,Sherer1980}. This is reinforced by studies suggesting that RNA secondary structure is potentially a more important feature in TIR determination \cite{DESMIT1994,EspahBorujeni2016}.\\

Currently, the volume of experimental data in synthetic biology is increasing exponentially. \cite{Freemont2019}. 
Recent work has combined these data sets with machine learning algorithms to generate new models and predictors for use in synthetic biology, vastly improving the DBTL cycle's performance \cite{Camacho2018,Radivojevic2020,LAWSON2021}. 
For example, Jervis \emph{et al.} used support vector machines and neural network to optimise production of monoterpenoids in \emph{Esherichia coli} \cite{Jervis2019}.
Similarly, Costello \emph{et al.} have used a number of machine learning approaches to analyse time-series multiomics data to predict metabolic pathway behaviour \cite{Costello2018}. Deep learning techniques have also been successfully used to analyse large synthetic biology data sets \cite{Alipanahi2015,Angermueller2016,Hollerer2020}.\\

In this work, we demonstrate how machine learning algorithms can be incorporated into the DBTL cycle to predict (Learn) and recommend (Design) variants of a bacterial RBS with the goal of optimising associated protein expression level. 
We used Gaussian Process Regression \cite{Rasmussen2004} and Upper Confidence Bound multi-armed Bandits algorithms \cite{desautels2014parallelizing} for prediction and recommendation respectively to analyse and optimise the initiation rates of the designed RBS.
As the RBS is one of the key genetic elements controlling protein expression and has a relatively short sequence, it is a perfect target for establishing a workflow that can be later translated to more complicated systems.
Our overall experimental goal was to maximise the Translation Initiation Rate (TIR) by building and testing batches of RBS sequences while minimising the number of DBTL cycle iterations.
This way, we have built an extensive and reliable library of novel RBSs with diverse sequences. We discovered new RBS sequences with very high TIRs, between 95 to 135\% of our very strong benchmark RBS. 

\section{Results}

We present our RBS-optimising DBTL workflow that uses machine learning in Section~\ref{sec:dbtl-workflow}.
Machine learning is used in two different ways: i) we show the efficacy of the ML recommendations
in the Design stage (Section~\ref{sec:ucb-results}),
and ii) we demonstrate that the ML predictions are accurate in the Learn stage (Section~\ref{sec:gp-results}).
We present our new RBS sequence library in Section~\ref{sec:characteristics-of-library} and describe some interesting
characteristics of the discovered sequences, 
as well as show the effectiveness of the automated laboratory workflow.

\subsection{The experimental workflow}
\label{sec:dbtl-workflow}

Our DBTL workflow, which uses machine learning to optimise protein expression, is shown in Figure \ref{fig: Flowchart}.
Build and Test are driven chiefly by choices made by human researchers and the use of automated methods.
Machine learning algorithms are applied in Learn and Design.
In the Learn phase, we use the Gaussian Process regression algorithm to predict the TIR of RBS sequences comprising the experimental space.
In the Design phase, we use Upper Confidence Bound multi-armed Bandit algorithm to recommend new RBS sequences based on the predictions from Learn.\\

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{plots/Main_Paper/flowchart.pdf}
    \caption{\textbf{Flowchart of the machine learning-based experimental design.} The RBS design is recommended by the Upper Confidence Bound Bandit algorithm. After generating the recommendations, the RBSs are built and tested using automated laboratory methods allowing for rapid construction and testing at scale. Finally, the obtained results are fed back to the Gaussian Process Regression prediction algorithm in the Learn phase. n is the current design round and k is the maximum number of rounds allowed by time and/or money.}
    \label{fig: Flowchart}
\end{figure}

In our genetic design, the investigated RBS controls expression of green fluorescent protein (GFP). 
By controlling expression of a fluorescent protein with the RBS we can quickly assess the perceived relative TIR by measuring fluorescence of cells harbouring the expression vector over time.
Finally, the mRNA is transcribed from an IPTG-inducible promoter pLlacO-1. 
Inducible expression allows one to synchronise the start of the GFP expression in all the cultures upon addition of IPTG.\\

Our template RBS sequence is 20 base pairs (bp) long with the sequence TTTAAGA\textbf{AGGAGA}TATACA, where the highlighted nucleotides constitute the core of the RBS.
This sequence is known to have a very high TIR and is present in the pBb series of plasmids \cite{Lee2011}. 
Since this is the sequence against which new RBS sequences will be benchmarked,
we will refer to this sequence as the \textit{benchmark sequence} hereafter.
We have experimentally confirmed that modifying the core sequence is statistically more impactful on TIR than changes made outside of it (see Supplementary materials).
In our design, we focus on modification of the core at nucelotide positions -8 to -13 (relative to the start codon of the GFP) of the RBS and keep other positions the same as the benchmark sequence, i.e. TTTAAGA + NNNNNN + TATACAT, where N can be any nucleotide (A, C, G, T). 
The total experimental (variant) space is then $4^6$ = 4096.

\subsection{DESIGN: Performance of the recommendation algorithm}
\label{sec:ucb-results}

Figure \ref{fig: Swarmplot and Quantplot}A shows the results for all the RBS groups tested experimentally. 
In each experimental round, in addition to the new RBS designs, we measure the TIR of the benchmark RBS as the internal standard. 
We then obtain the normalised TIR (called \textit{TIR ratio}) by taking the ratio between the raw TIR of a new design and the average TIR of benchmark sequences in each round (which are run in triplicate in each round).\\

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/swarmplot.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/quantplot.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.35]{plots/Main_Paper/swarmplot_proj.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/histogram.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.42]{plots/Main_Paper/tsneplot.pdf}
    \end{subfigure}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/quantplot.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/swarmplot_proj.pdf}
    % \includegraphics[scale=0.3]{plots/Main_Paper/histogram.pdf}
    % \includegraphics[scale=0.4]{plots/Main_Paper/tsneplot.pdf}
    \caption{
    \textbf{TIRs of RBS groups examined in this study.} 
    \textbf{A)} Swarm plot showing the obtained TIRs divided into RBS groups.
    BPS-NC: base-by-base changes in the non-core region. 
    BPS-C: base-by-base changes in the core region. 
    UNI: Randomly generated sequences with uniform distribution. 
    PPM: Randomly generated sequences with distribution following the PPM for all natural RBS in \emph{E. coli}. 
    Bandit-0/1/2/3 - Bandit algorithm generated results for Round 0, 1, 2 and 3 respectively.
    SD - Shine-Dalgarno sequence.
    Dash line is set to 1 and represents the averaged benchmark sequence TIR for that group. 
    BN - benchmark sequences for all plates. 
    (not all are exactly 1 due to them being shown as separate samples rather than per round averages.)
    \textbf{B)} Line plot showing TIR obtained in a given quantile of results divided into groups as in A).
    % save the random groups which were shown together due to similar distributions.
    UNI and PPM are merged into Random group and BPS-NC is not shown due to changes being made outside the core in that group. 
    \textbf{C)} Exploitation v.s. Exploration for Bandit 1-3. Blue-hued points represent exploitation, those hued red represent exploration. 
    \textbf{D)} Histogram with kernel density estimations (KDE) showing distributions of TIRs for Bandit groups.
    \textbf{E)} t-SNE plot showing the relative distances between sequences in our design spaces as calculated by our kernel function (weighted degree kernel with shift). 
    The area of the circle corresponds to the the experimentally obtained TIR value.
    The TIR results in all subplots are shown normalised to the respective benchmark sequence sample which acts as internal standard, that is TIR of a given RBS is divided by TIR of the benchmark RBS run in the same plate. }
    \label{fig: Swarmplot and Quantplot}
\end{figure}

To generate the data set that the algorithm could learn from we have decided to characterise a total of 450 RBS variants, little over 10\% of the whole experimental space. 
To fit into our automated workflow, we have divided the 450 variants into batches of 90 split into 4 design rounds.\\

In the zeroth round we have tested two batches of designs, giving a total of 180 variants split as below: 

\begin{itemize}
    \item BPS-NC and BPS-C group: 60 RBS sequences which are subsequent single nucleotide variations of all 20 nucleotides of the original, consensus sequence. This batch is designed to show us influence of such single nucleotide changes on the overall performance of the RBS and the potential impact of changes made beyond the core part (see Supplementary Figure \ref{fig:core_vs_noncore}).
    % \mengyan{put it in main paper?}
    \item UNI group: 30 RBS sequences that were  uniformly randomised, i.e. equal probability of choosing any nucleotide for each position. 
    \item PPM group: 30 RBS sequences randomised based on the position probability matrix (PPM) generated from all the naturally occurring RBS sequences in \emph{E. coli} genome \cite{barrick1994quantitative}.
    % \cite{Stormo1982}.
    \item Bandit-0: 60 RBS sequences recommended by our implementation of the recommendation algorithm based on a data set obtained from literature \cite{jervis2018machine}, which contains 113 non-repeated records for 56 unique RBS sequences with the respective TIRs.
    This data set has been used due to perceived similarity of its goal to the one of this work - prediction of TIR based on phenotypic output.
\end{itemize}

In the subsequent 3 rounds, with one batch each, all 90 designs were generated using our machine learning algorithm based on the data obtained from the previous rounds (these groups are called Bandit 1 to 3 respectively).
The bandit recommendations were made using the batch Upper Confidence Bound algorithm.
% In short, this algorithm is a stochastic method of probing of the experimental space. 
% It aims at maximising the reward (output) from testing a limited number of instances from a large pool which cannot be tested exhaustively due to limited resources (time, computational power, money).
It balances the exploration-exploitation paradigm, where exploration focuses on testing data points which maximise information gain and exploitation focuses on recommending RBSs with high predicted TIR.\\

All Round 0 groups (BPS-NC, BPS-C, UNI, PPM, Bandit-0) have performed worse than our benchmark sequence in terms of TIR. 
The best performing of these groups is the BPS-NC, which is explained by relatively small impact on the TIR of changes made outside the RBS core. 
The Bandit-0 group's performance is similar to randomly generated designs, despite being machine learning driven, due to being trained on approximate data.\\

However, starting from Round 1, where the prediction and recommendation algorithms were fed data from Round 0, the results improve significantly, with a number of sequences which perform better than the consensus Shine-Dalgarno sequence and in one case better than the benchmark (by 8\%).
In Round 2 we have observed further improvement by obtaining more sequences with TIRs similar to our benchmark sequence.
Finally, in Round 3 the algorithm identified two sequences that were 34\% and 15\% stronger than the benchmark sequence.\\

Figure \ref{fig: Swarmplot and Quantplot}B shows the same results but divided into quantiles where the specific point for a given group shows the highest TIR for that quantile.
The gradual increase for all quantiles can be observed for all Bandit groups, suggesting that the algorithms have a better understanding of the experimental space given more data.
The decreased result in the 0.9th quantile compared to the max value for Bandit 3 group can be attributed to the increased emphasis on exploitation that has been set for that round compared to others.
We see this effect in Figure \ref{fig: Swarmplot and Quantplot}C, where we coloured the data points for Bandit 1-3 groups according to their relative exploration - exploitation affinity.
Those with a high predicted mean are coloured blue and represent exploitation, those coloured red are with high predicted uncertainty and represent exploration.
We can see that the RBSs with high TIRs tend to come from exploitation of the design space, whereas the explorative points give relatively low TIRs, but expand our knowledge of the unknown part of the design space.\\

Figure \ref{fig: Swarmplot and Quantplot}D shows the TIRs of RBSs tested in the Bandit groups divided into bins with width equal to a TIR ratio of 0.1.
KDE plots have been overlaid to depict the calculated density for each group.
The increase in prevalence of later bandit groups in the higher bins is evident, especially for Bandit 2 and 3, constituting the bulk of results in the $>0.8$ TIR ratio bins.
Notably, the distributions calculated for all the groups are bimodal - we discuss the possible reasons for that later in the text.\\

In \ref{fig: Swarmplot and Quantplot}E we show a t-SNE plot depicting the experimental space.
Each RBS is located on the plot according to its distance from other RBSs as calculated by our embedding function.
The RBSs recommended by Bandit groups have covered majority of the design space. 
Additionally, a number of clusters were especially targeted by our recommendation algorithm.
For example, the circled clusters labelled as ``G-Rich Clusters" have been actively recommended by the algorithm.
More specifically, sequences with 4 or more guanines in any position constituted 10\% of the randomly selected sequences and 5, 9, 16 and finally 25\% in each of the 4 Bandit guided batches respectively.

\subsection{LEARN: Prediction of RBS performance}
\label{sec:gp-results}

Figure \ref{fig: Scatterplot} shows how our implementation of the Gaussian Process algorithm performed in terms of predictions in each round. 
As expected, the predictions in Round 0 were poor due to use of approximated data. 
The predictions improved for the subsequent rounds, from R\textsuperscript{2} of 0.067 for round 0 to R\textsuperscript{2} of 0.213 for round 3.
Similarly, the Spearman correlation coefficient rose from 0.269 for Round 0 to 0.546 for Round 3.\\

One important point to note is that the predictions are also influenced by our recommendation choices. 
In each round, we select a number of data points for exploration, which means that these data points, when tested, have a high chance of having a real mean different from what was predicted.
However, this is still very useful information for future predictions as it allows us to understand the underlying space better.\\

    
\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_0.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_1.pdf}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_2.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \caption{}
        \includegraphics[scale=0.4]{plots/Main_Paper/scatter_abc1_FF_3.pdf}
    \end{subfigure}
    \caption{\textbf{Performance of the prediction algorithm.} The scatter plots A-D are showing the performance of our prediction algorithm calculated after each round.
    Note that the TIR values are normalised according to the standardisation described in section \ref{sec: method data pre-procesing}, which is different from the TIR ratio reported in the Figure \ref{fig: Swarmplot and Quantplot}.
    The values of $R^2$ and Spearman correlation coefficient (with corresponding p-value) are provided for each plot.
    The p-value here is for null hypothesis stating that two sets of data are uncorrelated.
    }
    \label{fig: Scatterplot}
\end{figure}

\subsection{BUILD \& TEST: Characteristics of the tested sequences}
\label{sec:characteristics-of-library}

Our data set taken together can be viewed as a reliable library of RBS sequences for \emph{E. coli}, some characteristics of which are shown in Table 1.\\

Figure \ref{fig:Library characteristics}A shows the sequence logo calculated for the Top 30 sequences (in terms of TIR ratio).
It is generally understood that guanine rich sequences are promoting strong transcription.
This expected bias towards guanine is clearly visible for all positions in our Top 30 RBSs.
This result combined with the Bandits' algorithm bias towards the G rich cluster shown in Figure \ref{fig: Swarmplot and Quantplot}D reinforces the notion that our algorithm successfully identified G rich sequences as the ones with high TIR probability.\\

\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \caption{}
         \includegraphics[scale=1.2]{plots/Main_Paper/TOP30_logo.pdf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \caption{}
         \includegraphics[scale=0.5]{plots/Main_Paper/Hd_Heatmap.pdf}
     \end{subfigure}
     \caption{\textbf{Characteristics of strong RBSs.} A) Sequence logo calculated for the Top 30 tested sequences. B) Heatmap showing what edit (Hamming) distance is required for positive change in TIR for RBS with high and medium TIR. The temperature scale shows the difference between a given RBS on y axis and the RBS with strongest TIR at the given distance. Every second RBS is labelled for increased legibility. }
     \label{fig:Library characteristics}
\end{figure}

\begin{table}[!h]
\centering
\begin{minipage}[c]{0.6\textwidth}
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Characteristics of the library}                                                       & \textbf{Statistics} \\ \hline
Total experimental space                                                                      & 4096                \\ \hline
Planned constructs                                                                            & 450                 \\ \hline
Successfully constructed                                                                      & 445                 \\ \hline
Sequences with CV\textless{}40\%                                                              & 79\%                \\ \hline
Sequences with CV\textless{}20\%                                                              & 27\%                \\ \hline
\begin{tabular}[c]{@{}c@{}}Efficiency of bandit design \\ (compared with random)\end{tabular} & 2                   \\ \hline
Raw TIR range                                                                                     &      [4.93, 105.38]               \\ \hline
TIR ratio range                                                                                     &      [0.06, 1.34]               \\ \hline
\end{tabular}
\end{minipage}
\begin{minipage}[c]{0.38\textwidth}
\centering
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{tabular}{|c|c|}
\hline
\textbf{Top RBS Core} & \textbf{TIR Ratio} \\ \hline
GGGGGC                & 1.34             \\ \hline
GGGGGT                & 1.15             \\ \hline
GGCTAT                & 1.08              \\ \hline
\textbf{AGGAGA}                & 1                  \\ \hline
GGCGTT                & 0.98            \\ \hline
GGGGGG                & 0.98             \\ \hline
GGCGAC                & 0.98             \\ \hline
CAGGAG                & 0.96             \\ \hline
GGCGAG                & 0.95             \\ \hline
\textbf{AGGAGG}       & 0.39            \\ \hline
\end{tabular}
\end{minipage}
\caption{\textbf{Characteristics of the library.}
Left table presents some of the characteristics of our library.
Right table presents 10 RBS sequences with their corresponding TIR ratios, the first 9 being the strongest sequences including the benchmark sequence (AGGAGA) and the last being the Shine-Dalgarno sequence (AGGAGG).
CV is coefficient of variation (STD of a sample divided by its mean).
Efficiency of the bandit design is calculated by dividing the highest TIR found using machine learning by the highest TIR found using random sequence generation. }
\end{table}

Another interesting characteristic uncovered by our research is the perceived editing distance between two sequences required for  improvement in the TIR when the given RBS' TIR is already high. 
We define the edit distance as Hamming distance, that is, how many positions have to be changed to get from one sequence to the other (Hamming distance of 0 means that the sequences are identical and 6 means that they are two completely different sequences).
Figure \ref{fig:Library characteristics}B shows what edit distance is required for positive change in TIR for RBS with TIR $>0.75$.
For RBS with high TIR ($>1$), the minimum distance that is required for increase of TIR is 2, with edit distance between 2 and 5 giving similar results.
For RBSs with medium TIRs ($<1$), a distance of 1 is enough to produce a meaningful increase in TIR.
This means that as the TIR of examined RBSs increases, exploring sequences which are more dissimilar to the current candidates tends to give more meaningful improvement. 
This also implies that the low rate of natural mutations will be very slow to explore more dissimilar sequences on such a short distance \cite{Lee2012}, which indicates that methods like Adaptive Laboratory Evolution may not be able to find very strong RBSs within limited budget.  
In other words, because the examined sequence is relatively short (6bp in a wider 20bp context) the time to accumulate 2 or more changes in the RBS region required for meaningful increase in TIR might be prohibitively long.
In such cases, a directed process should be strongly encouraged.
This is in line with common practices in e.g. protein engineering, where similar approaches, that is making more directed changes, are often observed \cite{Jackel2008}.\\

Finally, our strong sequences showed neither strong binding to the anti-sense sequence of the ribosome known to bind to RBS, nor any obvious secondary structures that could explain their TIRs (see Supplementary).
This result combined with the unexpectedly bimodal nature of KDEs in Figure \ref{fig: Swarmplot and Quantplot} reinforces the notion based on the previously reported literature \cite{Saito2020,EspahBorujeni2016} that there may be a number of different mechanisms governing the probability of effective RBS-ribosome binding.\\


\section{Discussion}

In this work, we have shown how machine learning and high-throughput, automated laboratory methods can be jointly applied to efficiently generate a library of small parts, in this case bacterial RBS. 
We have used Gaussian Process regression to predict the TIR function to be optimised and Upper Confidence Bound Bandit algorithm to recommend sequences to be tested.
% We have investigated a number of methods of digitising the DNA sequence, finally settling on 
To represent RBSs and capture the similarities between them, we chose to use the
Weighted Degree Kernel with Shift method, which fit well into our prediction method.
In building and testing, we have performed the bulk of our experiments using automation to increase their speed, reliability and reproducibility.
By using our workflow and testing only 450 RBS variants, we have discovered 
% designed and experimentally-validated 
RBSs with high translation initiation rates equalling or exceeding the currently known strong RBSs by up to 34\%.
Furthermore, we have generated an extensive library of diverse RBSs that can be used in future studies.\\

Our approach has shown that machine learning is able to correctly detect and exploit rules of biological design that otherwise require substantial time and experiments to uncover.
For example, our algorithm has correctly identified the correlation of high guanine content in the RBS with high TIR.
We have achieved this despite the relatively low accuracy of our predictions, which benefits from recommendation algorithms with the exploitation-exploration balance.
This gives hope that the continuing rise in use of machine learning in synthetic biology will enable us to uncover new biologically relevant rules of design.\\

We have found our approach of bringing machine learning and synthetic biology experts together very powerful.
We envision that pairing machine learning with high-throughput automation will keep delivering a high number of good quality data sets and improved methods for biological engineering.
Synthetic biology is promoting standardised and normalised testing in biology which naturally pairs with machine learning which can leverage the high quality biological data sets generated when the correct design rules are observed.
The addition of machine learning to synthetic biology also adds an additional layer of scrutiny to the generated data sets through the advanced statistical methods that can be used to design and analyse the experiments.
On top of that, the use of automation has helped us to produce more reliable results, which gave us the required confidence in our predictions and recommendations.\\

In this study we have limited the number of design rounds to four.
There was a number of reasons for this, including time and money limitations, but also the results obtained showed that we have achieved our goal of generating very strong RBS designs.
There is a possibility that increasing the number of experimental rounds would enable us to improve the results further, however this has to be put in the context of the limited resources.
For example, scanning the whole space would surely achieve the best results, i. e. would enable us to find the strongest possible RBS, but that would require unreasonable use of resources.
Our approach also compares favourably with approaches using deep learning \cite{Hollerer2020}, where the data sets required for accurate predictions are 3 orders of magnitude bigger than ours. \\

There are still open questions that need to be addressed for applying machine learning in synthetic biology.
Firstly, we would like to understand how we can extract more biologically-important information from the decisions made by our algorithms.
We have shown that the algorithms are able to exploit them, but it will be important to create tools that will enable their reliable extraction from the obtained results.
Secondly, given the small number of RBS sequences tested, how can machine learning algorithms provide more accurate predictions and uncertainty measurement? 
Thirdly, the generalisability of the method is unknown.
We believe that the method described here would be useful for designing other small genetic parts, but the complexity of the task quickly increases with the size of the analysed sequence, so the method's applicability might be impacted at some point.
Finally, the optimal exploration-exploitation balance between rounds and samples is still to be determined.
In this research we have decided to gradually move the balance from exploration to exploitation across the rounds, but it is not yet clear what is the optimal way of conducting this change.\\

In the future, we hope to extend the algorithm to other more complicated genetic elements, including promoters and terminators.
However, it is important to reiterate that the complexity of the task quickly increases with the length of the sequence.
This is because the experimental space grows exponentially with the number of examined positions, so the space becomes increasingly hard to cover with experiments.
To solve this problem, different algorithms or experimental techniques might be needed, but the general workflow can be reused.\\

\input{methods.tex}

\section*{Code and data availability}

All code and data required to reproduce the results is available at Github: \url{https://github.com/mholowko/Solaris/tree/master/synbio_rbs} .
All the processed and raw data is included in the repository.
Sequences of plasmids and oligos and assembly reports used in this study are available in supplementary as a separate file.

\section*{Contributions}
Zhang M. and Ong C. S. designed and implemented the machine learning algorithms and workflow. Holowko M. B. and Hayman Zumpe H. have designed and performed the laboratory experiments. Holowko M. B. and Ong C. S. conceived and planned the project. All authors analysed the data, contributed to and reviewed the manuscript.

\section*{Competing interests}
The authors declare no competing interests.

\section*{Acknowledgments}
The authors would like to acknowlege CSIRO's Machine Learning and Artificial Intelligence, and Synthetic Biology Future Science Platforms for providing funding for this research. The authors would also like to thank CSIRO BioFoundry for help with performing the experiments.


\newpage

\printbibliography

\clearpage

\setcounter{figure}{0}
\makeatletter 
\renewcommand{\thefigure}{S\@arabic\c@figure}
\makeatother
\appendix
\input{supplementary}
\end{document}
