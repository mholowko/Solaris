\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[maxcitenames=1,style=numeric]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{caption}
%\usepackage{subcaption}

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\title{Machine Learning guided workflow for Ribosome Binding Site engineering}
\author{Zhang M., Holowko M. B., Hayman Zumpe H., Ong, C. S.}
\date{\today{}}

\bibliography{ref.bib}
\DeclareUnicodeCharacter{2212}{-}
\begin{document}

\maketitle

\section{Abstract}

Fine control of gene expression is extremely important for engineered biological systems.
Such control can be achieved through engineering transcriptional and translation control elements, including the Ribosome Binding Site (RBS).
Unfortunately, our understanding of how RBSs perform in vivo is insufficiently well understood to allow predictable, reliable design of RBSs at the level of finesse needed for some applications.
To address this problem, we have created a machine learning (ML) enabled workflow for the design of bacterial RBSs.
We used Gaussian Process Regression for prediction of relative transcription initiation rates and the Upper Confidence Bound-based Bandit algorithm for recommendation of genetic designs to be tested in vitro.
The Bandit algorithm addresses the exploration-exploitation trade-off, balancing the search for highly novel sequences with the desire for strong RBSs.
We have integrated the ML algorithms with laboratory automation and high-throughput processes, creating a robust workflow for the design of custom RBSs.
By employing these techniques, we were able to increase the reliability and reproducibility of results and increase the confidence in the design process.
Using our workflow, we generated a novel library of diverse RBSs with a wide range of expression levels.
Notably, a high number of these sites demonstrate translation initiation rates equalling or exceeding the currently known strong RBSs.
Additionally, this work elucidated some design guidelines, including the favourable level of difference between two sequences and efficient numerical representation of the analysed DNA/RNA sequence.
In summary, by employing both machine learning and high-throughput laboratory methods we have created a workflow for creation of small genetic devices enabling efficient generation of parts with required characteristics.
As a next step, we hope to expand this workflow to more complicated parts including promoters and terminators. 


\section{Introduction}

One of the main tenets of synthetic biology is design, evaluation and standardisation of genetic parts \cite{Brophy2014,Canton2008,Stanton2014}.
This is usually done in terms of the Design-Build-Test-Learn (DBTL) cycle, where the given genetic part or organism are continually improved by going through a number of turns of the said cycle.
This normally involves designing the DNA sequence in Computer Aided Design (CAD) software and then physically testing it in a laboratory. 
Additionally, computer modelling and prediction of part behaviour based on the designed DNA sequence or design of DNA sequence based on expected function can be used\cite{Yeoh2019,Nielsen2016}.
Most of these models are based on either the thermodynamic properties of the involved molecules (DNA, RNA, proteins, etc.) or empirically obtained values describing a relevant to a given design property, like Translation Initiation Rate (TIR) in case of Ribosome Binding Sites (RBS) \cite{Xia1998,Chen2013,Reeve2014}.
However, de-novo design of small genetic elements is still challenging due to unknown relationships between sequence and performance of such elements. 
This means that many designers have to rely on known and characterised parts that may not be optimal for their constructs.\\
The biggest limitation for the DBTL approach currently is the Learn part of the cycle - there is very limited access to methods and software that can improve and understand designs based on the experimental results.
For example, according to Reeve \emph{et al.} there are three main RBS calculators, all predicting the TRI based on the thermodynamic properties of the RBS and the ribosome \cite{Seo2013,Na2010,Salis2009}. 
Predictions from all of these models are relatively good ($R^2 >0.8$), but they come with a number of caveats: i) they rely on calculations of free energies that can be hard to estimate with high precision ii) in general, one of the best ways to improve the models' accuracy is by increasing the number of phenomenons taken into account, but this can lead to paradoxically decreased model accuracy due to accumulation of errors \cite{EspahBorujeni2016} and iii) by using deterministic coefficients to calculate energies one disregards often stochastic nature of processes in the cells which again increases perceived prediction error \cite{Goss1998}. 
There are also sources showing that binding energy calculations may be poor predictors of RBS strength \cite{Saito2020,Sherer1980}.\\
Synthetic biology is currently going through a phase of exponential increase in volume of data produced during experiments \cite{Freemont2019}. 
New experimental methods heavily relying on advances in automation and microfludics allow unprecedented precision and throughput in data generation.
These new data-sets can be combined with data reliant machine learning algorithms to generate new models and predictors for use in synthetic biology, vastly improving the DBTL cycle's performance \cite{Camacho2018}. 
In the past few years there was a significant uptake of Machine Learning based approaches in synthetic biology \cite{LAWSON2021}.
Jervis \emph{et al.} used support vector machine and neural network to optimise production of monoterpenoid in \emph{Esherichia coli} \cite{Jervis2019}.
Similarly, Costello \emph{et al.} have used a number of machine learning approaches to analyse time-series multiomics data to predict metabolic pathway behaviour \cite{Costello2018}.
There were also successful attempts at using deep learning techniques for analysis of big data-sets \cite{Alipanahi2015,Angermueller2016}. Machine learning has been also used for prediction in proteins \cite{Yang2018}.
However, the use of machine learning in synthetic biology is still in its infancy and will require additional research to show its full potential. \\
Here we present how machine learning algorithms can be used as part of the DBTL cycle to predict (Learn) and recommend (Design) variants of RBS with goal of optimization of protein level expression. RBS being one of the key genetic elements controlling protein expression and at the same time having a relatively short sequence is a perfect target for establishing workflows that can be later translated to more complicated systems.
We have used Gaussian Process-Upper Confidence Bound and multi-armed Bandits algorithms for prediction and recommendation respectively to analyse and optimise the initiation rates of the designed RBS \cite{desautels2012parallelizing, Rasmussen2004}.
Our overall experimental goal was to maximise the Translation Initiation Rate (TIR) by identifying the set of RBS sequences with top TIR scores while minimising the number of DBTL cycle turns that we had to do.
We did this by designing a sequential experimental workflow, where designs in the zeroth round where randomised RBS sequences designed to explore the experimental space and some preliminary machine learning recommended designs based on literature. 
In the subsequent rounds, designs where recommended by the algorithm based on the data obtained in previous rounds. 
The designs were then physically constructed in batches of 90 to fit our automated process (see \textbf{Methods} section).
After constructing, the plasmids harbouring the new genetic devices were tested in microplate reader.
The results were then fed back to the algorithm for it to recommend the next round of designs.\\

\input{methods}

\section{Results}

Figure \ref{fig: Flowchart} shows the DBTL cycle with the machine learning process emphasised. Our machine learning workflow encompasses two phases of the cycle, namely the LEARN and DESIGN phase. 
In LEARN phase we use the Gaussian Process regression algorithm to predict the behaviour of different RBS sequences and in the DESIGN phase we use the multi-armed Bandit algorithm to recommend sequences to be Built and Tested next. 
Conventionally, the Design phase is considered the beginning of the cycle, however we will start description of our results from the Learn phase, which we find a more natural starting point for a machine learning centred workflow.\\

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{plots/flowchart.pdf}
    \caption{Flowchart of machine learning based experimental design.}
    \label{fig: Flowchart}
\end{figure}

\subsection{DESIGN of the genetic device}
There is a number of factors that impact the protein expression rate, many of them concerned with how the ribosome recognises and binds to the RBS sequence \cite{Chen1994,Vellanoweth1992,SHULTZABERGER2001}.
In our genetic design, the investigated RBS controls expression of the Green Fluorescent Protein (GFP) in its mut3b variant. 
By controlling expression of a fluorescent protein with the RBS we can quickly assess the perceived relative TIR by measuring fluorescence of cells harbouring plasmid with the device over time.
Finally, the mRNA is transcribed from an IPTG-inducible promoter pLlacO-1. 
By making the whole device inducible we can synchronise the start of the expression of the GFP in all the cultures by inducing them at the same optical density (OD\textsubscript{600}) with addition of IPTG.\\
In \emph{E. coli} the RBS is usually located in the 20 bases upstream of the start codon. 
The RBS usually has a distinguishable, consensus, core sequence called the Shine-Dalgarno sequence, which in \emph{E. coli} is AGGAGG. 
Here, we put that 20 bp long sequence into focus with main emphasis being put on the 6bp core region (Figure \ref{fig:Anatomy}).\\

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{plots/RBS_anatomy.pdf}
    \caption{\textbf{Diagram of the investigated sequence.} The sequence of the investigated RBS is shown with the randomised core sequence in red. The start codon of the GFP coding sequence is also shown with numbers showing relative nucleotide positions.}
    \label{fig:Anatomy}
\end{figure}

The investigated RBS sequence is 20 bps long with the sequence TTTAAGAAGGAGATATACAT, which is a known high TIR RBS that comes with the pBb series plasmids \cite{Lee2011}. 
In our design we focus on randomising of the core -8 to -13 (relative to the start codon of the GFP) nucleotides of the RBS and fix others to be the same as the consensus sequence, i.e. TTTAAGA + NNNNNN + TATACAT.
Since for each of the 6 position there are 4 possibilities: A, C, G, T the total experimental (variant) space is $4^6$ = 4096.\\
To generate the dataset that the algorithm could learn from we have decided to characterise a total of 450 RBS variants, which constitutes a little over 10\% of the whole experimental space. 
To fit into our automated workflow, we have divided the 450 variants into batches of 90.
In the zeroth round we have tested two batches of designs, for total of 180 variants. 
Since at that stage we didn't have access to any prior data fitting our design we approximated the predictions using the data from \textcite{jervis2018machine} and the Gaussian Process regression method (see \textbf{Methods} section) \cite{srinivas2012information}.
This set contains 113 non-repeated records for 56 unique RBS sequences with the respective TIR.
This dataset has been used due to perceived similarity of its goal to the one of this work - prediction of TIR based on phenotypic output.\\
Apart from the abovementioned algorithm recommnded sequences, we have also decided to try some random sequences for the total of 180 sequences:

\begin{enumerate}
    \item 60 RBS sequences which are subsequent single nucleotide changes of all 20 nucleotides of the original, consensus sequence. This batch is designed to show us influence of such single nucleotide changes on the overall performance of the RBS and the potential impact of changes made beyond the core part.
    \item 30 RBS sequences that were fully and uniformly randomised (equal probability of choosing either nucleotide for each position) 
    \item 30 RBS sequences randomised based on the position probability matrix (PPM) generated from all the naturally occuring RBS sequences in \emph{E. coli} genome
    \item 60 RBS sequences recommended by our multi armed bandit recommendation algorithm based on the initial literature, as described above.
\end{enumerate}{}

In the subsequent rounds, data obtained in the previous ones have been used to teach the algorithm. 
These subsequent rounds were run in batches of 90 each. \\

\subsection{BUILD and TEST}

For the machine learning to work effectively the analysed data-set needs to show two qualities: high relative volume and high quality of data.
These two elements don't have a specific definitions, but in general the data-set have to be going into at least hundreds possible data points and one have to cover 5-10\% of that space.
And since quality of the obtained data has a direct and strong correlation with quality of the predictions and in effect - recommendations, one need to ensure that the obtained results represent the real value as close as possible.\\
To help us obtain reliable and reproducible results we have employed automation-heavy workflow for our experiments.
This way we hope to eliminate a big part of sample-to-sample variation as well as human-introduced variation.
Additionally, performing all the procedures directly in 96-well microplate format enabled us to significantly cut down time to prepare our variants.\\
In short, the genetic variations of the RBS were introduced to the plasmids with combination of PCR and isothermal assembly. 
The plasmids were then transformed and the resulting transformants were tested using microplate reader.
Vast majority of reactions were prepared using liquid handling equipment.
Similarly, colony picking was done by automated colony picker.\\

\subsection{LEARN - prediction of RBS performance}

The three main considerations when designing a machine learning workflow for synthetic biology is feature representation, prediction and recommendation algorithms.
Since machine learning is just starting to be used in synthetic biology, there is no agreed approach to any of these problems.\\
In our case, the problem of feature representation was how to embed the DNA sequence to a numerical vector which can be used in machine learning predictions. 
Kernel functions implicitly embed DNA sequences into high-dimensional feature space and capturing similarities between data points.  
%Gaussian Process regression has been used, we have investigated use of different types of kernels (also called covariances) for embedding \cite{Ben-Hur2008}. 
% The additional, and positive, effect of using kernels is that our data will be moved to a higher-dimensional space, which makes the regression process easier. 
% We compared performance of Dot Product, RBF and a number of string kernels: spectrum, mixed spectrum, weighted degree and weighted degree with shifting \textcolor{red}{Figure Xa}. 
% Since we found that the spectrum kernel performed the best, we have used it in subsequent studies.
% TODO: citation needed. 
In our case, we consider string kernels \cite{}, which takes two DNA sequences as input and output a real value which represents the similarity between those two sequences. 
The commonly used string kernels includes spectrum kernels \cite{}, mixed spectrum kernels \cite{}, weighted degree kernels \cite{} and weighted degree kernels with shifts \cite{}. 
We review the design of the above string kernel in Appendix \ref{}.
For Round 0, we chose to use the spectrum kernel to process the core 6bp and dot product kernel to process the 7bp flanking sequences both upstream and downstream of the core sequence.
For other rounds, we chose to use the weighted degree kernel with shifts. \\
% TODO: reasoning needed. 
% This approach allowed for good balance between computational complexity and performance.\\

The predictions were made using the Gaussian Process regression (GPR) algorithm. 
In essence, Gaussian Process is a stochastic (Bayesian) predictor of the shape of a function, which is built based on perceived similarities between data points (kernels), where not only a mean value, but also probability distribution of the mean is calculated. 
Such an approach makes GPR well suited to predict biological phenomena which are also highly stochastic. \\

The recommendations were made using the Multi-armed Bandit algorithm.
In short, the this algorithm is a stochastic method of probing of the experimental space. 
This algorithm aims at maximising the reward (output) from testing a limited number of instances from a big pool which cannot be wholly tested due to limited resources (time, computational power, capital). 
In our case we use the Upper Confidence Bound version of the algorithm, which focuses its recommendations on sequences that should give highest TIR based on the probabilities computed by the prediction algorithm (GPR). 
Another feature of the bandits algorithms is that it balances two approaches: exploration and exploitation. 
Exploration makes the algorithm recommend designs that will improve the predictions, whereas exploitation will recommend designs that focus on delivering the most efficient design the fastest. 
The two approaches can be controlled with the \textbeta\enspace parameter. 
We have decided that in the first iterations of the cycle it would be beneficial to skew the algorithm towards the exploration with exploitation taking increasing role in later iterations. 
One thing of note is that the bandit algorithm is stochastic, that is it exploits the probabilities of given event occurring (in this case RBS having a specific TIR). 
As such, it pairs naturally with our prediction algorithm, the Gaussian Process, which provides probability based function regression.
\\

\subsection{EXIT}
There are two potential points where the exit from the cycle should be considered: i) the optimum solution has been found or ii) depletion of available resources (time and/or money). In our case we have performed a total of four rounds, with performance of the RBSes increasing as seen in \textcolor{red}{Figure X}.\\

\subsection{Characteristics of the obtained library and sequences}

A genetic part library should display a number of characteristics to be deemed useful.
First quality expected from a good library is a wide range of relevant trait values (in our case it is the TIR) so that relevant genetic devices can be appropriately optimised.
Second important feature of a library is the diversity of offered sequences, which ensures that sequences compatible with different assembly methods and plasmids are available. 
Additionally, it is very beneficial for the library to show a number of parts with similar numerical values for relevant trait, but with different sequences, so that a designer would have to settle on suboptimal part due to, for example, assembly incompatibility.
Finally, a good library has to be reliable - if the numbers provided in the library are unreliable designers will be reluctant to use.
Figure X shows how library conforms to these requirements.
In short, due to focusing on exploitation distribution of our TIRs is skewed toward strong ones, but there is still a significant amount of RBSs with medium and low strengths.
High diversity of our sequences is shown by lack of relative lack of bias in position weight matrices for different bins.
Finally, we were able to keep the average standard deviation for each bin within XX\% with 6 biological replicates for each sample, which we find satisfactory for reliability.\\
Our library is also showing two characteristics that may be useful in future RBS design efforts.
One of them is the perceived editing distance between two sequences required for meaningful improvement in the TIR. 
We define the editing distance as Hamming distance, that is, how many positions have to be changed to get from one sequence to the other (Hamming distance of 0 means that the sequences are identical and 6 means two completely different sequences).
Figure X shows what edit distance is required for positive change in TIR for RBS with high and medium TIR.
For high TIR RBS, the minimum distance that is required for increase of TIR is 2, with edit distance between 2 and 5 giving similar results.
For medium TIR RBS, a distance of just 1 is enough to produce a meaningful increase in TIR.
That means that as the TIR of examined RBSs increases, the algorithm should be "encouraged" to explore sequences which are more dissimilar to the current candidates.
This finding could also indicate that Adaptive Laboratory Evolution may not be able to find very strong RBSs - the low rate of natural mutations will be too slow to discover through more dissimilar sequences \cite{Lee2012}.
In other words, because the examined sequence is relatively short (6bp in a wider 20bp context) the time to accumulate 2 or 5 changes in the RBS region might be prohibitively long.
In such cases, a directed process should be strongly encouraged.\\
Another feature noticeable in our library is prevalence of specific patterns.
These include X, Y and Z (Figure X). 
Interestingly, RBSs containing these patterns are not detected as capable of binding the relevant ribosome anti-sequence by binding energy calculations \cite{Mann2017}.
At present, there is no strong evidence why these patterns might by permissive for high TIRs, nevertheless, it is important to note that the used algorithm does not require that knowledge to effectively find them.\\

\section{Summary}

In this work, we have shown how machine learning and high-throughput, automated laboratory methods can be used to efficiently generate a library of small parts, in this case bacterial RBS. 
We have used Gaussian Process regression to predict the shape of our function and Upper Confindence Bound Bandit algorithm to recommend sequences to be tested.
We have investigated a number of methods of digitising the DNA sequence, finally settling on Spectrum Kernel with Shift method, which fit into our regression method.
We performed bulk of our experiments using automation to increase their speed, reliability and reproducibility.
By using our workflow, we have generated an extensive library of diverse RBS that can be used in the future studies.\\
There is a number of issues that we would like to address in the future.
The biggest of them is elucidating the reason for none of our sequences being able to significantly cross the TIR of our benchmark sequence.
Essentially, there are two possible reasons for this, one is due to the algorithm and one due to nature.
If the problem is with the algorithm, it might due to its inability to predict TIRs higher than the ones it has been already presented with.
How to solve this?
It might be also possible that the TIR seen in the benchamark is simply already the strongest possible and represents a natural ceiling for it. 
In that case, change in the algorithm will not help.

\section{Contributions}
Zhang M. and Ong C. S. designed and implemented the machine learning algorithms and workflow. Holowko M. B. and Hayman Zumpe H. have designed and performed the laboratory experiments. All authors analysed the data and contributed to and reviewed the manuscript.


\newpage

\printbibliography

\clearpage

\appendix
\input{appendix}
\end{document}
