\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[maxcitenames=1,style=numeric]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{textgreek}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk}
% \usepackage{subfigure}

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\newcommand{\mengyan}[1]{\textcolor{magenta}{#1}}
\newcommand{\maciej}[1]{\textcolor{blue}{#1}}

\title{Machine Learning guided workflow for Ribosome Binding Site engineering}

\author[1,2]{Zhang M.}
\author[3]{Holowko M. B.}
\author[3]{Hayman Zumpe H.}
\author[1,2,4]{Ong, C. S.}
\affil[1]{Machine Learning and Artificial Intelligence Future Science Platform, CSIRO}
\affil[2]{Australian National University}
\affil[3]{CSIRO Synthetic Biology Future Science Platform, CSIRO Land and Water}
\affil[4]{Data61, CSIRO}

\date{\today{}}

\bibliography{ref.bib}
\DeclareUnicodeCharacter{2212}{-}
\begin{document}

\maketitle

\section*{Abstract}

Fine control of gene expression can be achieved through engineering transcriptional and translation control elements, including the Ribosome Binding Site (RBS).
Unfortunately, RBSs are not understood at the level of finesse required for reliable design. 
To address this problem, we have created a machine learning (ML) enabled workflow for the design of bacterial RBSs.
We used Gaussian Process Regression for prediction and the Upper Confidence Bound-based Bandit algorithm for recommendation of genetic designs to be tested in vitro.
We have integrated the ML algorithms with laboratory automation and high-throughput processes, creating a robust workflow for the design of custom RBSs.
Using our workflow, we generated a novel library of diverse RBSs with a wide range of expression levels.
Notably, a high number of these sites demonstrate translation initiation rates equalling or exceeding the currently known strong RBSs.
Additionally, this work elucidated some important RBS design guidelines.

\section{Introduction}

One of the main tenets of synthetic biology is design, evaluation and standardisation of genetic parts \cite{Brophy2014,Canton2008,Stanton2014}.
This is usually done in terms of the Design-Build-Test-Learn (DBTL) cycle, where the given genetic part or organism are continually improved by going through a number of turns of the said cycle.
This normally involves designing the DNA sequence in Computer Aided Design (CAD) software and then physically testing it in a laboratory. 
Additionally, computer modelling and prediction of part behaviour based on the designed DNA sequence or design of DNA sequence based on expected function can be used\cite{Yeoh2019,Nielsen2016}.
Most of these models are based on either the thermodynamic properties of the involved molecules (DNA, RNA, proteins, etc.) or empirically obtained values describing a relevant to a given design property, like Translation Initiation Rate (TIR) in the case of Ribosome Binding Sites (RBS) \cite{Xia1998,Chen2013,Reeve2014}.
However, de-novo design of small genetic elements is still challenging due to unknown relationships between sequence and performance of such elements. 
This means that many designers have to rely on known and characterised parts that may not be optimal for their constructs.
The problem with this approach is that such part libraries can also be unreliable due to poor reliability of methods used to obtain them.\\
The biggest limitation for the DBTL approach currently is the Learn part of the cycle - there is very limited access to methods and software that can improve and understand designs based on the experimental results.
For example, according to Reeve \emph{et al.} there are three main RBS calculators, all predicting the TRI based on the thermodynamic properties of the RBS and the ribosome \cite{Seo2013,Na2010,Salis2009}. 
Reported predictions from all of these models are relatively good ($R^2 >0.8$), but they come with a number of caveats: i) they rely on calculations of free energies that can be hard to estimate with high precision ii) in general, one of the best ways to improve the models' accuracy is by increasing the number of phenomenons taken into account, but this can lead to paradoxically decreased model accuracy due to accumulation of errors \cite{EspahBorujeni2016} and iii) by using deterministic coefficients to calculate energies one disregards often stochastic nature of processes in the cells which again increases perceived prediction error \cite{Goss1998}. 
There are also sources showing that binding energy calculations may be poor predictors of RBS strength \cite{Saito2020,Sherer1980} . This is reinforced by studies suggesting that RNA secondary structure is potentially a more important feature in TIR determination \cite{DESMIT1994,EspahBorujeni2016} .\\
Synthetic biology is currently going through a phase of exponential increase in volume of data produced during experiments \cite{Freemont2019}. 
New experimental methods heavily relying on advances in automation and microfludics allow unprecedented precision and throughput in data generation.
These new data-sets can be combined with data reliant machine learning algorithms to generate new models and predictors for use in synthetic biology, vastly improving the DBTL cycle's performance \cite{Camacho2018}. 
In the past few years there was a significant uptake of Machine Learning based approaches in synthetic biology \cite{LAWSON2021}.
Jervis \emph{et al.} used support vector machine and neural network to optimise production of monoterpenoid in \emph{Esherichia coli} \cite{Jervis2019}.
Similarly, Costello \emph{et al.} have used a number of machine learning approaches to analyse time-series multiomics data to predict metabolic pathway behaviour \cite{Costello2018}.
There were also successful attempts at using deep learning techniques for analysis of big data-sets \cite{Alipanahi2015,Angermueller2016}. Machine learning has been also used for prediction in proteins \cite{Yang2018}.
However, the use of machine learning in synthetic biology is still in its infancy and will require additional research to show its full potential. \\
Here we present how machine learning algorithms can be used as part of the DBTL cycle to predict (Learn) and recommend (Design) variants of RBS with goal of optimisation of associated protein level expression. 
RBS being one of the key genetic elements controlling protein expression and at the same time having a relatively short sequence is a perfect target for establishing workflows that can be later translated to more complicated systems.
In this work We have used Gaussian Process Regression and Upper Confidence Bound multi-armed Bandits algorithms for prediction and recommendation respectively to analyse and optimise the initiation rates of the designed RBS \cite{desautels2012parallelizing, Rasmussen2004}.
Our overall experimental goal was to maximise the Translation Initiation Rate (TIR) by identifying the set of RBS sequences with top TIR scores while minimising the number of DBTL cycle turns that we had to do.
We did this by designing a sequential experimental workflow, where designs in the zeroth round where randomised RBS sequences designed to explore the experimental space and some preliminary machine learning recommended designs based on literature. 
In the subsequent rounds, designs where recommended by the algorithm based on the data obtained in the previous rounds. 
The designs were then physically constructed in batches of 90 to fit our automated process (see \textbf{Methods} section).
After constructing, the plasmids harbouring the new genetic devices were tested in microplate reader.
The results were then fed back to the algorithm for it to recommend the next round of designs.
This way, we were able to build an extensive, reliable library of novel RBSs with diverse sequences.
At the same time we were able to discover new RBS sequences with very high TIRs.\\

\section{Results}


Our experiments followed the Design-Build-Test-Learn cycle methodology.
Figure \ref{fig: Flowchart} shows our DBTL cycle with the machine learning process emphasised.
Principally, machine learning is used in the LEARN and DESIGN phase and the other two are driven by choices made by human researchers. 
In LEARN phase we use the Gaussian Process regression algorithm to predict the TIR of different RBS sequences and in the DESIGN phase we use the Upper Confidence Bound multi-armed Bandit algorithm to recommend sequences to be Built and Tested next.\\

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.7]{plots/flowchart.pdf}
    \caption{Flowchart of machine learning based experimental design.}
    \label{fig: Flowchart}
\end{figure}



\subsection{DESIGN of the genetic device}
In our genetic design, the investigated RBS controls expression of the Green Fluorescent Protein (GFP) in its mut3b variant. 
By controlling expression of a fluorescent protein with the RBS we can quickly assess the perceived relative TIR by measuring fluorescence of cells harbouring plasmid with the device over time.
Finally, the mRNA is transcribed from an IPTG-inducible promoter pLlacO-1. 
By making the whole device inducible we can synchronise the start of the expression of the GFP in all the cultures by inducing them at the same time with addition of IPTG.\\

In \emph{E. coli} the RBS is usually located in the 20 bases upstream of the start codon. 
Additionally, there is a consensus RBS core sequence called the Shine-Dalgarno sequence, which in \emph{E. coli} is AGGAGG. 
Here, we put that 20 bp long sequence into focus with main emphasis being put on the 6bp core region (see detail in Figure \ref{fig: Flowchart}).\\

Our template RBS sequence is 20 bps long with the sequence TTTAAGAAGGAGATATACA.
This sequence is a known high TIR RBS that comes with the pBb series plasmids \cite{Lee2011}. 
Since this is the sequence against which new RBS sequenced will be benchmarked we will refer to this sequence as the benchmark sequence from now on.
In our design we focus on randomising of the core at positions -8 to -13 (relative to the start codon of the GFP) nucleotides of the RBS and fix others to be the same as the consensus sequence, i.e. TTTAAGA + NNNNNN + TATACAT.
We have experimentally confirmed that changing the core sequence is statistically more impactful on TIR than changes made outside of it (see Supplementary materials). 
Since for each of the 6 position there are 4 possibilities: A, C, G, T the total experimental (variant) space is $4^6$ = 4096.\\

The design recommendations were made using the Multi-armed Bandit algorithm.
In short, the this algorithm is a stochastic method of probing of the experimental space. 
This algorithm aims at maximising the reward (output) from testing a limited number of instances from a big pool which cannot be wholly tested due to limited resources (time, computational power, capital). 
In our case we use the Upper Confidence Bound version of the algorithm, which focuses its recommendations on sequences that should give highest TIR based on the probabilities computed by the prediction algorithm (GPR). 
Another feature of the bandits algorithms is that it balances two approaches: exploration and exploitation. 
Exploration makes the algorithm recommend designs that will improve the predictions, whereas exploitation will recommend designs that focus on delivering the most efficient design the fastest. 
The two approaches can be controlled with the \textbeta\enspace parameter. 
We have decided that in the first iterations of the cycle it would be beneficial to skew the algorithm towards the exploration with exploitation taking increasing role in later iterations. 
One thing of note is that the bandit algorithm is stochastic, that is it exploits the probabilities of given event occurring (in this case RBS having a specific TIR). 
As such, it pairs naturally with our prediction algorithm, the Gaussian Process, which provides probability based function regression.
\\

To generate the dataset that the algorithm could learn from we have decided to characterise a total of 450 RBS variants, which constitutes a little over 10\% of the whole experimental space. 
To fit into our automated workflow, we have divided the 450 variants into batches of 90.
In the zeroth round we have tested two batches of designs, for total of 180 variants split as below: 
\mengyan{we might want to re-format according to the swarm plot groups, with groups names and in order. }

\begin{enumerate}
    \item 60 RBS sequences which are subsequent single nucleotide changes of all 20 nucleotides of the original, consensus sequence. This batch is designed to show us influence of such single nucleotide changes on the overall performance of the RBS and the potential impact of changes made beyond the core part.
    \item 30 RBS sequences that were  uniformly randomised, i.e. equal probability of choosing either nucleotide for each position. 
    \item 30 RBS sequences randomised based on the position probability matrix (PPM) generated from all the naturally occurring RBS sequences in \emph{E. coli} genome \mengyan{Maciej: can you add reference here?}
    \item 60 RBS sequences recommended by our implementation of recommendation algorithm based on a data set obtained from literature \cite{jervis2018machine}, which contains 113 non-repeated records for 56 unique RBS sequences with the respective TIR.
    This data set has been used due to perceived similarity of its goal to the one of this work - prediction of TIR based on phenotypic output.
\end{enumerate}

In the subsequent rounds, all 90 designs were generated using our machine learning algorithm based on the data obtained from the previous rounds. 

\subsection{BUILD and TEST}

For the machine learning to work effectively the analysed data set needs to show two qualities: high relative volume and high quality of data.
These two elements don't have specific definitions, but in general the data set has to be going into at least hundreds data points and have to cover 5-10\% of that space.
\mengyan{Maciej: I think it varies from case to case. We need to either change it or add reference.}
And since quality of the obtained data has a direct and strong correlation with quality of the predictions and in effect - recommendations, one need to ensure that the obtained results represent the real value as close as possible.\\
To help us obtain reliable and reproducible results we have employed automation-heavy workflow for our experiments.
This way we hope to eliminate a big part of sample-to-sample variation as well as human-introduced variation.
Additionally, performing all the procedures directly in 96-well microplate format enabled us to significantly cut down time to prepare our variants.\\
In short, the genetic variations of the RBS were introduced to the plasmids with combination of PCR and isothermal assembly. 
The plasmids were then transformed and the resulting transformants were tested using microplate reader.
Vast majority of reactions were prepared using liquid handling equipment.
Similarly, colony picking was done by an automated colony picker.\\

\subsection{LEARN - prediction of RBS performance}
\mengyan{Maciej: shall we put LEARN before DESIGN? since in DESIGN, we have already used the LEARN result from benchmark data anyway.}

The three main considerations when designing a machine learning workflow for synthetic biology is feature representation, prediction and recommendation algorithms.
We will discuss our workflow in terms of the Design-Build-Learn-Test cycle, where prediction will be discussed in the Design section and feature representation with prediction in the Learn one.\\


Our goal in the LEARN part is to learn a model which takes the RBS sequences as input and predicts the TIR values with uncertainty level about the prediction, based on the experimental data.
Gaussian Process regression (GPR) is a Bayesian approach and has been widely used for experimental design \cite{srinivas2012information, romero_navigating_2013}.
The explicit representation of model uncertainty provides further guide for efficient searching through large experimental space of possible sequences in DESIGN section. 

A crucial ingredient in a Gaussian Process predictor \cite{Rasmussen2004} is the kernel (covariance) function, which captures the similarity between RBS sequences.
Kernel functions implicitly embed RBS sequences into high-dimensional feature space which enables capturing similarities between data points and makes the regression process easier.
% The additional effect of using kernels is that our data will be moved to a higher-dimensional space, which makes the regression process easier.
For Round 0, since we only have access to limited number of literature sequences, we chose to use one of the basic string kernels, the \textit{spectrum kernel} \cite{leslie2001spectrum} to process the core 6bp and dot product kernel \cite{Rasmussen2004} (with one-hot embedding) to process the 7bp flanking sequences both upstream and downstream of the core sequence.
For subsequent rounds, we used the a more powerful kernel function, the \textit{weighted degree kernel with shift} (wds) \cite{ratsch_rase_2005_wds}, which has been shown to have a strong performance in varies prediction tasks \cite{Ben-Hur2008}.
% The predictions were made using the Gaussian Process regression (GPR) algorithm. 
% In essence, Gaussian Process is a stochastic (Bayesian) predictor of the shape of a function, 
% which is built based on perceived similarities between data points (kernels), where not only a mean value, but also probability distribution of the mean is calculated. 
% Such an approach makes GPR well suited to predict biological phenomena which are also highly stochastic. \\

% For our predictions, we had to solve the problem of feature representation first.
% In our case, that problem was how to embed the DNA sequence to a numerical vector which can be used in machine learning predictions.\\
% Due to our use of the Gaussian Process Regression for predictions we have chosen kernel (covariance) functions for our embedding.
  
% There is a number of useful kernel functions that can be used for embedding of DNA.
% In our case, we use string kernels \cite{}, which take two DNA sequences as input and output a real value which represents the similarity between them. 
% The commonly used string kernels includes spectrum kernels \cite{}, mixed spectrum kernels \cite{}, weighted degree kernels \cite{} and weighted degree kernels with shifts \cite{}. 
% We review the design of the above string kernels in Supplementary materials \ref{}.

% This approach allowed for good balance between computational complexity and performance.\\

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.32]{paper/plots/scatter_abc1_FF.pdf}
    \caption{Scatterplot.}
    \label{fig: Scatterplot}
\end{figure}

\subsection{EXIT}
There are two potential points where the exit from the cycle should be considered: i) the optimum solution has been found or ii) depletion of available resources (time and/or money). In our case we have performed a total of four rounds, with performance of the RBSes increasing as seen in Figure \ref{fig: Swarmplot and Quantplot}.\\
\mengyan{Maciej: shall we show the result (swarmplot) in DESIGN when we introduce different groups?}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.32]{paper/plots/swarmplot.pdf}
    \includegraphics[scale=0.32]{paper/plots/quantplot.pdf}
    \caption{Swarmplot and Quantplot.}
    \label{fig: Swarmplot and Quantplot}
\end{figure}

\subsection{Characteristics of the obtained library and sequences}

A genetic part library should display a number of characteristics to be deemed useful.
First quality expected from a good library is a wide range of relevant trait values (in our case it is the TIR) so that relevant genetic devices can be appropriately optimised.
Second important feature of a library is the diversity of offered sequences, which ensures that sequences compatible with different assembly methods and plasmids are available. 
Additionally, it is very beneficial for the library to show a number of parts with similar numerical values for relevant trait, but with different sequences, so that a designer would have to settle on suboptimal part due to, for example, assembly incompatibility.
Finally, a good library has to be reliable - if the numbers provided in the library are unreliable designers will be reluctant to use.
Figure X shows how library conforms to these requirements.
In short, due to focusing on exploitation distribution of our TIRs is skewed toward strong ones, but there is still a significant amount of RBSs with medium and low strengths.
High diversity of our sequences is shown by lack of relative lack of bias in position weight matrices for different bins.
Finally, we were able to keep the average standard deviation for each bin within XX\% with 6 biological replicates for each sample, which we find satisfactory for reliability.\\

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[scale=1.2]{plots/logo.pdf}
         \caption{$y=x$}
         \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[scale=0.5]{plots/Patterns.png}
         \caption{$y=3sinx$}
         \label{fig:three sin x}
     \end{subfigure}
     \vskip\baselineskip
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[scale=0.5]{plots/Hd_Heatmap.png}
         \caption{$y=5/x$}
         \label{fig:five over x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[scale=0.5]{plots/Histogram.png}
         \caption{$y=5/x$}
         \label{fig:five over x}
     \end{subfigure}
        \caption{Three simple graphs}
        \label{fig:three graphs}
\end{figure}

Our library is also showing two characteristics that may be useful in future RBS design efforts.
One of them is the perceived editing distance between two sequences required for meaningful improvement in the TIR. 
We define the editing distance as Hamming distance, that is, how many positions have to be changed to get from one sequence to the other (Hamming distance of 0 means that the sequences are identical and 6 means two completely different sequences).
Figure X shows what edit distance is required for positive change in TIR for RBS with high and medium TIR.
For high TIR RBS, the minimum distance that is required for increase of TIR is 2, with edit distance between 2 and 5 giving similar results.
For medium TIR RBS, a distance of just 1 is enough to produce a meaningful increase in TIR.
That means that as the TIR of examined RBSs increases, the algorithm should be "encouraged" to explore sequences which are more dissimilar to the current candidates.
This finding could also indicate that Adaptive Laboratory Evolution may not be able to find very strong RBSs - the low rate of natural mutations will be too slow to discover through more dissimilar sequences \cite{Lee2012}.
In other words, because the examined sequence is relatively short (6bp in a wider 20bp context) the time to accumulate 2 or 5 changes in the RBS region might be prohibitively long.
In such cases, a directed process should be strongly encouraged.\\
Another feature noticeable in our library is prevalence of specific patterns.
These include X, Y and Z (Figure X). 
Interestingly, RBSs containing these patterns are not detected as capable of binding the relevant ribosome anti-sequence by binding energy calculations \cite{Mann2017}.
At present, there is no strong evidence why these patterns might by permissive for high TIRs, nevertheless, it is important to note that the used algorithm does not require that knowledge to effectively find them.\\

\section{Discussion}

In this work, we have shown how machine learning and high-throughput, automated laboratory methods can be used to efficiently generate a library of small parts, in this case bacterial RBS. 
We have used Gaussian Process regression to predict the shape of our function and Upper Confidence Bound Bandit algorithm to recommend sequences to be tested.
We have investigated a number of methods of digitising the DNA sequence, finally settling on Spectrum Kernel with Shift method, which fit into our regression method.
We performed bulk of our experiments using automation to increase their speed, reliability and reproducibility.
By using our workflow, we have generated an extensive library of diverse RBS that can be used in the future studies.\\
There is a number of issues that we would like to address in the future.
The biggest of them is elucidating the reason for none of our sequences being able to significantly cross the TIR of our benchmark sequence.
Essentially, there are two possible reasons for this, one is due to the algorithm and one due to nature.
If the problem is with the algorithm, it might due to its inability to predict TIRs higher than the ones it has been already presented with.
How to solve this?
It might be also possible that the TIR seen in the benchmark is simply already the strongest possible and represents a natural ceiling for it. 
In that case, change in the algorithm will not help.
Another problem we would like to tackle is the not always good prediction of the TIR of a given RBS.
In principle, the predictions should improve with more data fed into the algorithm, but that would be against the goal of the study which is to minimise the number of data points that have to be obtained.\\
In the future, we hope to extend the algorithm to other, more complicated genetic elements.
This could include promoters and terminators.
The complexity of the task quickly increases with the length of the sequence.
This is because the experimental space grows exponentially with the number of examined positions and so the space becomes increasingly hard to cover with experiments.
To solve this problem, a different algorithms or experimental techniques might be needed, but the general workflow can be reused.\\
Finally, we hope to see our RBS library being adapted by the community.
We believe that its favourable characteristics will make if very useful to designers.

\input{methods}

\section*{Contributions}
Zhang M. and Ong C. S. designed and implemented the machine learning algorithms and workflow. Holowko M. B. and Hayman Zumpe H. have designed and performed the laboratory experiments. All authors analysed the data and contributed to and reviewed the manuscript.

\section*{Competing interests}
The authors declare no competing interests

\section*{Acknowledgments}
The authors would like to acknowlege CSIRO's AI/ML and Synthetic Biology Future Science Platforms for providing funding for this research.


\newpage

\printbibliography

\clearpage

\appendix
\input{supplementary}
\end{document}
